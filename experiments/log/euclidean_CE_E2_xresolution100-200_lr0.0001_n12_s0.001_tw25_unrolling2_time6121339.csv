Training on dataset data/CE_train_E2.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time6121339.tar
Beta parameter added to the GNN solver
Number of parameters: 1033789.0
Saved initial model at models/init6121339.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
Node: 14 (pos: 0.141): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 15 (pos: 0.152): 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07
Node: 00 (pos: 0.000): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 01 (pos: 0.010): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 02 (pos: 0.020): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
-
Node: 03 (pos: 0.030): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 05 (pos: 0.051): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 06 (pos: 0.061): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 07 (pos: 0.071): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 08 (pos: 0.081): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 09 (pos: 0.091): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 36 (pos: 0.364): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
Node: 37 (pos: 0.374): 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03
Node: 38 (pos: 0.384): 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03
Node: 39 (pos: 0.394): 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04
Node: 40 (pos: 0.404): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
-
Node: 41 (pos: 0.414): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 42 (pos: 0.424): 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07
Node: 19 (pos: 0.192): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 20 (pos: 0.202): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
Node: 21 (pos: 0.212): 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04
Node: 22 (pos: 0.222): 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03
=========================================================================================================
Training Loss (progress: 0.00), 1.1662783341343044, [0.0019380753971909484, 0.01040392935865313, 0.010778336438469921, 0.009272557143795627, 0.011207935800494914, 0.011028105706517074], [1.0085189829370058, 1.014407068787103, 1.0139881075239603, 1.012380993148865, 1.013387273687893, 1.0132904699779655]
Training Loss (progress: 0.08), 0.19783402590942797, [0.000911618386369488, 0.0007192905071278834, 0.002643203408557015, 0.0010737896996693794, 0.02164065677651342, 0.008193379483530992], [1.03300208778909, 1.0410901162653716, 1.0334953842049461, 1.0412796527616994, 1.0338766412282756, 1.0428808496038635]
Training Loss (progress: 0.16), 0.14511348202810143, [0.0008027745456537217, 0.0006200699889532854, 0.002777231147215611, 0.000923622456610089, 0.024275524262471274, 0.007472966982721866], [1.0351279892975833, 1.0486320615848626, 1.0343709308525428, 1.0480855869037908, 1.0380898531320428, 1.0486697423277715]
Training Loss (progress: 0.24), 0.1327948355800778, [0.0008468636371022434, 0.0005219902276193108, 0.0030360730456738382, 0.001057341537873111, 0.02685716346534031, 0.007752219781668253], [1.037589661999865, 1.0543810689116948, 1.0353664188284826, 1.0522420295011572, 1.0416148968696448, 1.0537199816440084]
Training Loss (progress: 0.32), 0.12347394703193769, [0.0011022196792675922, 0.0005602124785851583, 0.0034392499044588317, 0.0009591461710506955, 0.028786156926009353, 0.008336680385124565], [1.0394660200056125, 1.06076416567958, 1.0373620466379365, 1.056904359347204, 1.0446915064685103, 1.0582035535147458]
Training Loss (progress: 0.40), 0.11118659936513269, [0.0008621589503956418, 0.0004618008580440514, 0.003353719681492431, 0.0011558418655319247, 0.030292711911683253, 0.008569448683661014], [1.0404440064884426, 1.069051003300718, 1.0399036493950455, 1.0613288027207493, 1.047334886302511, 1.0612381136827784]
Training Loss (progress: 0.48), 0.11026972481599798, [0.0008293424964615001, 0.000510208693417384, 0.003379589482584034, 0.0012841057191935157, 0.03400541120851625, 0.009157378116451753], [1.0408301167839293, 1.0763592520789877, 1.0441688377836353, 1.0664193504702477, 1.0512498498775125, 1.0652144482231023]
Training Loss (progress: 0.56), 0.09791331724292754, [0.0008552833653665652, 0.00040128340093649565, 0.0032546221527784036, 0.0013213809083724235, 0.036832406757516034, 0.009545282753502644], [1.0401552661224958, 1.082246512674084, 1.0469381462353422, 1.0711527926628355, 1.053647323498427, 1.0690885306229059]
Training Loss (progress: 0.64), 0.0900023645923357, [0.0007579282657883704, 0.0005453930736155, 0.0035557860905964843, 0.0013197804111740733, 0.03915062040157112, 0.009773176646107683], [1.0396948976926927, 1.088878891342838, 1.0500340987402086, 1.076359088117234, 1.0559151259020279, 1.0711920638104675]
Training Loss (progress: 0.72), 0.09003531615971513, [0.00083718621198795, 0.0005322899959899585, 0.003292619761478257, 0.0012473439640817302, 0.041573151215278145, 0.00981563199449425], [1.0380310126498682, 1.0943537182603567, 1.0524599217376738, 1.081077425289054, 1.0575957559131002, 1.0736359569951752]
Training Loss (progress: 0.80), 0.08955783181071275, [0.0008425808219373478, 0.0004355577285580962, 0.003064395124412492, 0.001355371493953439, 0.043882207997037445, 0.009971764034091946], [1.0368776723122415, 1.0977877659343733, 1.0561832994485478, 1.0859353657833768, 1.060053844909029, 1.0757457535657087]
Training Loss (progress: 0.88), 0.08943979401957859, [0.0009017011755850758, 0.0003921916379571186, 0.0032344761427672273, 0.0012988963561649477, 0.04685260509386807, 0.010035306261999019], [1.0347026951013156, 1.101920484150321, 1.0598423150573681, 1.0906065565211014, 1.062150778806481, 1.0783074526675978]
Training Loss (progress: 0.96), 0.08258776566487146, [0.0008245348343327469, 0.0004976581531808338, 0.003358308092317915, 0.00131663034448975, 0.049341783008155164, 0.010343465089649579], [1.034332071558366, 1.106287531147335, 1.0632779795353917, 1.0942108072857948, 1.0635357879317417, 1.0801408167077509]
Evaluation on validation dataset:
Step 25, mean loss 0.06706496444715009
Step 50, mean loss 0.06501173314462401
Step 75, mean loss 0.07719066054871136
Step 100, mean loss 0.08487025610560711
Step 125, mean loss 0.10159343805145832
Step 150, mean loss 0.10220295604338822
Step 175, mean loss 0.11340084610000523
Step 200, mean loss 0.1316713853767885
Step 225, mean loss 0.1858107123580961
Unrolled forward losses 15.229815852123256
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.45991e-07, 2.63018e-09, 4.00274e-02, 5.37163e-04, 8.67067e-01, 3.88515e-01
Node: 14 (pos: 0.141): 1.51115e-08, 4.06243e-11, 2.00986e-02, 1.08391e-04, 8.30659e-01, 3.13376e-01
Node: 15 (pos: 0.152): 4.89832e-10, 4.21785e-13, 9.45099e-03, 1.87792e-05, 7.92536e-01, 2.47649e-01
Node: 00 (pos: 0.000): 9.50820e-02, 4.62153e-02, 6.29692e-01, 3.24013e-01, 1.02937e+00, 9.17849e-01
Node: 01 (pos: 0.010): 2.69992e-01, 1.85575e-01, 7.92244e-01, 5.52416e-01, 1.04420e+00, 9.86018e-01
Node: 02 (pos: 0.020): 5.68988e-01, 5.00907e-01, 9.33459e-01, 8.08667e-01, 1.05492e+00, 1.03779e+00
-
Node: 03 (pos: 0.030): 8.89927e-01, 9.08866e-01, 1.03000e+00, 1.01642e+00, 1.06140e+00, 1.07015e+00
Node: 05 (pos: 0.051): 8.89927e-01, 9.08866e-01, 1.03000e+00, 1.01642e+00, 1.06140e+00, 1.07015e+00
Node: 06 (pos: 0.061): 5.68988e-01, 5.00907e-01, 9.33459e-01, 8.08667e-01, 1.05492e+00, 1.03779e+00
Node: 07 (pos: 0.071): 2.69992e-01, 1.85575e-01, 7.92244e-01, 5.52416e-01, 1.04420e+00, 9.86018e-01
Node: 08 (pos: 0.081): 9.50820e-02, 4.62153e-02, 6.29692e-01, 3.24013e-01, 1.02937e+00, 9.17849e-01
Node: 09 (pos: 0.091): 2.48510e-02, 7.73672e-03, 4.68708e-01, 1.63176e-01, 1.01062e+00, 8.37082e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 2.48510e-02, 7.73672e-03, 4.68708e-01, 1.63176e-01, 1.01062e+00, 8.37082e-01
Node: 36 (pos: 0.364): 4.82048e-03, 8.70629e-04, 3.26725e-01, 7.05584e-02, 9.88165e-01, 7.47954e-01
Node: 37 (pos: 0.374): 6.93961e-04, 6.58589e-05, 2.13289e-01, 2.61963e-02, 9.62269e-01, 6.54776e-01
Node: 38 (pos: 0.384): 7.41445e-05, 3.34888e-06, 1.30394e-01, 8.35084e-03, 9.33232e-01, 5.61591e-01
Node: 39 (pos: 0.394): 5.87926e-06, 1.14470e-07, 7.46544e-02, 2.28570e-03, 9.01381e-01, 4.71909e-01
Node: 40 (pos: 0.404): 3.45991e-07, 2.63018e-09, 4.00274e-02, 5.37163e-04, 8.67067e-01, 3.88515e-01
-
Node: 41 (pos: 0.414): 1.51115e-08, 4.06243e-11, 2.00986e-02, 1.08391e-04, 8.30659e-01, 3.13376e-01
Node: 42 (pos: 0.424): 4.89832e-10, 4.21785e-13, 9.45099e-03, 1.87792e-05, 7.92536e-01, 2.47649e-01
Node: 19 (pos: 0.192): 1.51115e-08, 4.06243e-11, 2.00986e-02, 1.08391e-04, 8.30659e-01, 3.13376e-01
Node: 20 (pos: 0.202): 3.45991e-07, 2.63018e-09, 4.00274e-02, 5.37163e-04, 8.67067e-01, 3.88515e-01
Node: 21 (pos: 0.212): 5.87926e-06, 1.14470e-07, 7.46544e-02, 2.28570e-03, 9.01381e-01, 4.71909e-01
Node: 22 (pos: 0.222): 7.41445e-05, 3.34888e-06, 1.30394e-01, 8.35084e-03, 9.33232e-01, 5.61591e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.05324936850045443
Step 50, mean loss 0.05765414688786939
Step 75, mean loss 0.07437505725529656
Step 100, mean loss 0.0953449845052966
Step 125, mean loss 0.11767041049568731
Step 150, mean loss 0.156565038434953
Step 175, mean loss 0.16923864741122419
Step 200, mean loss 0.2122856280994533
Step 225, mean loss 0.26595759625449517
Unrolled forward losses 15.585094379058651
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time6121339.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.19621797120004703, [0.0007786069201178726, 0.00046564974285260235, 0.0036532520841543407, 0.0013433184444009878, 0.050323198620592834, 0.009900071142229905], [1.0332672280048614, 1.1082602255427063, 1.0649704468325112, 1.09770866864648, 1.0637255628597906, 1.0810766820422872]
Training Loss (progress: 0.08), 0.19274145120091518, [0.001023097509659273, 0.0005395910192590849, 0.003986673974083626, 0.0014346843036305186, 0.05610783165053517, 0.01107067905119154], [1.0312065744611585, 1.1092042458402087, 1.066539422385015, 1.1033453172557257, 1.066960652135312, 1.0837160797861425]
Training Loss (progress: 0.16), 0.180051356556236, [0.001027044348706015, 0.0006208561146296345, 0.004268019384298329, 0.0014836783242958716, 0.05939217166860192, 0.0111469692952724], [1.029159317334851, 1.1102830925360212, 1.0691843142289505, 1.107659726390946, 1.0682072719157896, 1.0859304498434184]
Training Loss (progress: 0.24), 0.18542207882650605, [0.0011373818795439956, 0.000664611324548952, 0.004760055790642839, 0.001649643291308413, 0.06310209665019988, 0.011398388943263777], [1.0274613449969965, 1.1107177693218413, 1.0732501478968075, 1.112014376265061, 1.0697820120818626, 1.0879255591743433]
Training Loss (progress: 0.32), 0.1568488901043042, [0.0009080118465852728, 0.0006272653780055787, 0.0044352354563139485, 0.0016089266949210691, 0.0671481639330973, 0.011071130654319233], [1.0265819080320915, 1.112159946153081, 1.0762949053003807, 1.116899586777345, 1.0717329244895744, 1.0895667675600196]
Training Loss (progress: 0.40), 0.1702897135173364, [0.001091232071415937, 0.0006650390566322644, 0.004541145253523751, 0.0015482095515268574, 0.06971520782631248, 0.011712064588300711], [1.0247427518361791, 1.1126223412058738, 1.0808145180752344, 1.1209504170040898, 1.0729188701214594, 1.0918450193275564]
Training Loss (progress: 0.48), 0.1534257061357934, [0.0009853535997759932, 0.0006243460337883669, 0.004421839784113342, 0.0016414069824435807, 0.07208245122729454, 0.01141529162869031], [1.0223548788080452, 1.1140950488036674, 1.0837748144231492, 1.1241550822019803, 1.0732481600533261, 1.093098237259492]
Training Loss (progress: 0.56), 0.15916824540268895, [0.0012000412491878626, 0.0007272270766052546, 0.004163034543570508, 0.0016544392565619158, 0.07479211586865048, 0.011576272331346355], [1.0205194827066133, 1.1149647970548726, 1.0878335592458903, 1.1286277218439564, 1.074138804609621, 1.0952294195393013]
Training Loss (progress: 0.64), 0.16807435578259095, [0.0010945697943754997, 0.0007698319270886905, 0.004626361673123673, 0.0015415159757473868, 0.07691386358047161, 0.011274829975992261], [1.0181765519763735, 1.1152799714290789, 1.09086224788533, 1.1327733230226014, 1.0750525097089203, 1.0976860471632306]
Training Loss (progress: 0.72), 0.1459982460532383, [0.0010408969731266313, 0.0006175775320473031, 0.004691780630431259, 0.001582818508724294, 0.08001104748342804, 0.011259971717679236], [1.016051790372549, 1.1149735625539585, 1.094449978130046, 1.1358792771472108, 1.0766906292228904, 1.0989554551541953]
Training Loss (progress: 0.80), 0.1529877142858425, [0.001278317521361013, 0.0006157950729512474, 0.004415282593304903, 0.001552175734877099, 0.08155854267765775, 0.011066155162559734], [1.0147203779892882, 1.1174309453087388, 1.0981419872090064, 1.1399207823382138, 1.0774443609633955, 1.1005245863568693]
Training Loss (progress: 0.88), 0.14727032172529045, [0.0014079567588835941, 0.0007657680103526465, 0.004205568393389478, 0.0015868769116633116, 0.08409585042712983, 0.011288109175702922], [1.0119115635443372, 1.115445739960572, 1.1006010585591592, 1.1418930176126998, 1.078286265999209, 1.1031739803840133]
Training Loss (progress: 0.96), 0.1535212621706004, [0.001292883516557949, 0.0008131693072346636, 0.004487717639561304, 0.001465179115155435, 0.08666504273481589, 0.011763974409244581], [1.0095583472727414, 1.1180931272708363, 1.1049786017210077, 1.144441975343593, 1.0793186091383224, 1.104723228070486]
Evaluation on validation dataset:
Step 25, mean loss 0.11250590821677733
Step 50, mean loss 0.06268803602347098
Step 75, mean loss 0.0700100971957904
Step 100, mean loss 0.07594906170269143
Step 125, mean loss 0.08619175168993018
Step 150, mean loss 0.1042898690424727
Step 175, mean loss 0.10896184470725578
Step 200, mean loss 0.1312039195208064
Step 225, mean loss 0.15699852640590184
Unrolled forward losses 5.675376775081343
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 1.12411e-04, 2.10279e-08, 1.08729e-01, 1.68222e-03, 9.60731e-01, 4.44305e-01
Node: 14 (pos: 0.141): 1.66212e-05, 5.01694e-10, 6.67997e-02, 4.27457e-04, 9.37498e-01, 3.66936e-01
Node: 15 (pos: 0.152): 2.04859e-06, 8.38632e-12, 3.91789e-02, 9.53312e-05, 9.12695e-01, 2.97568e-01
Node: 00 (pos: 0.000): 2.35177e-01, 6.48968e-02, 7.63200e-01, 4.03506e-01, 1.05957e+00, 9.55089e-01
Node: 01 (pos: 0.010): 4.44743e-01, 2.25429e-01, 8.97765e-01, 6.37061e-01, 1.06825e+00, 1.01798e+00
Node: 02 (pos: 0.020): 7.01073e-01, 5.48636e-01, 1.00818e+00, 8.82766e-01, 1.07450e+00, 1.06543e+00
-
Node: 03 (pos: 0.030): 9.21204e-01, 9.35513e-01, 1.08084e+00, 1.07360e+00, 1.07826e+00, 1.09495e+00
Node: 05 (pos: 0.051): 9.21204e-01, 9.35513e-01, 1.08084e+00, 1.07360e+00, 1.07826e+00, 1.09495e+00
Node: 06 (pos: 0.061): 7.01073e-01, 5.48636e-01, 1.00818e+00, 8.82766e-01, 1.07450e+00, 1.06543e+00
Node: 07 (pos: 0.071): 4.44743e-01, 2.25429e-01, 8.97765e-01, 6.37061e-01, 1.06825e+00, 1.01798e+00
Node: 08 (pos: 0.081): 2.35177e-01, 6.48968e-02, 7.63200e-01, 4.03506e-01, 1.05957e+00, 9.55089e-01
Node: 09 (pos: 0.091): 1.03661e-01, 1.30896e-02, 6.19390e-01, 2.24313e-01, 1.04851e+00, 8.79900e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.03661e-01, 1.30896e-02, 6.19390e-01, 2.24313e-01, 1.04851e+00, 8.79900e-01
Node: 36 (pos: 0.364): 3.80872e-02, 1.84979e-03, 4.79889e-01, 1.09444e-01, 1.03515e+00, 7.95994e-01
Node: 37 (pos: 0.374): 1.16648e-02, 1.83149e-04, 3.54950e-01, 4.68666e-02, 1.01958e+00, 7.07087e-01
Node: 38 (pos: 0.384): 2.97795e-03, 1.27051e-05, 2.50636e-01, 1.76145e-02, 1.00191e+00, 6.16769e-01
Node: 39 (pos: 0.394): 6.33715e-04, 6.17508e-07, 1.68955e-01, 5.81045e-03, 9.82248e-01, 5.28273e-01
Node: 40 (pos: 0.404): 1.12411e-04, 2.10279e-08, 1.08729e-01, 1.68222e-03, 9.60731e-01, 4.44305e-01
-
Node: 41 (pos: 0.414): 1.66212e-05, 5.01694e-10, 6.67997e-02, 4.27457e-04, 9.37498e-01, 3.66936e-01
Node: 42 (pos: 0.424): 2.04859e-06, 8.38632e-12, 3.91789e-02, 9.53312e-05, 9.12695e-01, 2.97568e-01
Node: 19 (pos: 0.192): 1.66212e-05, 5.01694e-10, 6.67997e-02, 4.27457e-04, 9.37498e-01, 3.66936e-01
Node: 20 (pos: 0.202): 1.12411e-04, 2.10279e-08, 1.08729e-01, 1.68222e-03, 9.60731e-01, 4.44305e-01
Node: 21 (pos: 0.212): 6.33715e-04, 6.17508e-07, 1.68955e-01, 5.81045e-03, 9.82248e-01, 5.28273e-01
Node: 22 (pos: 0.222): 2.97795e-03, 1.27051e-05, 2.50636e-01, 1.76145e-02, 1.00191e+00, 6.16769e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.09906161843314973
Step 50, mean loss 0.057189609062980845
Step 75, mean loss 0.06214461345437705
Step 100, mean loss 0.0847660484949925
Step 125, mean loss 0.10306049801571279
Step 150, mean loss 0.15365680385766523
Step 175, mean loss 0.16477902467088681
Step 200, mean loss 0.1878762068004664
Step 225, mean loss 0.24288063483101724
Unrolled forward losses 5.68237036515813
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time6121339.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.21913711268521746, [0.0012664442414807777, 0.0006283102077240033, 0.004833733965833571, 0.0017234112650124811, 0.08763701839582512, 0.0112270544409586], [1.0091928714245146, 1.1174941257542492, 1.1067018691655084, 1.1464310203501127, 1.0796132618570176, 1.1051316237796682]
Training Loss (progress: 0.08), 0.1856771662189762, [0.0012730397992564055, 0.0005562976913826401, 0.004808248994852602, 0.001673825436080804, 0.08912127079029965, 0.011788835200548712], [1.009109124327431, 1.1175429513711082, 1.1109240028356129, 1.1473882263283548, 1.0809069017421027, 1.1075454484752247]
Training Loss (progress: 0.16), 0.2080807401779594, [0.0011261539890487916, 0.0006717385070226519, 0.004939608040782789, 0.0017307730327304105, 0.09110066257516823, 0.011991047767523492], [1.008891192893902, 1.1180522225612735, 1.1123203163940747, 1.147745138792371, 1.0824380924860248, 1.1095550653814972]
Training Loss (progress: 0.24), 0.19735061727315084, [0.0013270262115921976, 0.0006760459922082518, 0.004870402926090795, 0.0018174877439061333, 0.09230071318838619, 0.011782351637740551], [1.0085216203174385, 1.1171371150877483, 1.1151257520131537, 1.1490136040087757, 1.0831504584635498, 1.1107505734416874]
Training Loss (progress: 0.32), 0.19196900964144648, [0.0012422991324798325, 0.0007194557404401356, 0.0048687719241113135, 0.001752229280125645, 0.0932503059428469, 0.012115655599242266], [1.008255533651803, 1.1168830509787282, 1.1169442711428819, 1.1484241238616673, 1.0840168364462799, 1.1124065483738697]
Training Loss (progress: 0.40), 0.19784282144813553, [0.001261158800407751, 0.0006735106225238542, 0.005025800722277305, 0.0018039677517556825, 0.0950713552471656, 0.012109353012761875], [1.0078188642579717, 1.1177631674310873, 1.1185240030822994, 1.149686858696572, 1.0852317748075906, 1.1140632806441944]
Training Loss (progress: 0.48), 0.19149134520842392, [0.00126135091763536, 0.0006454747523526458, 0.004885077814026187, 0.0017956781082735485, 0.09619888297156176, 0.01215742901381307], [1.0073225092541063, 1.1176388370668107, 1.120471522439524, 1.1500404343228812, 1.0862666749519077, 1.1156239852564513]
Training Loss (progress: 0.56), 0.19988666157056925, [0.0012635860403363015, 0.0007020924682573795, 0.004980778450796637, 0.0017474909058718075, 0.09780783448534322, 0.012223651045539734], [1.0074144254529283, 1.118135713998448, 1.123727294062947, 1.150962236677694, 1.0872523656182151, 1.1168387179985233]
Training Loss (progress: 0.64), 0.16622022202490272, [0.0012725817407322562, 0.0006690231096516897, 0.004851181545139271, 0.0017870687320769524, 0.0988887364478694, 0.012131435368027027], [1.007388777843022, 1.1182695432546739, 1.1250650314597392, 1.1520387595520745, 1.0878567173180584, 1.117972578573741]
Training Loss (progress: 0.72), 0.1918222798828148, [0.0012399621966695021, 0.0006969760808438052, 0.0050179669910646926, 0.0018370739043254598, 0.10019862941807182, 0.01228814153596686], [1.006654871442721, 1.1183165771160872, 1.1271497513905646, 1.1528189084215499, 1.088734232351842, 1.1190160402419669]
Training Loss (progress: 0.80), 0.1646991932856454, [0.00130516022332822, 0.0007294341286752062, 0.004835644739956562, 0.0018027305312835042, 0.10143440433561797, 0.012647571531486806], [1.0065210269590632, 1.1186430529983735, 1.1292387709326768, 1.1528842579337855, 1.0896424643481706, 1.120368341817671]
Training Loss (progress: 0.88), 0.17326756771179433, [0.0013476651929405956, 0.0006811115816312871, 0.004956298570961843, 0.00181593925608673, 0.1024616310840025, 0.012665915510150125], [1.0056464393338471, 1.1194449987273358, 1.1309527200780238, 1.1539904184458756, 1.0900499375735873, 1.1215921864624705]
Training Loss (progress: 0.96), 0.18256000744996143, [0.0012097703845933001, 0.0007863610891065347, 0.005129721027660538, 0.001861679172224817, 0.10351898030930069, 0.012540348283689925], [1.005370615005871, 1.1196446335609553, 1.1321373658308083, 1.153867370314789, 1.0907301981222086, 1.1225864694046601]
Evaluation on validation dataset:
Step 25, mean loss 0.04787278267557454
Step 50, mean loss 0.024718530791274618
Step 75, mean loss 0.030736515561253196
Step 100, mean loss 0.03585611906963219
Step 125, mean loss 0.04543979699329551
Step 150, mean loss 0.05281128712507988
Step 175, mean loss 0.059427147392454624
Step 200, mean loss 0.08203728863838838
Step 225, mean loss 0.11905589537919056
Unrolled forward losses 2.272403646937427
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 1.66646e-04, 1.57016e-07, 1.46450e-01, 3.81852e-03, 9.88454e-01, 4.93635e-01
Node: 14 (pos: 0.141): 2.67873e-05, 5.71228e-09, 9.53080e-02, 1.15089e-03, 9.68206e-01, 4.15383e-01
Node: 15 (pos: 0.152): 3.61789e-06, 1.51570e-10, 5.95388e-02, 3.09434e-04, 9.46505e-01, 3.43838e-01
Node: 00 (pos: 0.000): 2.49610e-01, 8.96367e-02, 8.16464e-01, 4.62741e-01, 1.07377e+00, 9.84535e-01
Node: 01 (pos: 0.010): 4.59076e-01, 2.70519e-01, 9.42159e-01, 6.90174e-01, 1.08121e+00, 1.04284e+00
Node: 02 (pos: 0.020): 7.09415e-01, 5.95455e-01, 1.04362e+00, 9.18276e-01, 1.08655e+00, 1.08658e+00
-
Node: 03 (pos: 0.030): 9.21107e-01, 9.55959e-01, 1.10967e+00, 1.08989e+00, 1.08976e+00, 1.11371e+00
Node: 05 (pos: 0.051): 9.21107e-01, 9.55959e-01, 1.10967e+00, 1.08989e+00, 1.08976e+00, 1.11371e+00
Node: 06 (pos: 0.061): 7.09415e-01, 5.95455e-01, 1.04362e+00, 9.18276e-01, 1.08655e+00, 1.08658e+00
Node: 07 (pos: 0.071): 4.59076e-01, 2.70519e-01, 9.42159e-01, 6.90174e-01, 1.08121e+00, 1.04284e+00
Node: 08 (pos: 0.081): 2.49610e-01, 8.96367e-02, 8.16464e-01, 4.62741e-01, 1.07377e+00, 9.84535e-01
Node: 09 (pos: 0.091): 1.14034e-01, 2.16627e-02, 6.79176e-01, 2.76766e-01, 1.06429e+00, 9.14338e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.14034e-01, 2.16627e-02, 6.79176e-01, 2.76766e-01, 1.06429e+00, 9.14338e-01
Node: 36 (pos: 0.364): 4.37722e-02, 3.81839e-03, 5.42325e-01, 1.47666e-01, 1.05281e+00, 8.35303e-01
Node: 37 (pos: 0.374): 1.41175e-02, 4.90894e-04, 4.15689e-01, 7.02818e-02, 1.03941e+00, 7.50659e-01
Node: 38 (pos: 0.384): 3.82569e-03, 4.60292e-05, 3.05851e-01, 2.98400e-02, 1.02416e+00, 6.63594e-01
Node: 39 (pos: 0.394): 8.71075e-04, 3.14789e-06, 2.16015e-01, 1.13019e-02, 1.00714e+00, 5.77063e-01
Node: 40 (pos: 0.404): 1.66646e-04, 1.57016e-07, 1.46450e-01, 3.81852e-03, 9.88454e-01, 4.93635e-01
-
Node: 41 (pos: 0.414): 2.67873e-05, 5.71228e-09, 9.53080e-02, 1.15089e-03, 9.68206e-01, 4.15383e-01
Node: 42 (pos: 0.424): 3.61789e-06, 1.51570e-10, 5.95388e-02, 3.09434e-04, 9.46505e-01, 3.43838e-01
Node: 19 (pos: 0.192): 2.67873e-05, 5.71228e-09, 9.53080e-02, 1.15089e-03, 9.68206e-01, 4.15383e-01
Node: 20 (pos: 0.202): 1.66646e-04, 1.57016e-07, 1.46450e-01, 3.81852e-03, 9.88454e-01, 4.93635e-01
Node: 21 (pos: 0.212): 8.71075e-04, 3.14789e-06, 2.16015e-01, 1.13019e-02, 1.00714e+00, 5.77063e-01
Node: 22 (pos: 0.222): 3.82569e-03, 4.60292e-05, 3.05851e-01, 2.98400e-02, 1.02416e+00, 6.63594e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.045309484616313116
Step 50, mean loss 0.02637080451781663
Step 75, mean loss 0.03182044077926792
Step 100, mean loss 0.04611082960165645
Step 125, mean loss 0.058250044970865424
Step 150, mean loss 0.0875406939658975
Step 175, mean loss 0.09339332169446696
Step 200, mean loss 0.13039127474858866
Step 225, mean loss 0.17367274363208496
Unrolled forward losses 2.8296579399260846
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time6121339.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.154791559061952, [0.0013378192162498811, 0.0007519148506453117, 0.005014126261195961, 0.0018943895843695558, 0.1036805725903164, 0.012265321243224537], [1.004792111924203, 1.119152334171904, 1.1330711759853942, 1.154002900620391, 1.0909215813518414, 1.122861858571091]
Training Loss (progress: 0.08), 0.17579275714929143, [0.0012811304223710501, 0.0006646630336479311, 0.005006292189496946, 0.0018055716820241747, 0.10484932742534452, 0.012658233298889746], [1.0050785868522425, 1.1195685964472535, 1.1347491977598256, 1.1547784696755448, 1.0917056940728014, 1.1241620508039234]
Training Loss (progress: 0.16), 0.15697050359666248, [0.0013145598166793143, 0.0006865835577913656, 0.00515026874486441, 0.0017601467081560375, 0.10616034904611703, 0.012620984892303216], [1.0037660715619718, 1.119567020749921, 1.1366918555295513, 1.1552845869230202, 1.0923346784698982, 1.1251892988993935]
Training Loss (progress: 0.24), 0.17838947160183977, [0.0012478373748105967, 0.0007074716951026702, 0.005041897397358487, 0.0018993107580112596, 0.1073796954165487, 0.01272809129242666], [1.0036106991443394, 1.1199033125625495, 1.13839058297898, 1.1563326987182199, 1.0931165030266938, 1.1265580415319798]
Training Loss (progress: 0.32), 0.18582305065010565, [0.001295392915756752, 0.000704586062483109, 0.00525967374652967, 0.0019225546979961129, 0.10818947848375898, 0.01256205030731674], [1.0036487146798332, 1.1198531501175992, 1.140583417853858, 1.156473546731334, 1.0937092468091925, 1.127443346527983]
Training Loss (progress: 0.40), 0.15435816939538152, [0.0012838767575224376, 0.0006831977686695313, 0.005061830065432065, 0.0018559907702253163, 0.10900347867093572, 0.012704289368411791], [1.002846400528742, 1.1200599964205125, 1.1421117796170837, 1.1568430748389844, 1.0941206376124315, 1.128635947400394]
Training Loss (progress: 0.48), 0.18535022498752604, [0.0012925111740830528, 0.0006944815556919469, 0.005174512034418787, 0.0018402385740727426, 0.11022286785269529, 0.012662912552707021], [1.0018891363080225, 1.1198383607670603, 1.1440177977723234, 1.158040693053132, 1.0949906783230443, 1.1298613841981044]
Training Loss (progress: 0.56), 0.16615759796196664, [0.0013386375207730607, 0.000766271643164793, 0.005207701776446063, 0.0019166505995317992, 0.11106680694241036, 0.012802656791376358], [1.0012805069789723, 1.1196117143172057, 1.1454275849220026, 1.1588349443648545, 1.0952187393268846, 1.1309123137868442]
Training Loss (progress: 0.64), 0.15070960694822505, [0.00137574001231857, 0.0006919731039902686, 0.0050328507985636685, 0.0018586852408816361, 0.11203712166632288, 0.012610093768644736], [1.0009866588474774, 1.1199705923276564, 1.146514013946868, 1.1596027588067677, 1.0957523115140178, 1.1312622517256514]
Training Loss (progress: 0.72), 0.1606577724953401, [0.0013688615469183004, 0.0008845482345000962, 0.005315106303736793, 0.001764085127961589, 0.1130919032534305, 0.012777808912154605], [1.0000149404315304, 1.1199015788038538, 1.148414340407048, 1.1594570656768246, 1.0964160982468572, 1.1328328305973832]
Training Loss (progress: 0.80), 0.16036232814668014, [0.0013319168968871434, 0.0006639887812010213, 0.005372971313039528, 0.001888973749208305, 0.11424542628042546, 0.012711081274109077], [0.9992540296299721, 1.1195273113961317, 1.1500966261610057, 1.1602466700519272, 1.0969261584773442, 1.134039774112213]
Training Loss (progress: 0.88), 0.1685334666860487, [0.0014005570790982143, 0.000719549147568319, 0.0053702058876116615, 0.00193463357872774, 0.11529828707102162, 0.012775612658977361], [0.9986415895007081, 1.1192021456862, 1.1516197174711347, 1.1606186809846557, 1.0974525175231393, 1.1352284334692964]
Training Loss (progress: 0.96), 0.16761303475990216, [0.0013719186235610773, 0.0007166707425109743, 0.00526619472586859, 0.001891411440373389, 0.11644092036898193, 0.012669138745466157], [0.998217018674727, 1.1192247122240788, 1.1533901243075888, 1.1604684254590074, 1.0979677453351198, 1.1359769401594668]
Evaluation on validation dataset:
Step 25, mean loss 0.04419105266312875
Step 50, mean loss 0.01697050573150236
Step 75, mean loss 0.02619642244551929
Step 100, mean loss 0.034307513093788725
Step 125, mean loss 0.03980869946163856
Step 150, mean loss 0.046129531736037205
Step 175, mean loss 0.04997688862773281
Step 200, mean loss 0.06900536447472264
Step 225, mean loss 0.09201583159532715
Unrolled forward losses 2.082053362938167
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.39161e-04, 5.96196e-08, 1.74146e-01, 6.55861e-03, 1.00635e+00, 5.04664e-01
Node: 14 (pos: 0.141): 6.33803e-05, 1.76967e-09, 1.17063e-01, 2.21170e-03, 9.88062e-01, 4.25574e-01
Node: 15 (pos: 0.152): 1.00955e-05, 3.75770e-11, 7.57697e-02, 6.72484e-04, 9.68413e-01, 3.53100e-01
Node: 00 (pos: 0.000): 2.78106e-01, 7.68019e-02, 8.52888e-01, 5.07166e-01, 1.08295e+00, 9.97956e-01
Node: 01 (pos: 0.010): 4.86438e-01, 2.48049e-01, 9.73621e-01, 7.28638e-01, 1.08959e+00, 1.05630e+00
Node: 02 (pos: 0.020): 7.25217e-01, 5.73095e-01, 1.07019e+00, 9.43871e-01, 1.09436e+00, 1.10005e+00
-
Node: 03 (pos: 0.030): 9.21579e-01, 9.47197e-01, 1.13267e+00, 1.10244e+00, 1.09723e+00, 1.12717e+00
Node: 05 (pos: 0.051): 9.21579e-01, 9.47197e-01, 1.13267e+00, 1.10244e+00, 1.09723e+00, 1.12717e+00
Node: 06 (pos: 0.061): 7.25217e-01, 5.73095e-01, 1.07019e+00, 9.43871e-01, 1.09436e+00, 1.10005e+00
Node: 07 (pos: 0.071): 4.86438e-01, 2.48049e-01, 9.73621e-01, 7.28638e-01, 1.08959e+00, 1.05630e+00
Node: 08 (pos: 0.081): 2.78106e-01, 7.68019e-02, 8.52888e-01, 5.07166e-01, 1.08295e+00, 9.97956e-01
Node: 09 (pos: 0.091): 1.35524e-01, 1.70110e-02, 7.19392e-01, 3.18294e-01, 1.07447e+00, 9.27653e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.35524e-01, 1.70110e-02, 7.19392e-01, 3.18294e-01, 1.07447e+00, 9.27653e-01
Node: 36 (pos: 0.364): 5.62921e-02, 2.69534e-03, 5.84267e-01, 1.80113e-01, 1.06420e+00, 8.48417e-01
Node: 37 (pos: 0.374): 1.99297e-02, 3.05507e-04, 4.56908e-01, 9.18973e-02, 1.05219e+00, 7.63454e-01
Node: 38 (pos: 0.384): 6.01421e-03, 2.47715e-05, 3.44047e-01, 4.22766e-02, 1.03849e+00, 6.75937e-01
Node: 39 (pos: 0.394): 1.54696e-03, 1.43684e-06, 2.49448e-01, 1.75362e-02, 1.02319e+00, 5.88816e-01
Node: 40 (pos: 0.404): 3.39161e-04, 5.96196e-08, 1.74146e-01, 6.55861e-03, 1.00635e+00, 5.04664e-01
-
Node: 41 (pos: 0.414): 6.33803e-05, 1.76967e-09, 1.17063e-01, 2.21170e-03, 9.88062e-01, 4.25574e-01
Node: 42 (pos: 0.424): 1.00955e-05, 3.75770e-11, 7.57697e-02, 6.72484e-04, 9.68413e-01, 3.53100e-01
Node: 19 (pos: 0.192): 6.33803e-05, 1.76967e-09, 1.17063e-01, 2.21170e-03, 9.88062e-01, 4.25574e-01
Node: 20 (pos: 0.202): 3.39161e-04, 5.96196e-08, 1.74146e-01, 6.55861e-03, 1.00635e+00, 5.04664e-01
Node: 21 (pos: 0.212): 1.54696e-03, 1.43684e-06, 2.49448e-01, 1.75362e-02, 1.02319e+00, 5.88816e-01
Node: 22 (pos: 0.222): 6.01421e-03, 2.47715e-05, 3.44047e-01, 4.22766e-02, 1.03849e+00, 6.75937e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.04164276398607124
Step 50, mean loss 0.01932560223825211
Step 75, mean loss 0.027323943439101474
Step 100, mean loss 0.03795922058867421
Step 125, mean loss 0.046787952871964335
Step 150, mean loss 0.0662327419608564
Step 175, mean loss 0.07646814913378133
Step 200, mean loss 0.09755366208891751
Step 225, mean loss 0.1427025044869355
Unrolled forward losses 2.610844757267291
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time6121339.tar

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.17159714964765044, [0.001314160536585288, 0.0006694268723892825, 0.005392751721681787, 0.0019688995522048613, 0.11683868654689711, 0.012662902785239842], [0.9981870817076554, 1.1196096637485784, 1.1545324269118409, 1.1609746483968113, 1.098184311777165, 1.136513237414152]
Training Loss (progress: 0.08), 0.1682618800296979, [0.0013140844197541566, 0.0007370497412736265, 0.005338718629688128, 0.0018836074876449483, 0.11792921436619849, 0.01278842236776293], [0.9970933752144966, 1.1191204781667972, 1.1554982860609793, 1.1615221729449907, 1.0987504127330163, 1.1376758345099804]
Training Loss (progress: 0.16), 0.15702022904063714, [0.001318135425277903, 0.0006636701500146334, 0.00522560903115147, 0.0018563680085710971, 0.11874967670839427, 0.012804182603383727], [0.9963498187767015, 1.120145517342414, 1.1566457356283857, 1.1621398369697409, 1.0990412192151864, 1.1388742082972827]
Training Loss (progress: 0.24), 0.17286419493029662, [0.0013342711021966805, 0.000660578222472138, 0.00547189755730526, 0.0019182523061596047, 0.11958841800663195, 0.012685672937007597], [0.9960291883266923, 1.1193394463399904, 1.1577979590745207, 1.1623268544347674, 1.099464989362759, 1.1396182625498235]
Training Loss (progress: 0.32), 0.16608115730521228, [0.0013727030751147793, 0.0008563961654974987, 0.00544137336599198, 0.0018362320928278358, 0.12083787333425008, 0.012832592999430882], [0.9953146636794318, 1.1191125575530319, 1.1592014581458574, 1.1638062183430142, 1.100112640946515, 1.1407325424966748]
Training Loss (progress: 0.40), 0.15880632217040416, [0.0012600962010663706, 0.000684811470486808, 0.005219165785879109, 0.00193464520044134, 0.12146381356783247, 0.012777858469836364], [0.9945771181530485, 1.1191685421560846, 1.1606204629224122, 1.1632419121238995, 1.1003564033635689, 1.141752997501901]
Training Loss (progress: 0.48), 0.14811360505561985, [0.0013387499196187657, 0.0007612875244255875, 0.00552779475129062, 0.0019365973312388343, 0.12243254782064412, 0.012607706791794902], [0.9942992871015113, 1.1193076236280084, 1.1619696598407745, 1.1641288760571387, 1.1006825750497802, 1.1423626020312971]
Training Loss (progress: 0.56), 0.1498897302372231, [0.0013655150902201567, 0.0007948952907001505, 0.005424759708197471, 0.0019149123433057475, 0.12383703023001903, 0.012808261500324204], [0.9934131922385452, 1.1194634048053966, 1.1636661592446285, 1.164284613172455, 1.1010293040674017, 1.1438049256079856]
Training Loss (progress: 0.64), 0.1588137424570172, [0.001398670532131656, 0.000695316214285047, 0.00573847403395994, 0.0019476925644308558, 0.12474252051014792, 0.012687151165629771], [0.9934689266528602, 1.1194863650484792, 1.1647473017748786, 1.164575267533159, 1.101497732102289, 1.1447177037321365]
Training Loss (progress: 0.72), 0.15549263422764054, [0.0013458667786284536, 0.000652002619180791, 0.005411899876361223, 0.0019861847163439405, 0.1257818128179753, 0.012792102840239863], [0.9925705435157443, 1.119391412290156, 1.1660171233186118, 1.1655292789915774, 1.1016231965228889, 1.145533029508184]
Training Loss (progress: 0.80), 0.17272997068570364, [0.001384831220754026, 0.0006895564767668142, 0.0056097889602872856, 0.0019178413418495226, 0.12680809505728552, 0.012877627499156353], [0.9917079622106449, 1.119296505797398, 1.1682108424162045, 1.1652032908435757, 1.1021172830727892, 1.1464740177973782]
Training Loss (progress: 0.88), 0.16594049899335064, [0.0013364574933061053, 0.0007334292681936888, 0.005370115989651153, 0.0019361984640874491, 0.1277989234709376, 0.01292316929788822], [0.9911060550342905, 1.1187229915797163, 1.1692337574507186, 1.1653898632653445, 1.1024877807039748, 1.1477575055031553]
Training Loss (progress: 0.96), 0.15094758780659476, [0.0012780147185061422, 0.0006324113468047773, 0.0055103589965754, 0.00197100372528517, 0.12894978347818112, 0.013019489020231584], [0.9902813036698683, 1.118793993072684, 1.1702040315943212, 1.1656049089104294, 1.1029227724548236, 1.1483858009105394]
Evaluation on validation dataset:
Step 25, mean loss 0.04540746951969857
Step 50, mean loss 0.030141982996974973
Step 75, mean loss 0.030635470970456255
Step 100, mean loss 0.03431712732056673
Step 125, mean loss 0.03956970187847262
Step 150, mean loss 0.046280590836232705
Step 175, mean loss 0.051658280556892505
Step 200, mean loss 0.06982186151278806
Step 225, mean loss 0.09700490998506398
Unrolled forward losses 2.591120199846488
Unrolled forward base losses 1.1720234445357585
Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.1246279850534986, [0.0013184598546191416, 0.0006959200688838881, 0.00556305305573369, 0.0019356853008792945, 0.12966710841920331, 0.013048275453402832], [0.9896588939272828, 1.1185728472513643, 1.1711621295899513, 1.166124117923876, 1.103212101810628, 1.1491433972543497]
Training Loss (progress: 0.08), 0.1371948168479885, [0.0013390674783654978, 0.0007314127827697211, 0.0054340070374515465, 0.0018921920695744844, 0.13013318444911143, 0.013009214250393473], [0.9896405681168265, 1.1186926705909797, 1.1725098606801065, 1.166232386402969, 1.103616908188192, 1.1499495835457507]
Training Loss (progress: 0.16), 0.13434034068188236, [0.0013295957169011886, 0.0007061434143558397, 0.005489690451735661, 0.00196307145177466, 0.13059952720587314, 0.012934508130658561], [0.9897556647116902, 1.1187748265875617, 1.173279257007805, 1.1664607359176196, 1.1040117782525416, 1.150598138228093]
Training Loss (progress: 0.24), 0.14145148149299394, [0.0013410004808191902, 0.0007175971670512265, 0.005537693132633073, 0.0019490790858479189, 0.13095133295948264, 0.013098432748483378], [0.9899291754154387, 1.118843176948071, 1.174061706023436, 1.1663198868554299, 1.1043169916083004, 1.151350738695461]
Training Loss (progress: 0.32), 0.1287060522699891, [0.0012940116040228726, 0.000752541321048273, 0.005606608765850727, 0.001959985466545071, 0.13126240097211747, 0.013056835187911887], [0.9896124485291478, 1.1191497118540694, 1.1751853557944316, 1.1668257422047486, 1.1047209141971748, 1.1522018942962469]
