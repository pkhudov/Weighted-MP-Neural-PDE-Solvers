Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar
Number of parameters: 1031657.0
Saved initial model at models/init5191557.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
Node: 01 (pos: 0.010): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 02 (pos: 0.020): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 03 (pos: 0.030): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 04 (pos: 0.040): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 05 (pos: 0.051): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
-
Node: 07 (pos: 0.071): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 08 (pos: 0.081): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 09 (pos: 0.091): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 10 (pos: 0.101): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 11 (pos: 0.111): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 12 (pos: 0.121): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 58 (pos: 0.586): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 59 (pos: 0.596): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 60 (pos: 0.606): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 61 (pos: 0.616): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 50 (pos: 0.505): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
-
Node: 51 (pos: 0.515): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 52 (pos: 0.525): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 53 (pos: 0.535): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 54 (pos: 0.545): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 55 (pos: 0.556): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 62 (pos: 0.626): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.202298847179997, [0.011357369046919575, 0.019448653108507203, 0.020083818460890603, 0.01866320242934453, 0.019431452437569205, 0.01910033513061819], [1.0046933085248801, 1.013024525923481, 1.013089216961415, 1.011944000816271, 1.0126270495788525, 1.0122483734588368]
Training Loss (progress: 0.08), 0.2533837073248277, [0.015916804420992935, 0.0004942094800948446, 0.0014054647690761887, 0.0011494005265336184, 0.007577445928671694, 0.01795078527110668], [1.0070046059183613, 1.0321692237900302, 1.0256119500515328, 1.0256534337426393, 1.0277887836414625, 1.0370661473470952]
Training Loss (progress: 0.16), 0.18395275944544404, [0.014057473580151178, 0.00042029196014225275, 0.001731415013035131, 0.0012731378629981395, 0.00850384402448646, 0.022809838768625138], [1.0045057650631497, 1.040718533744714, 1.0322513869044923, 1.0292833623082474, 1.033759438391671, 1.0455477195358787]
Training Loss (progress: 0.24), 0.1663058043916059, [0.012778901902898541, 0.0004513204201015951, 0.001864764117927014, 0.0013861798400436282, 0.008950766325116875, 0.027172217786409852], [1.0017620913347223, 1.0456686664393842, 1.0389587989643903, 1.0314409626413814, 1.038572380147723, 1.0521044936848944]
Training Loss (progress: 0.32), 0.15583802144666095, [0.012035448387115871, 0.0004526332058363237, 0.001726903318299616, 0.0012306574664295174, 0.009744763248974669, 0.030514192667177958], [0.9991061775184006, 1.0496887547706721, 1.045570628096475, 1.0339669313238649, 1.043379044160905, 1.0570657808412636]
Training Loss (progress: 0.40), 0.14326396108276043, [0.011145499541591612, 0.0003949162966546304, 0.0019682890385158804, 0.0014202461903743244, 0.009135147942589475, 0.0326801546763496], [0.9964501728342, 1.052744580868031, 1.0518254250871972, 1.0370125885525243, 1.0470182664642158, 1.0610923945239636]
Training Loss (progress: 0.48), 0.13141517055926025, [0.010582875234080184, 0.0004915331801755332, 0.0021321176992621536, 0.0014363756221929857, 0.009378956934712213, 0.03529851734694915], [0.9938288693081172, 1.0552143350393512, 1.0580926838234548, 1.0399986137532309, 1.051001035596778, 1.0649182232804166]
Training Loss (progress: 0.56), 0.12830130366509593, [0.009931490648703623, 0.0003760513157728014, 0.002058213192065292, 0.001342877934291718, 0.00927013526199502, 0.03644579654571977], [0.9909652773445913, 1.0568689747391724, 1.0633578297169817, 1.0432243650297481, 1.0545320985791413, 1.0680020990611307]
Training Loss (progress: 0.64), 0.1092535208470142, [0.009417514180771499, 0.0004452981589141781, 0.0021650409497566835, 0.0014718303613042666, 0.008857106770134824, 0.03820743589924773], [0.9884075772141088, 1.058220875397089, 1.0680162934144797, 1.0463809976940874, 1.0573856198037486, 1.0716198659265659]
Training Loss (progress: 0.72), 0.11259093011670666, [0.008872529484101725, 0.0005025884499179189, 0.0020057950812252708, 0.0014310722481102275, 0.008475165442800072, 0.03955880873450088], [0.9855623083107997, 1.058467472697269, 1.07240430053795, 1.049170520910056, 1.060723165559366, 1.0743935013377783]
Training Loss (progress: 0.80), 0.10251247271433003, [0.008570460745216222, 0.000585731145222927, 0.0023090342386705067, 0.0014331923141737283, 0.008618494528670664, 0.04110009939501867], [0.9828597863226106, 1.0599439061496825, 1.0754787093361755, 1.0523831661327323, 1.0635370656925038, 1.0771145965897846]
Training Loss (progress: 0.88), 0.09365800688138065, [0.008461499468369701, 0.0005163793124130435, 0.0025021617715499933, 0.0013500140756823227, 0.00837014131231333, 0.042178274293372425], [0.9801930788227655, 1.061325206975969, 1.0797733393798827, 1.0560714228463055, 1.0659718487294696, 1.0800607156233042]
Training Loss (progress: 0.96), 0.09946094826283136, [0.008220612266216737, 0.0004946991253126232, 0.002067276880535224, 0.0013072746420256937, 0.007969694583683306, 0.04374435482458979], [0.9776012709323891, 1.0609767230907556, 1.0827834865420551, 1.0589877332825146, 1.0682892645257556, 1.0831345540357291]
Evaluation on validation dataset:
Step 25, mean loss 0.1064153584998552
Step 50, mean loss 0.11753091828864809
Step 75, mean loss 0.15512581514669332
Step 100, mean loss 0.2905385516425927
Step 125, mean loss 0.2723488348491392
Step 150, mean loss 0.19484440145995485
Step 175, mean loss 0.28484598255252347
Step 200, mean loss 0.4845368580796556
Step 225, mean loss 0.38719743334237355
Unrolled forward losses 14.505937151242456
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 6.15523e-01, 2.28367e-03, 2.00842e-01, 6.12405e-02, 6.85408e-01, 9.98338e-01
Node: 01 (pos: 0.010): 7.08701e-01, 1.49170e-02, 3.36210e-01, 1.46348e-01, 7.85333e-01, 1.02392e+00
Node: 02 (pos: 0.020): 7.95336e-01, 6.92688e-02, 5.12490e-01, 2.98499e-01, 8.77834e-01, 1.04533e+00
Node: 03 (pos: 0.030): 8.69977e-01, 2.28668e-01, 7.11338e-01, 5.19647e-01, 9.57248e-01, 1.06230e+00
Node: 04 (pos: 0.040): 9.27543e-01, 5.36637e-01, 8.99050e-01, 7.72120e-01, 1.01833e+00, 1.07458e+00
Node: 05 (pos: 0.051): 9.63895e-01, 8.95295e-01, 1.03469e+00, 9.79197e-01, 1.05684e+00, 1.08202e+00
-
Node: 07 (pos: 0.071): 9.63895e-01, 8.95295e-01, 1.03469e+00, 9.79197e-01, 1.05684e+00, 1.08202e+00
Node: 08 (pos: 0.081): 9.27543e-01, 5.36637e-01, 8.99050e-01, 7.72120e-01, 1.01833e+00, 1.07458e+00
Node: 09 (pos: 0.091): 8.69977e-01, 2.28668e-01, 7.11338e-01, 5.19647e-01, 9.57248e-01, 1.06230e+00
Node: 10 (pos: 0.101): 7.95336e-01, 6.92688e-02, 5.12490e-01, 2.98499e-01, 8.77834e-01, 1.04533e+00
Node: 11 (pos: 0.111): 7.08701e-01, 1.49170e-02, 3.36210e-01, 1.46348e-01, 7.85333e-01, 1.02392e+00
Node: 12 (pos: 0.121): 6.15523e-01, 2.28367e-03, 2.00842e-01, 6.12405e-02, 6.85408e-01, 9.98338e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.63895e-01, 8.95295e-01, 1.03469e+00, 9.79197e-01, 1.05684e+00, 1.08202e+00
Node: 58 (pos: 0.586): 9.27543e-01, 5.36637e-01, 8.99050e-01, 7.72120e-01, 1.01833e+00, 1.07458e+00
Node: 59 (pos: 0.596): 8.69977e-01, 2.28668e-01, 7.11338e-01, 5.19647e-01, 9.57248e-01, 1.06230e+00
Node: 60 (pos: 0.606): 7.95336e-01, 6.92688e-02, 5.12490e-01, 2.98499e-01, 8.77834e-01, 1.04533e+00
Node: 61 (pos: 0.616): 7.08701e-01, 1.49170e-02, 3.36210e-01, 1.46348e-01, 7.85333e-01, 1.02392e+00
Node: 50 (pos: 0.505): 6.15523e-01, 2.28367e-03, 2.00842e-01, 6.12405e-02, 6.85408e-01, 9.98338e-01
-
Node: 51 (pos: 0.515): 7.08701e-01, 1.49170e-02, 3.36210e-01, 1.46348e-01, 7.85333e-01, 1.02392e+00
Node: 52 (pos: 0.525): 7.95336e-01, 6.92688e-02, 5.12490e-01, 2.98499e-01, 8.77834e-01, 1.04533e+00
Node: 53 (pos: 0.535): 8.69977e-01, 2.28668e-01, 7.11338e-01, 5.19647e-01, 9.57248e-01, 1.06230e+00
Node: 54 (pos: 0.545): 9.27543e-01, 5.36637e-01, 8.99050e-01, 7.72120e-01, 1.01833e+00, 1.07458e+00
Node: 55 (pos: 0.556): 9.63895e-01, 8.95295e-01, 1.03469e+00, 9.79197e-01, 1.05684e+00, 1.08202e+00
Node: 62 (pos: 0.626): 6.15523e-01, 2.28367e-03, 2.00842e-01, 6.12405e-02, 6.85408e-01, 9.98338e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.09807585850427519
Step 50, mean loss 0.1070731364119388
Step 75, mean loss 0.13953603421331473
Step 100, mean loss 0.14560677490366092
Step 125, mean loss 0.3231446515349443
Step 150, mean loss 0.17275437205006008
Step 175, mean loss 0.3405790232571019
Step 200, mean loss 0.3787724087813425
Step 225, mean loss 0.263609867436457
Unrolled forward losses 12.840621110173213
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.19596445501842477, [0.008138643251842563, 0.000471000757600868, 0.002770465560746841, 0.0013961907335260045, 0.008308119032244016, 0.044809609486346734], [0.9764058418262914, 1.0603960408245676, 1.084742823459392, 1.0596783938294931, 1.0700090582765391, 1.0849062785880714]
Training Loss (progress: 0.08), 0.1909402709285082, [0.008817548435274198, 0.000605930937148905, 0.0028012747309083016, 0.0014381405991779877, 0.008949155821973618, 0.04994167738912825], [0.9742961239327754, 1.0612148624060034, 1.0868271424909803, 1.0626853827990614, 1.0721116960709296, 1.0886753787904138]
Training Loss (progress: 0.16), 0.19904425148441385, [0.009642066228091252, 0.0005901872058481617, 0.003670921201838174, 0.0014767655270292704, 0.009980212910062307, 0.05481074123491025], [0.9723313014105023, 1.0601529792206916, 1.0913842354116388, 1.0640364971254859, 1.0760292238762716, 1.0931242170617592]
Training Loss (progress: 0.24), 0.19636101027424974, [0.010349769233958837, 0.0005321734408471426, 0.003464708548619872, 0.001480496739223617, 0.010681391279486712, 0.05860930834925067], [0.9701593709339829, 1.0596493033556715, 1.0936613186754185, 1.0653801403968737, 1.0776546841788233, 1.0960437787258934]
Training Loss (progress: 0.32), 0.1793080000755589, [0.010961831832409837, 0.0005863081547928449, 0.003909208072929583, 0.0015163748272380112, 0.010425073595488865, 0.061320001288514586], [0.9686024286605748, 1.0581414352909047, 1.0962545978652543, 1.0655232679953206, 1.0787284476893602, 1.0984371154717438]
Training Loss (progress: 0.40), 0.18814343229828115, [0.011588886240051409, 0.0005407521741768576, 0.00430279093975869, 0.0015441114348135425, 0.011360096485689324, 0.06545855341759027], [0.9669606598227786, 1.057685458576653, 1.1015013082674623, 1.0670507671662655, 1.0816118458288926, 1.1021761407910273]
Training Loss (progress: 0.48), 0.18463529635157705, [0.012401890804340968, 0.0005879741015511464, 0.0044661805306106, 0.0016029558602606326, 0.01154957917202824, 0.06758761100937245], [0.9658103204946163, 1.0567270582624784, 1.1048261622421642, 1.067611181503338, 1.083448726815567, 1.1042673164890986]
Training Loss (progress: 0.56), 0.176969158412345, [0.013419686336356287, 0.0006858762850525064, 0.004650512810926931, 0.0016578481892198036, 0.0121619227085272, 0.0701225889348917], [0.9640190508656239, 1.0549971464054986, 1.1084743853450427, 1.0678949443069583, 1.085732701233104, 1.1065251347897975]
Training Loss (progress: 0.64), 0.1570232385288797, [0.013770727494980792, 0.0005187078470117784, 0.004353082475415284, 0.0015801001202844364, 0.01186416164039564, 0.07242670694517181], [0.9622762122004707, 1.0536585387039623, 1.1102230102378259, 1.0680854632487329, 1.0872576271654444, 1.1080808195306278]
Training Loss (progress: 0.72), 0.16133589890824646, [0.014323921553542328, 0.0006843418533903443, 0.005564552628126009, 0.001533715187729398, 0.012001719411881507, 0.07485414630684194], [0.9603337673137341, 1.0519288473717314, 1.1143780405297248, 1.0685331577301174, 1.088101197684994, 1.1098945106189735]
Training Loss (progress: 0.80), 0.17105950573869075, [0.014615302662578156, 0.0006472514689632451, 0.005235695492958886, 0.0015710117349869083, 0.012020468143575545, 0.07601529465589024], [0.9582411731427602, 1.0505922113631911, 1.1163876362298095, 1.0686878573315295, 1.0901997178453104, 1.1109183996423408]
Training Loss (progress: 0.88), 0.17315401143370135, [0.015009889217465799, 0.0005807215567910147, 0.0058633788620934815, 0.0015296954128268833, 0.011601563452517153, 0.07702202511680124], [0.9564539252223199, 1.050207602447935, 1.118981817311482, 1.0680716748620633, 1.0917740804621092, 1.1119567677001376]
Training Loss (progress: 0.96), 0.15332877727295605, [0.015000476750841749, 0.0005716804511798171, 0.005878465024531417, 0.001705097587125828, 0.011234888667736639, 0.07749034503910567], [0.9536245299031763, 1.047755271428435, 1.1201825737412447, 1.0671986232097508, 1.0922584413755456, 1.1118633598885133]
Evaluation on validation dataset:
Step 25, mean loss 0.08578579025450123
Step 50, mean loss 0.057656268301476424
Step 75, mean loss 0.0923238665972882
Step 100, mean loss 0.0942985191264151
Step 125, mean loss 0.09621481608106831
Step 150, mean loss 0.08402087997721662
Step 175, mean loss 0.13557921438446327
Step 200, mean loss 0.45909579593081107
Step 225, mean loss 0.2992200340065547
Unrolled forward losses 3.8882849531540566
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.48358e-01, 1.93413e-03, 6.15629e-01, 1.01671e-01, 7.97082e-01, 1.06252e+00
Node: 01 (pos: 0.010): 8.05616e-01, 1.32356e-02, 7.39466e-01, 2.08536e-01, 8.77907e-01, 1.07779e+00
Node: 02 (pos: 0.020): 8.55707e-01, 6.38468e-02, 8.59103e-01, 3.75353e-01, 9.50096e-01, 1.09045e+00
Node: 03 (pos: 0.030): 8.96810e-01, 2.17104e-01, 9.65383e-01, 5.92892e-01, 1.01032e+00, 1.10040e+00
Node: 04 (pos: 0.040): 9.27373e-01, 5.20393e-01, 1.04926e+00, 8.21839e-01, 1.05567e+00, 1.10756e+00
Node: 05 (pos: 0.051): 9.46208e-01, 8.79284e-01, 1.10304e+00, 9.99709e-01, 1.08384e+00, 1.11188e+00
-
Node: 07 (pos: 0.071): 9.46208e-01, 8.79284e-01, 1.10304e+00, 9.99709e-01, 1.08384e+00, 1.11188e+00
Node: 08 (pos: 0.081): 9.27373e-01, 5.20393e-01, 1.04926e+00, 8.21839e-01, 1.05567e+00, 1.10756e+00
Node: 09 (pos: 0.091): 8.96810e-01, 2.17104e-01, 9.65383e-01, 5.92892e-01, 1.01032e+00, 1.10040e+00
Node: 10 (pos: 0.101): 8.55707e-01, 6.38468e-02, 8.59103e-01, 3.75353e-01, 9.50096e-01, 1.09045e+00
Node: 11 (pos: 0.111): 8.05616e-01, 1.32356e-02, 7.39466e-01, 2.08536e-01, 8.77907e-01, 1.07779e+00
Node: 12 (pos: 0.121): 7.48358e-01, 1.93413e-03, 6.15629e-01, 1.01671e-01, 7.97082e-01, 1.06252e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.46208e-01, 8.79284e-01, 1.10304e+00, 9.99709e-01, 1.08384e+00, 1.11188e+00
Node: 58 (pos: 0.586): 9.27373e-01, 5.20393e-01, 1.04926e+00, 8.21839e-01, 1.05567e+00, 1.10756e+00
Node: 59 (pos: 0.596): 8.96810e-01, 2.17104e-01, 9.65383e-01, 5.92892e-01, 1.01032e+00, 1.10040e+00
Node: 60 (pos: 0.606): 8.55707e-01, 6.38468e-02, 8.59103e-01, 3.75353e-01, 9.50096e-01, 1.09045e+00
Node: 61 (pos: 0.616): 8.05616e-01, 1.32356e-02, 7.39466e-01, 2.08536e-01, 8.77907e-01, 1.07779e+00
Node: 50 (pos: 0.505): 7.48358e-01, 1.93413e-03, 6.15629e-01, 1.01671e-01, 7.97082e-01, 1.06252e+00
-
Node: 51 (pos: 0.515): 8.05616e-01, 1.32356e-02, 7.39466e-01, 2.08536e-01, 8.77907e-01, 1.07779e+00
Node: 52 (pos: 0.525): 8.55707e-01, 6.38468e-02, 8.59103e-01, 3.75353e-01, 9.50096e-01, 1.09045e+00
Node: 53 (pos: 0.535): 8.96810e-01, 2.17104e-01, 9.65383e-01, 5.92892e-01, 1.01032e+00, 1.10040e+00
Node: 54 (pos: 0.545): 9.27373e-01, 5.20393e-01, 1.04926e+00, 8.21839e-01, 1.05567e+00, 1.10756e+00
Node: 55 (pos: 0.556): 9.46208e-01, 8.79284e-01, 1.10304e+00, 9.99709e-01, 1.08384e+00, 1.11188e+00
Node: 62 (pos: 0.626): 7.48358e-01, 1.93413e-03, 6.15629e-01, 1.01671e-01, 7.97082e-01, 1.06252e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.06481581469830422
Step 50, mean loss 0.04152761246454127
Step 75, mean loss 0.058856056538629895
Step 100, mean loss 0.06529379527601077
Step 125, mean loss 0.10893130754423919
Step 150, mean loss 0.10098781263178438
Step 175, mean loss 0.12061718609100422
Step 200, mean loss 0.17586025122758775
Step 225, mean loss 0.14950192381685146
Unrolled forward losses 3.287208379688251
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.20344666795726005, [0.01530865160665348, 0.000627586265248329, 0.006160500611820922, 0.001531534447603495, 0.011696436189604276, 0.07861855272800637], [0.9525021015835273, 1.0472184324268334, 1.1219301033074534, 1.0672389291368936, 1.0938619201684783, 1.1132756986831367]
Training Loss (progress: 0.08), 0.23740513910399638, [0.015818271379888622, 0.0006613064432055327, 0.0063886993577884, 0.0015053242171564772, 0.0118508821160695, 0.08011902909230996], [0.9527126551958638, 1.0466194094291776, 1.123735694338429, 1.067181690219146, 1.0949327684395993, 1.11485390943186]
Training Loss (progress: 0.16), 0.20438586454911298, [0.015811735624611925, 0.0006086724154320387, 0.006740158068746724, 0.0015431843478899013, 0.012167037572777406, 0.08136177215316802], [0.9523891576199929, 1.0465731848516309, 1.1251852795885322, 1.0673504019050948, 1.0958940424308803, 1.1161033248110412]
Training Loss (progress: 0.24), 0.21810205402562297, [0.015878739160618365, 0.0006469729052244198, 0.0066289478496254816, 0.0015101762479753159, 0.012465440575935991, 0.08268260135219106], [0.9519753226271772, 1.0461255248053418, 1.1263978245099426, 1.067372927270558, 1.096882042107327, 1.1173941257790307]
Training Loss (progress: 0.32), 0.17996872218663265, [0.015851571192313334, 0.0006471772207112771, 0.0068172065185643895, 0.0015262602733569503, 0.012462233584076297, 0.08352116220915974], [0.9513848189673181, 1.045748180703861, 1.12732285117971, 1.067198820883693, 1.0973743746981122, 1.1183065210784109]
Training Loss (progress: 0.40), 0.18897857678410107, [0.015962904666805163, 0.0006074104098563721, 0.006581075095570785, 0.0015581651572278712, 0.012698817117126466, 0.08410408651888629], [0.9506625581769461, 1.045194078826101, 1.1286795081229215, 1.0671789716736964, 1.098150854140493, 1.1189490046482302]
Training Loss (progress: 0.48), 0.19009387350557824, [0.01596194641938157, 0.0007048239610948096, 0.007281996266632449, 0.0015050640225080206, 0.01291176421276999, 0.08485243236401062], [0.9501758406092895, 1.0447043391765234, 1.1311805158342823, 1.067399279497599, 1.0995200669307224, 1.1198338888374555]
Training Loss (progress: 0.56), 0.21523299548997896, [0.01611959697283324, 0.0005735497022209823, 0.007171539014176859, 0.0016665136093978205, 0.01300102070961564, 0.08549902081371316], [0.9497374592554669, 1.0439941302293059, 1.1320895664153827, 1.0670817171942943, 1.1004113975523813, 1.1205368654773982]
Training Loss (progress: 0.64), 0.19540887126262257, [0.01617967829410407, 0.0005876128359633858, 0.007618123486377632, 0.0015350818223442615, 0.01312031956330298, 0.08628282935752754], [0.9489932436148464, 1.0433558389969477, 1.1330960285994538, 1.0668532158156134, 1.100753710081972, 1.1214643275134184]
Training Loss (progress: 0.72), 0.17131811317035958, [0.016307405870987057, 0.0006333331816384604, 0.007792346885411003, 0.0015209819844616398, 0.01313428178047751, 0.08705597483621105], [0.9486325685753293, 1.0432141941750943, 1.1347381352514938, 1.0666706180183132, 1.1015968711107196, 1.1220606467867391]
Training Loss (progress: 0.80), 0.19002766647110214, [0.016355523718670637, 0.0006223220228413995, 0.007685966531640566, 0.0015500926851340607, 0.013125691663432687, 0.08760994598912165], [0.9477118897778133, 1.0425797934693308, 1.1355436471364917, 1.0667764499959236, 1.1022557169892677, 1.1226867580319375]
Training Loss (progress: 0.88), 0.19466929608729233, [0.016445957092532933, 0.0006239096122550864, 0.007938255431222688, 0.0015265013587697069, 0.013448219139691573, 0.08843956418275188], [0.9471225391076308, 1.0421492270395267, 1.1372063179065368, 1.0667648254620203, 1.1034434879661152, 1.1236262908070065]
Training Loss (progress: 0.96), 0.18516429285406824, [0.01631547438460696, 0.0006674230821880939, 0.007912638348864427, 0.0014904463809688236, 0.013462124255929402, 0.0891418690812653], [0.9463026648555745, 1.0416535485167153, 1.1379768323513095, 1.0666433186470694, 1.10385806620439, 1.1241790099184605]
Evaluation on validation dataset:
Step 25, mean loss 0.06691257732798052
Step 50, mean loss 0.03203510491314002
Step 75, mean loss 0.04341942384113532
Step 100, mean loss 0.054423421491011884
Step 125, mean loss 0.07239156979997621
Step 150, mean loss 0.0632033041287279
Step 175, mean loss 0.09438883509130364
Step 200, mean loss 0.25902181747203445
Step 225, mean loss 0.19335687856793443
Unrolled forward losses 2.5223786354031903
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.57281e-01, 2.03574e-03, 7.14865e-01, 9.12952e-02, 8.44052e-01, 1.07904e+00
Node: 01 (pos: 0.010): 8.10618e-01, 1.36924e-02, 8.24120e-01, 1.93483e-01, 9.16323e-01, 1.09269e+00
Node: 02 (pos: 0.020): 8.57039e-01, 6.51233e-02, 9.25820e-01, 3.57710e-01, 9.80033e-01, 1.10397e+00
Node: 03 (pos: 0.030): 8.94975e-01, 2.19024e-01, 1.01352e+00, 5.76913e-01, 1.03263e+00, 1.11284e+00
Node: 04 (pos: 0.040): 9.23096e-01, 5.20888e-01, 1.08121e+00, 8.11673e-01, 1.07192e+00, 1.11921e+00
Node: 05 (pos: 0.051): 9.40391e-01, 8.75985e-01, 1.12397e+00, 9.96193e-01, 1.09621e+00, 1.12305e+00
-
Node: 07 (pos: 0.071): 9.40391e-01, 8.75985e-01, 1.12397e+00, 9.96193e-01, 1.09621e+00, 1.12305e+00
Node: 08 (pos: 0.081): 9.23096e-01, 5.20888e-01, 1.08121e+00, 8.11673e-01, 1.07192e+00, 1.11921e+00
Node: 09 (pos: 0.091): 8.94975e-01, 2.19024e-01, 1.01352e+00, 5.76913e-01, 1.03263e+00, 1.11284e+00
Node: 10 (pos: 0.101): 8.57039e-01, 6.51233e-02, 9.25820e-01, 3.57710e-01, 9.80033e-01, 1.10397e+00
Node: 11 (pos: 0.111): 8.10618e-01, 1.36924e-02, 8.24120e-01, 1.93483e-01, 9.16323e-01, 1.09269e+00
Node: 12 (pos: 0.121): 7.57281e-01, 2.03574e-03, 7.14865e-01, 9.12952e-02, 8.44052e-01, 1.07904e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.40391e-01, 8.75985e-01, 1.12397e+00, 9.96193e-01, 1.09621e+00, 1.12305e+00
Node: 58 (pos: 0.586): 9.23096e-01, 5.20888e-01, 1.08121e+00, 8.11673e-01, 1.07192e+00, 1.11921e+00
Node: 59 (pos: 0.596): 8.94975e-01, 2.19024e-01, 1.01352e+00, 5.76913e-01, 1.03263e+00, 1.11284e+00
Node: 60 (pos: 0.606): 8.57039e-01, 6.51233e-02, 9.25820e-01, 3.57710e-01, 9.80033e-01, 1.10397e+00
Node: 61 (pos: 0.616): 8.10618e-01, 1.36924e-02, 8.24120e-01, 1.93483e-01, 9.16323e-01, 1.09269e+00
Node: 50 (pos: 0.505): 7.57281e-01, 2.03574e-03, 7.14865e-01, 9.12952e-02, 8.44052e-01, 1.07904e+00
-
Node: 51 (pos: 0.515): 8.10618e-01, 1.36924e-02, 8.24120e-01, 1.93483e-01, 9.16323e-01, 1.09269e+00
Node: 52 (pos: 0.525): 8.57039e-01, 6.51233e-02, 9.25820e-01, 3.57710e-01, 9.80033e-01, 1.10397e+00
Node: 53 (pos: 0.535): 8.94975e-01, 2.19024e-01, 1.01352e+00, 5.76913e-01, 1.03263e+00, 1.11284e+00
Node: 54 (pos: 0.545): 9.23096e-01, 5.20888e-01, 1.08121e+00, 8.11673e-01, 1.07192e+00, 1.11921e+00
Node: 55 (pos: 0.556): 9.40391e-01, 8.75985e-01, 1.12397e+00, 9.96193e-01, 1.09621e+00, 1.12305e+00
Node: 62 (pos: 0.626): 7.57281e-01, 2.03574e-03, 7.14865e-01, 9.12952e-02, 8.44052e-01, 1.07904e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.05469996029166195
Step 50, mean loss 0.024708266518982943
Step 75, mean loss 0.036041690442305326
Step 100, mean loss 0.046116955417329665
Step 125, mean loss 0.06018616723805646
Step 150, mean loss 0.0628217551484172
Step 175, mean loss 0.12418607776947002
Step 200, mean loss 0.11745743503430167
Step 225, mean loss 0.12050434020768169
Unrolled forward losses 2.3102719827909213
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.20248436979275225, [0.016571053470018887, 0.0006345992975173633, 0.007826869554149019, 0.0015174101254906579, 0.013661439996683409, 0.08948411679911394], [0.9463410214423753, 1.0416422056225147, 1.1386406583043542, 1.0664609648145156, 1.1045267700904433, 1.124459833917772]
Training Loss (progress: 0.08), 0.18037223541427283, [0.016514068340124404, 0.0006422820029031805, 0.008185002504829092, 0.001598406002283655, 0.013795219697538604, 0.09028717847086343], [0.9453164128942463, 1.0407950890650068, 1.140188026220687, 1.0663357980929598, 1.1053565127448381, 1.1252893470068692]
Training Loss (progress: 0.16), 0.17638975681190533, [0.016697283051464628, 0.0006201358298154289, 0.008405649378154285, 0.0015478326751991681, 0.014013359238150428, 0.09077171137261017], [0.9447118707054005, 1.040522103944027, 1.1413684422395771, 1.0662059222940152, 1.1060557754805116, 1.1255292824925256]
Training Loss (progress: 0.24), 0.18437577805290908, [0.016618998991997987, 0.0006610739599439658, 0.008549219212986642, 0.0016036575286071382, 0.014196583084087463, 0.09166842142297826], [0.9438534331635162, 1.03998917030047, 1.1424776396256318, 1.0662011148518664, 1.107116637982248, 1.1263536308451596]
Training Loss (progress: 0.32), 0.17862687757029583, [0.01662627259437906, 0.0006372789470569866, 0.009013824124006855, 0.001511562508252548, 0.014076233912056549, 0.09277970767362524], [0.9432724519864296, 1.0394159583988438, 1.1436037275723707, 1.0660403178775795, 1.1073589777890984, 1.1271557470698217]
Training Loss (progress: 0.40), 0.164479610209778, [0.01684548080331634, 0.0006006668427213203, 0.00901867283096542, 0.0015327214204462869, 0.014564533372860138, 0.09314052560486706], [0.9425564520572149, 1.038999762589694, 1.1449538368485073, 1.0659378569908764, 1.108497331180428, 1.1276754021793813]
Training Loss (progress: 0.48), 0.19893188833722297, [0.016847261186694402, 0.0006062614738260893, 0.009359080018649135, 0.001618974473900911, 0.014750398043888523, 0.09393626283407239], [0.9419099861691477, 1.0388754771447832, 1.146508969975462, 1.0657862640841966, 1.1097614869424937, 1.128336209920575]
Training Loss (progress: 0.56), 0.18968865322359763, [0.0169677891360668, 0.0005963439850177143, 0.009971105474866798, 0.001608864098720685, 0.01440958703581774, 0.09413772344085536], [0.9410913058142378, 1.038444511886789, 1.1486079070563093, 1.065384963276244, 1.1099092069557601, 1.1286540093974446]
Training Loss (progress: 0.64), 0.18045705207311324, [0.017150122281604535, 0.0006536331804697618, 0.00989326634597099, 0.0015874597001866865, 0.014632273057491188, 0.09506301969506213], [0.9405462991205452, 1.0377227659808697, 1.1493760413941738, 1.0651853781090932, 1.110653411570765, 1.1296201445321932]
Training Loss (progress: 0.72), 0.1918773767625829, [0.017295972376459544, 0.0006070879779446022, 0.0103058282240131, 0.0015523645700844491, 0.014820990910814806, 0.09543302262552404], [0.9399823360228529, 1.0370651097468566, 1.1505005798002914, 1.0648801989828558, 1.1113960511631473, 1.1300571672530242]
Training Loss (progress: 0.80), 0.17506056003040216, [0.017328558859458623, 0.0006726802169839207, 0.010682706833825328, 0.0015526254244982427, 0.015030933503791043, 0.0959807322996089], [0.9389765786060866, 1.0367012225873289, 1.15135854000527, 1.064925841667882, 1.1123364358740515, 1.1303997420467178]
Training Loss (progress: 0.88), 0.1673727581510117, [0.01743702048102257, 0.0005704876049271484, 0.010928140937449461, 0.00160029035935611, 0.015087635930129685, 0.09665125067578635], [0.9382710492863017, 1.0361349314725354, 1.1524261610496753, 1.0647791795606174, 1.1130803839880543, 1.1310329347048536]
Training Loss (progress: 0.96), 0.1814412037488884, [0.017589914990036447, 0.0006611764974429076, 0.011159328855432814, 0.0016098268408498465, 0.015248946591157792, 0.09701978390267647], [0.9375992446091037, 1.0356290411626554, 1.153637643943257, 1.0645285407278933, 1.113670655334582, 1.1312710428186834]
Evaluation on validation dataset:
Step 25, mean loss 0.05625290966140415
Step 50, mean loss 0.026604601099379625
Step 75, mean loss 0.037361536402725756
Step 100, mean loss 0.046091975782498185
Step 125, mean loss 0.0641038630200104
Step 150, mean loss 0.05346148688790031
Step 175, mean loss 0.08287251553782575
Step 200, mean loss 0.24035200665211254
Step 225, mean loss 0.1732564760635181
Unrolled forward losses 2.169445318259715
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.61045e-01, 2.61656e-03, 8.31667e-01, 1.00824e-01, 8.76842e-01, 1.08979e+00
Node: 01 (pos: 0.010): 8.11062e-01, 1.62692e-02, 9.19187e-01, 2.07159e-01, 9.43354e-01, 1.10242e+00
Node: 02 (pos: 0.020): 8.54420e-01, 7.25609e-02, 9.97602e-01, 3.73407e-01, 1.00151e+00, 1.11287e+00
Node: 03 (pos: 0.030): 8.89739e-01, 2.32136e-01, 1.06319e+00, 5.90471e-01, 1.04921e+00, 1.12107e+00
Node: 04 (pos: 0.040): 9.15858e-01, 5.32704e-01, 1.11266e+00, 8.19131e-01, 1.08466e+00, 1.12696e+00
Node: 05 (pos: 0.051): 9.31895e-01, 8.76863e-01, 1.14344e+00, 9.96887e-01, 1.10651e+00, 1.13051e+00
-
Node: 07 (pos: 0.071): 9.31895e-01, 8.76863e-01, 1.14344e+00, 9.96887e-01, 1.10651e+00, 1.13051e+00
Node: 08 (pos: 0.081): 9.15858e-01, 5.32704e-01, 1.11266e+00, 8.19131e-01, 1.08466e+00, 1.12696e+00
Node: 09 (pos: 0.091): 8.89739e-01, 2.32136e-01, 1.06319e+00, 5.90471e-01, 1.04921e+00, 1.12107e+00
Node: 10 (pos: 0.101): 8.54420e-01, 7.25609e-02, 9.97602e-01, 3.73407e-01, 1.00151e+00, 1.11287e+00
Node: 11 (pos: 0.111): 8.11062e-01, 1.62692e-02, 9.19187e-01, 2.07159e-01, 9.43354e-01, 1.10242e+00
Node: 12 (pos: 0.121): 7.61045e-01, 2.61656e-03, 8.31667e-01, 1.00824e-01, 8.76842e-01, 1.08979e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.31895e-01, 8.76863e-01, 1.14344e+00, 9.96887e-01, 1.10651e+00, 1.13051e+00
Node: 58 (pos: 0.586): 9.15858e-01, 5.32704e-01, 1.11266e+00, 8.19131e-01, 1.08466e+00, 1.12696e+00
Node: 59 (pos: 0.596): 8.89739e-01, 2.32136e-01, 1.06319e+00, 5.90471e-01, 1.04921e+00, 1.12107e+00
Node: 60 (pos: 0.606): 8.54420e-01, 7.25609e-02, 9.97602e-01, 3.73407e-01, 1.00151e+00, 1.11287e+00
Node: 61 (pos: 0.616): 8.11062e-01, 1.62692e-02, 9.19187e-01, 2.07159e-01, 9.43354e-01, 1.10242e+00
Node: 50 (pos: 0.505): 7.61045e-01, 2.61656e-03, 8.31667e-01, 1.00824e-01, 8.76842e-01, 1.08979e+00
-
Node: 51 (pos: 0.515): 8.11062e-01, 1.62692e-02, 9.19187e-01, 2.07159e-01, 9.43354e-01, 1.10242e+00
Node: 52 (pos: 0.525): 8.54420e-01, 7.25609e-02, 9.97602e-01, 3.73407e-01, 1.00151e+00, 1.11287e+00
Node: 53 (pos: 0.535): 8.89739e-01, 2.32136e-01, 1.06319e+00, 5.90471e-01, 1.04921e+00, 1.12107e+00
Node: 54 (pos: 0.545): 9.15858e-01, 5.32704e-01, 1.11266e+00, 8.19131e-01, 1.08466e+00, 1.12696e+00
Node: 55 (pos: 0.556): 9.31895e-01, 8.76863e-01, 1.14344e+00, 9.96887e-01, 1.10651e+00, 1.13051e+00
Node: 62 (pos: 0.626): 7.61045e-01, 2.61656e-03, 8.31667e-01, 1.00824e-01, 8.76842e-01, 1.08979e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.04761902326814534
Step 50, mean loss 0.020300603157588595
Step 75, mean loss 0.0291232364076258
Step 100, mean loss 0.03414395037088183
Step 125, mean loss 0.054565722073396404
Step 150, mean loss 0.06184416771517054
Step 175, mean loss 0.09042314572324392
Step 200, mean loss 0.10195373812977616
Step 225, mean loss 0.11166164724347921
Unrolled forward losses 1.8926365307777382
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.18459246911133637, [0.017726772963411443, 0.0006261284098020139, 0.011329756755014702, 0.0015426007802921764, 0.01543245978192136, 0.09746395907541969], [0.9373445170317982, 1.0352428187925067, 1.1539474248124877, 1.0643613601886979, 1.1139302085186815, 1.1318236072790817]
Training Loss (progress: 0.08), 0.18252993920021282, [0.017816030029668688, 0.0006656806037964784, 0.01130891977758476, 0.0015982139272376567, 0.015616607029818555, 0.0979793550095088], [0.9364183550643026, 1.0347872228332948, 1.154709317716369, 1.0641988866186618, 1.1149196397872645, 1.1321905886056716]
Training Loss (progress: 0.16), 0.15790812659163186, [0.0181117478870645, 0.0006369053488503205, 0.011906819013909184, 0.0015702514398773563, 0.01598045874760591, 0.0989238133008764], [0.9358951730346229, 1.034188519603806, 1.1565511598824523, 1.0640240439452602, 1.1159694906265336, 1.1331193265333885]
Training Loss (progress: 0.24), 0.14944882949086213, [0.018194994152446596, 0.0006019465724173443, 0.012303928730137392, 0.0015804887196699514, 0.01601062223444153, 0.09947470408415311], [0.935045621761293, 1.0337436314312314, 1.1580814969586706, 1.0639870665531488, 1.1165755727418498, 1.1336596658119087]
Training Loss (progress: 0.32), 0.18103650953049608, [0.018125168412985226, 0.0006404491850041942, 0.012638422650689232, 0.0016173404764950985, 0.015963179338871432, 0.09969076087969436], [0.9342122200172713, 1.0331839421236029, 1.1585165394009578, 1.0636717249370384, 1.1173297190020754, 1.1338438985844232]
Training Loss (progress: 0.40), 0.16924121171523837, [0.01832328976377826, 0.0006469365438916584, 0.012599236380496284, 0.0015507505378569394, 0.015956454280824168, 0.10015225825606587], [0.9333338143103048, 1.0327542033166555, 1.1597506941669464, 1.06317821549727, 1.1180279823898167, 1.13444996094356]
Training Loss (progress: 0.48), 0.15811324100664825, [0.018385105894388867, 0.0006414155539875073, 0.012964779675221586, 0.0016014077992785366, 0.016019783634657395, 0.10083279726015629], [0.9326848103069262, 1.0323572217105905, 1.1612716263958647, 1.0632110301474267, 1.118468464163058, 1.1349745473234842]
Training Loss (progress: 0.56), 0.16970354872077623, [0.018332817940903485, 0.0005882704444407665, 0.012863856110789301, 0.0015863251067188184, 0.016285994545703134, 0.10113708601100257], [0.9318370009955039, 1.032039376688921, 1.1618255111601532, 1.0627696461386942, 1.1192148444539054, 1.1350858345486174]
Training Loss (progress: 0.64), 0.16033867684218586, [0.018457345849279053, 0.0006315470125279449, 0.013566957611403013, 0.0016532101095335964, 0.016112871927675063, 0.1017781181227355], [0.9309913059701046, 1.03153030535588, 1.1633421991994508, 1.0624198461756584, 1.1193927611176446, 1.1356402153578424]
Training Loss (progress: 0.72), 0.16606694915899772, [0.018591557030434212, 0.000654972764453128, 0.013685649318127154, 0.0015645947509881346, 0.016506136262233937, 0.10222140499714913], [0.9303583059141117, 1.030918052740074, 1.1646754313791017, 1.0622796026914372, 1.1205549461621245, 1.1361446775912096]
Training Loss (progress: 0.80), 0.17100830505250786, [0.018671895446605694, 0.0006558626379111733, 0.01361466049820907, 0.0016067583598801862, 0.016304275021367977, 0.10259056565947533], [0.9294335179305357, 1.0303210270333192, 1.1647393540634745, 1.0622376002489853, 1.1208336511968697, 1.136389176371698]
Training Loss (progress: 0.88), 0.18418276926221622, [0.01891125654933099, 0.0005540901960452937, 0.01435516905073095, 0.0015632275490609652, 0.016496471467389615, 0.10314740390246727], [0.9288071442636422, 1.0297927305174572, 1.1660427546525836, 1.0619701017844032, 1.1215802289071821, 1.1369834216402093]
Training Loss (progress: 0.96), 0.17170636474237552, [0.019026950012332122, 0.000627785928799276, 0.014280373203666737, 0.0015851468385560993, 0.016340649552438094, 0.10352267316836995], [0.9279973238666492, 1.0293137708834867, 1.167476120642968, 1.0616954369481715, 1.1223352067752357, 1.1372922669525067]
Evaluation on validation dataset:
Step 25, mean loss 0.04961106788614987
Step 50, mean loss 0.030224443235969674
Step 75, mean loss 0.04351956285218139
Step 100, mean loss 0.04495217944325715
Step 125, mean loss 0.06588278203921066
Step 150, mean loss 0.05291912773817556
Step 175, mean loss 0.10226366649746843
Step 200, mean loss 0.21777998892498063
Step 225, mean loss 0.16678884618922044
Unrolled forward losses 2.8119413407476923
Unrolled forward base losses 2.565701273852575
Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.14876528903746367, [0.01895822427031511, 0.0006291244944127965, 0.014537327455015825, 0.0015781337016100812, 0.0165541712170254, 0.10368347794309846], [0.9276342279344183, 1.0290017751538638, 1.167703015618647, 1.0615337439956083, 1.1227194570223642, 1.1375564519871606]
Training Loss (progress: 0.08), 0.13285369492429985, [0.019033758652156896, 0.0006536693970376276, 0.014985281018043537, 0.0015751114608597485, 0.016598399405590714, 0.10399128232367831], [0.9274242846416196, 1.0288343015620254, 1.1687206903377163, 1.0615217254013658, 1.1233857949798796, 1.1381342984437843]
Training Loss (progress: 0.16), 0.13870444609985413, [0.019145451698520402, 0.0006733123556668538, 0.01528157372302604, 0.0016017945217465547, 0.01666666008244252, 0.10432582882489877], [0.9272988465882368, 1.0287104029630432, 1.1694968686123959, 1.061418707692256, 1.123843161615463, 1.1387946943842002]
Training Loss (progress: 0.24), 0.13602249300342287, [0.019208219952659117, 0.0006563752421537988, 0.015335831452591807, 0.0015962807708521235, 0.016803521988535148, 0.10457148748597397], [0.9271954579059702, 1.0286482383129123, 1.1700330409289619, 1.0614000527404013, 1.1244849926759735, 1.1391431338307276]
Training Loss (progress: 0.32), 0.1285925377604994, [0.01914043102807207, 0.0006069214100739625, 0.015336665901387004, 0.0015788491962979436, 0.016786450546840057, 0.1046957754276547], [0.9269707836968543, 1.028507796141296, 1.1704259627137283, 1.0615004936399344, 1.1248515123373297, 1.1394915739729101]
Training Loss (progress: 0.40), 0.1357454484629589, [0.019267390738606602, 0.0006583510578511145, 0.015490701319242581, 0.0015442890179579742, 0.016813658493170527, 0.10492987577936683], [0.9268319289847966, 1.0283084989580493, 1.1711996696829334, 1.0615534628247816, 1.1251399028133986, 1.1399803452420225]
Training Loss (progress: 0.48), 0.14276163136212705, [0.019385026665595797, 0.0006491267268369843, 0.015909944414516746, 0.001595017908178889, 0.016890168200634225, 0.10519377702670596], [0.9266920271375829, 1.0279988162599139, 1.171968836177789, 1.0616371441307038, 1.1256521194557418, 1.1403544924499727]
Training Loss (progress: 0.56), 0.13436239818289017, [0.019413731487275775, 0.0006410645510230808, 0.01609846923749172, 0.0016046199887733228, 0.016876488832334644, 0.10544306240845729], [0.9265119089465258, 1.0278590798938614, 1.1726322317954023, 1.0615451826682158, 1.126093512115493, 1.140766944201324]
Training Loss (progress: 0.64), 0.12585504858881932, [0.01940287986581078, 0.0006532986012376428, 0.016349199606447998, 0.0015856426484005001, 0.01697105971537769, 0.10567830264709577], [0.9262604678723527, 1.027754162401424, 1.1733388489785912, 1.0614918911929048, 1.126516223804003, 1.141119934993463]
Training Loss (progress: 0.72), 0.12358463345207216, [0.019519631276767768, 0.0006605240711700849, 0.016372229460065377, 0.0015627149091526876, 0.016902412665042214, 0.10585130119656488], [0.9261276036080862, 1.0276426004624268, 1.1740668249122446, 1.0614784819955134, 1.126825942487589, 1.1414988122399918]
Training Loss (progress: 0.80), 0.1434391763693835, [0.019441352784187096, 0.0006408509697577901, 0.016757763446389913, 0.00157199077646989, 0.0171494788682132, 0.10617624465347102], [0.9258868076453182, 1.0274449155137126, 1.1747588055897926, 1.06143438495937, 1.1272830513039196, 1.1419167457705663]
Training Loss (progress: 0.88), 0.1333157263106225, [0.019527399330478045, 0.0006531713486674196, 0.01714778805012942, 0.0015944948307099504, 0.017225639856036704, 0.10658806756035218], [0.9257447512616059, 1.0271799960083858, 1.1754641734778335, 1.061181659297877, 1.1277623681351847, 1.1424389042678404]
Training Loss (progress: 0.96), 0.14263350345650797, [0.01964625907678665, 0.0006612371066232077, 0.017035845973593127, 0.0015902347362547083, 0.017321041064321387, 0.10670869429635699], [0.925593307606125, 1.027221940437552, 1.1757487641450304, 1.061171038283134, 1.1282872866837783, 1.1426995448965598]
Evaluation on validation dataset:
Step 25, mean loss 0.03549567384696398
Step 50, mean loss 0.019956147427378824
Step 75, mean loss 0.031027347443769653
Step 100, mean loss 0.034305660651287706
Step 125, mean loss 0.04469590259658393
Step 150, mean loss 0.04229300841295271
Step 175, mean loss 0.0678285583493366
Step 200, mean loss 0.20744548264031096
Step 225, mean loss 0.13820900096037048
Unrolled forward losses 1.8139192654536869
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.67376e-01, 4.72169e-03, 9.49249e-01, 1.02771e-01, 9.12663e-01, 1.10435e+00
Node: 01 (pos: 0.010): 8.12568e-01, 2.44535e-02, 1.01349e+00, 2.09736e-01, 9.73813e-01, 1.11601e+00
Node: 02 (pos: 0.020): 8.51517e-01, 9.39125e-02, 1.06926e+00, 3.75966e-01, 1.02688e+00, 1.12564e+00
Node: 03 (pos: 0.030): 8.83097e-01, 2.67450e-01, 1.11476e+00, 5.91965e-01, 1.07015e+00, 1.13319e+00
Node: 04 (pos: 0.040): 9.06368e-01, 5.64806e-01, 1.14844e+00, 8.18683e-01, 1.10216e+00, 1.13861e+00
Node: 05 (pos: 0.051): 9.20624e-01, 8.84490e-01, 1.16913e+00, 9.94507e-01, 1.12183e+00, 1.14188e+00
-
Node: 07 (pos: 0.071): 9.20624e-01, 8.84490e-01, 1.16913e+00, 9.94507e-01, 1.12183e+00, 1.14188e+00
Node: 08 (pos: 0.081): 9.06368e-01, 5.64806e-01, 1.14844e+00, 8.18683e-01, 1.10216e+00, 1.13861e+00
Node: 09 (pos: 0.091): 8.83097e-01, 2.67450e-01, 1.11476e+00, 5.91965e-01, 1.07015e+00, 1.13319e+00
Node: 10 (pos: 0.101): 8.51517e-01, 9.39125e-02, 1.06926e+00, 3.75966e-01, 1.02688e+00, 1.12564e+00
Node: 11 (pos: 0.111): 8.12568e-01, 2.44535e-02, 1.01349e+00, 2.09736e-01, 9.73813e-01, 1.11601e+00
Node: 12 (pos: 0.121): 7.67376e-01, 4.72169e-03, 9.49249e-01, 1.02771e-01, 9.12663e-01, 1.10435e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.20624e-01, 8.84490e-01, 1.16913e+00, 9.94507e-01, 1.12183e+00, 1.14188e+00
Node: 58 (pos: 0.586): 9.06368e-01, 5.64806e-01, 1.14844e+00, 8.18683e-01, 1.10216e+00, 1.13861e+00
Node: 59 (pos: 0.596): 8.83097e-01, 2.67450e-01, 1.11476e+00, 5.91965e-01, 1.07015e+00, 1.13319e+00
Node: 60 (pos: 0.606): 8.51517e-01, 9.39125e-02, 1.06926e+00, 3.75966e-01, 1.02688e+00, 1.12564e+00
Node: 61 (pos: 0.616): 8.12568e-01, 2.44535e-02, 1.01349e+00, 2.09736e-01, 9.73813e-01, 1.11601e+00
Node: 50 (pos: 0.505): 7.67376e-01, 4.72169e-03, 9.49249e-01, 1.02771e-01, 9.12663e-01, 1.10435e+00
-
Node: 51 (pos: 0.515): 8.12568e-01, 2.44535e-02, 1.01349e+00, 2.09736e-01, 9.73813e-01, 1.11601e+00
Node: 52 (pos: 0.525): 8.51517e-01, 9.39125e-02, 1.06926e+00, 3.75966e-01, 1.02688e+00, 1.12564e+00
Node: 53 (pos: 0.535): 8.83097e-01, 2.67450e-01, 1.11476e+00, 5.91965e-01, 1.07015e+00, 1.13319e+00
Node: 54 (pos: 0.545): 9.06368e-01, 5.64806e-01, 1.14844e+00, 8.18683e-01, 1.10216e+00, 1.13861e+00
Node: 55 (pos: 0.556): 9.20624e-01, 8.84490e-01, 1.16913e+00, 9.94507e-01, 1.12183e+00, 1.14188e+00
Node: 62 (pos: 0.626): 7.67376e-01, 4.72169e-03, 9.49249e-01, 1.02771e-01, 9.12663e-01, 1.10435e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.030927970479757733
Step 50, mean loss 0.014257839409076226
Step 75, mean loss 0.022796553035759143
Step 100, mean loss 0.027052534313446487
Step 125, mean loss 0.040446048721911876
Step 150, mean loss 0.04184265318070303
Step 175, mean loss 0.06644083609073477
Step 200, mean loss 0.0831487358785751
Step 225, mean loss 0.09169519315479692
Unrolled forward losses 1.4708224565996024
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.13959554499002502, [0.01966765833041752, 0.000618412274126077, 0.017169565241568582, 0.001611740511311881, 0.017370991276683304, 0.10687138486957881], [0.9255011607696285, 1.0271143139752212, 1.176105061197441, 1.0611645429148062, 1.1285469828337604, 1.1429741218424503]
Training Loss (progress: 0.08), 0.13448462412259582, [0.019714391969480178, 0.0006586601803148148, 0.017340568131936723, 0.0015918042710859803, 0.017386517480570723, 0.10687201563075871], [0.9252911108092682, 1.0269180816952497, 1.1768420574214338, 1.0612604256073448, 1.1289990991396688, 1.1431718362483076]
Training Loss (progress: 0.16), 0.128228884546303, [0.01968043638718536, 0.0006586432012815215, 0.017592532583472643, 0.0015542489111246723, 0.017489426788766, 0.10716812544137086], [0.9250394554054213, 1.0267556677934897, 1.177623013374465, 1.0611157877124993, 1.1293774789138231, 1.1436701711047486]
Training Loss (progress: 0.24), 0.13833165548668291, [0.019769138370848868, 0.0006635059883175907, 0.017738160918194065, 0.0015740578742888525, 0.017589474982304013, 0.10726486698526308], [0.9249534872654245, 1.0265999800173449, 1.1778294784992263, 1.0609972831209293, 1.1296478109908858, 1.1437491504729416]
Training Loss (progress: 0.32), 0.14413836642242506, [0.01961935560647019, 0.0006684163915563851, 0.017633092864892334, 0.0016040468179769139, 0.017585731636677718, 0.10739746589480573], [0.9245123690542363, 1.0265801491732183, 1.1783733465360648, 1.0611374712066455, 1.1301277277018746, 1.1440566983483968]
Training Loss (progress: 0.40), 0.14075624791627075, [0.01974740589146903, 0.0006430407293600885, 0.01781685618574884, 0.0015568537760424358, 0.017573944138556882, 0.10768407652697357], [0.9243693059376468, 1.026343065426828, 1.178879324848261, 1.0610873637960365, 1.1304332088764513, 1.1444276524316352]
Training Loss (progress: 0.48), 0.14446670072155177, [0.019843255300097205, 0.0006218328892023383, 0.01800421771098804, 0.0016000030371289464, 0.017620468621824818, 0.10779395251341126], [0.924240602998485, 1.0260438876956908, 1.1793935530958057, 1.0610307973754294, 1.1308576645445023, 1.1447731398696717]
Training Loss (progress: 0.56), 0.13459090867991036, [0.01984740693902337, 0.00063602542831257, 0.018387937409198078, 0.001575294798493489, 0.017642857718622843, 0.10802258219425413], [0.9239588548641701, 1.025997087569413, 1.1801853299692124, 1.0608585336149485, 1.1312461187292995, 1.1450800901004146]
Training Loss (progress: 0.64), 0.11646840538096741, [0.019801192267004917, 0.0006643031570529526, 0.018431182023954832, 0.0015985212230688573, 0.017626120939167784, 0.10808089992254065], [0.9237775942926394, 1.0256859013164388, 1.1805495541894337, 1.0609253820553497, 1.1316242951675906, 1.1453774842666473]
Training Loss (progress: 0.72), 0.13480823484574755, [0.019846461413592725, 0.000661477227216457, 0.01873566082414686, 0.0016156005217550747, 0.017803131476185605, 0.10831560852549292], [0.9235073882099972, 1.0256710420568462, 1.1811253663863535, 1.0608945200233209, 1.13216896791899, 1.145698627787334]
Training Loss (progress: 0.80), 0.12437928121284046, [0.01988153998379796, 0.0006470058462734981, 0.01883212733422865, 0.001575466037300059, 0.01773433590965746, 0.10848387714971393], [0.9232691525438409, 1.0254904542846122, 1.1817166668529548, 1.0608113115117936, 1.1323238946606704, 1.1460211560795948]
Training Loss (progress: 0.88), 0.1372376299674838, [0.01988901424466246, 0.0006619157022665956, 0.01906173936148612, 0.0015658601763828292, 0.01795639492829481, 0.10873356297154574], [0.9230210096734907, 1.0251644548723522, 1.1822539536179817, 1.060608570667672, 1.133036542816439, 1.1464215672736837]
Training Loss (progress: 0.96), 0.1354883092650899, [0.01998715902093444, 0.0006365449988083368, 0.01909750672131798, 0.0015851885644664635, 0.017988638200965853, 0.10890623779214628], [0.9228894444279513, 1.0251665246922492, 1.1826901067757079, 1.0606490634684094, 1.1333308294489428, 1.14683993595899]
Evaluation on validation dataset:
Step 25, mean loss 0.030917868264555497
Step 50, mean loss 0.017577337188520798
Step 75, mean loss 0.02989047743219424
Step 100, mean loss 0.031911561940761904
Step 125, mean loss 0.043828443286236994
Step 150, mean loss 0.0431515823259736
Step 175, mean loss 0.07432091190275034
Step 200, mean loss 0.2393956446664273
Step 225, mean loss 0.13177414064687842
Unrolled forward losses 1.6824970745610908
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.67154e-01, 4.78217e-03, 9.78651e-01, 1.02483e-01, 9.25140e-01, 1.10892e+00
Node: 01 (pos: 0.010): 8.11674e-01, 2.46547e-02, 1.03701e+00, 2.09298e-01, 9.84365e-01, 1.12040e+00
Node: 02 (pos: 0.020): 8.50013e-01, 9.43341e-02, 1.08734e+00, 3.75400e-01, 1.03563e+00, 1.12988e+00
Node: 03 (pos: 0.030): 8.81081e-01, 2.67876e-01, 1.12817e+00, 5.91344e-01, 1.07734e+00, 1.13731e+00
Node: 04 (pos: 0.040): 9.03965e-01, 5.64541e-01, 1.15826e+00, 8.18090e-01, 1.10816e+00, 1.14265e+00
Node: 05 (pos: 0.051): 9.17980e-01, 8.82982e-01, 1.17671e+00, 9.93980e-01, 1.12707e+00, 1.14586e+00
-
Node: 07 (pos: 0.071): 9.17980e-01, 8.82982e-01, 1.17671e+00, 9.93980e-01, 1.12707e+00, 1.14586e+00
Node: 08 (pos: 0.081): 9.03965e-01, 5.64541e-01, 1.15826e+00, 8.18090e-01, 1.10816e+00, 1.14265e+00
Node: 09 (pos: 0.091): 8.81081e-01, 2.67876e-01, 1.12817e+00, 5.91344e-01, 1.07734e+00, 1.13731e+00
Node: 10 (pos: 0.101): 8.50013e-01, 9.43341e-02, 1.08734e+00, 3.75400e-01, 1.03563e+00, 1.12988e+00
Node: 11 (pos: 0.111): 8.11674e-01, 2.46547e-02, 1.03701e+00, 2.09298e-01, 9.84365e-01, 1.12040e+00
Node: 12 (pos: 0.121): 7.67154e-01, 4.78217e-03, 9.78651e-01, 1.02483e-01, 9.25140e-01, 1.10892e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.17980e-01, 8.82982e-01, 1.17671e+00, 9.93980e-01, 1.12707e+00, 1.14586e+00
Node: 58 (pos: 0.586): 9.03965e-01, 5.64541e-01, 1.15826e+00, 8.18090e-01, 1.10816e+00, 1.14265e+00
Node: 59 (pos: 0.596): 8.81081e-01, 2.67876e-01, 1.12817e+00, 5.91344e-01, 1.07734e+00, 1.13731e+00
Node: 60 (pos: 0.606): 8.50013e-01, 9.43341e-02, 1.08734e+00, 3.75400e-01, 1.03563e+00, 1.12988e+00
Node: 61 (pos: 0.616): 8.11674e-01, 2.46547e-02, 1.03701e+00, 2.09298e-01, 9.84365e-01, 1.12040e+00
Node: 50 (pos: 0.505): 7.67154e-01, 4.78217e-03, 9.78651e-01, 1.02483e-01, 9.25140e-01, 1.10892e+00
-
Node: 51 (pos: 0.515): 8.11674e-01, 2.46547e-02, 1.03701e+00, 2.09298e-01, 9.84365e-01, 1.12040e+00
Node: 52 (pos: 0.525): 8.50013e-01, 9.43341e-02, 1.08734e+00, 3.75400e-01, 1.03563e+00, 1.12988e+00
Node: 53 (pos: 0.535): 8.81081e-01, 2.67876e-01, 1.12817e+00, 5.91344e-01, 1.07734e+00, 1.13731e+00
Node: 54 (pos: 0.545): 9.03965e-01, 5.64541e-01, 1.15826e+00, 8.18090e-01, 1.10816e+00, 1.14265e+00
Node: 55 (pos: 0.556): 9.17980e-01, 8.82982e-01, 1.17671e+00, 9.93980e-01, 1.12707e+00, 1.14586e+00
Node: 62 (pos: 0.626): 7.67154e-01, 4.78217e-03, 9.78651e-01, 1.02483e-01, 9.25140e-01, 1.10892e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.027580372610675197
Step 50, mean loss 0.01241724986131697
Step 75, mean loss 0.02141112665636161
Step 100, mean loss 0.02655407898128733
Step 125, mean loss 0.03914626158436954
Step 150, mean loss 0.04262082946305846
Step 175, mean loss 0.0642378398376974
Step 200, mean loss 0.07757850981127415
Step 225, mean loss 0.08662868849036684
Unrolled forward losses 1.4670608755895718
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.1378984177602551, [0.019934688154707513, 0.0006562693364184238, 0.019410267875702734, 0.0015835027592963121, 0.01810465542982824, 0.10899587956428346], [0.9227032658560246, 1.0249849242140328, 1.1830300277149997, 1.0606261100967138, 1.1335372493923968, 1.1469601514585022]
Training Loss (progress: 0.08), 0.1259395530154707, [0.01991852194883116, 0.0006287197069684747, 0.019681407448213065, 0.0016209516091966305, 0.018088877896764735, 0.10904846222953433], [0.9224782589155366, 1.0248384465358724, 1.1835805133584674, 1.0605932979678685, 1.1338029903618612, 1.1471719715318167]
Training Loss (progress: 0.16), 0.12996944066151309, [0.01991795875097229, 0.0006632682869149473, 0.019855652507602385, 0.0015824118257878543, 0.018106048313908052, 0.10917984749685686], [0.9221091613732237, 1.0248088830429132, 1.184106611488345, 1.0605290828602825, 1.134321139630978, 1.1475263167245768]
Training Loss (progress: 0.24), 0.13688253475039894, [0.020074111057483378, 0.0006614267251591609, 0.019938929832775794, 0.0015670445974703985, 0.01824855182724636, 0.10947069621016964], [0.922140482926183, 1.0245401468443558, 1.1845482671608827, 1.0604719532408522, 1.134655556885023, 1.1479139896673634]
Training Loss (progress: 0.32), 0.1433550527107183, [0.0200054395302848, 0.0006620057850892622, 0.020306747607348626, 0.0015662185430304367, 0.018353727641849452, 0.10953388342912586], [0.921819586571826, 1.0244205227588927, 1.185138374216879, 1.0604601355168095, 1.1349753112515613, 1.14818381496743]
Training Loss (progress: 0.40), 0.11614297718014063, [0.02001343606587785, 0.0006756816111879046, 0.02051833690846783, 0.001578686564697736, 0.018310238019132622, 0.10952742815389667], [0.9214632798074356, 1.0241617082187533, 1.1857470958915566, 1.0603678520016258, 1.1352081950957111, 1.1483800068020744]
Training Loss (progress: 0.48), 0.13268074670889507, [0.020011093142942464, 0.000690904805965262, 0.02076043516048728, 0.0015469392435400495, 0.01834058247277574, 0.10983979421610514], [0.9211229504757871, 1.0240738434853072, 1.186351399356835, 1.0602780128291995, 1.1356184386631394, 1.1487906632325708]
Training Loss (progress: 0.56), 0.12366687780258477, [0.020087648052821226, 0.0006995465406873812, 0.021081808313931513, 0.0015853917827728086, 0.01845693741508391, 0.10998795593263444], [0.920997509688063, 1.0238203669727928, 1.1867751340952517, 1.0603119141650998, 1.1360858078882368, 1.149063334343368]
Training Loss (progress: 0.64), 0.12840892563556902, [0.020130967870310525, 0.0006686147893689283, 0.021116657481865742, 0.0015699379913896637, 0.018556590471927162, 0.11028936171299349], [0.9208720192999621, 1.023736619111514, 1.1873063713687877, 1.0601668775704718, 1.1364782488376777, 1.1494558055425674]
Training Loss (progress: 0.72), 0.12840941966838926, [0.020189093690123915, 0.0006485356280235469, 0.0213230408311872, 0.001585327744075082, 0.018755460073067373, 0.11042972169719849], [0.9206552408319794, 1.0234872316854233, 1.1876666401508487, 1.0601180269754529, 1.1369885459330826, 1.149705714636482]
Training Loss (progress: 0.80), 0.10987882802553153, [0.020199519701586274, 0.0006782837362558072, 0.021245646686974044, 0.0016133018398661351, 0.018721085908004156, 0.11064466703912404], [0.9203351136927692, 1.0233227784685588, 1.1879523017369185, 1.0600911064905518, 1.137228806950369, 1.1500383773424254]
Training Loss (progress: 0.88), 0.13297618130826355, [0.020183484447444666, 0.0006807822279060154, 0.021861402907618274, 0.0015711071571893553, 0.01879698060873211, 0.11073689787695272], [0.9200481159314299, 1.0231209017990845, 1.1886905697590735, 1.0600842785158033, 1.1375584315023493, 1.1503672627957147]
Training Loss (progress: 0.96), 0.12479043645830981, [0.020259364367119698, 0.0006578177290863433, 0.02195056905672748, 0.0015889331858394257, 0.018805320481550584, 0.11089486836981062], [0.9198399854608874, 1.0230247032595328, 1.1891378096547134, 1.0600073522038411, 1.137885831387691, 1.1506954727969458]
Evaluation on validation dataset:
Step 25, mean loss 0.04449513077766871
Step 50, mean loss 0.01753645245980326
Step 75, mean loss 0.0276922087101243
Step 100, mean loss 0.030362042272661413
Step 125, mean loss 0.043764639072802
Step 150, mean loss 0.036242873279289094
Step 175, mean loss 0.059626711820395466
Step 200, mean loss 0.2192784439203231
Step 225, mean loss 0.134650524742084
Unrolled forward losses 1.7785577319928079
Unrolled forward base losses 2.565701273852575
Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.13151834854886318, [0.02019644499252612, 0.0006267875203509641, 0.022062652358691585, 0.0016075326964776942, 0.01891650714087343, 0.11104426091961332], [0.9196816284996479, 1.0229713664546152, 1.189336972453783, 1.0601106777227631, 1.1381676957904934, 1.1508422995270189]
Training Loss (progress: 0.08), 0.14463903577907136, [0.02021769897336775, 0.0006634728552845417, 0.022109050785505572, 0.0016100270578211172, 0.018853074525578314, 0.11117646630394043], [0.9194228748372277, 1.0228107432448614, 1.1897871690695594, 1.0599580773923958, 1.1382363028756428, 1.1510554789845628]
Training Loss (progress: 0.16), 0.11985587474744738, [0.020324515695804686, 0.0006364494121998077, 0.022466863760626978, 0.0015991159129538403, 0.019059736898585707, 0.11136178753735038], [0.9192181731660141, 1.0227479219411155, 1.1903051642181004, 1.059864047042527, 1.1387157240711028, 1.151349457441481]
Training Loss (progress: 0.24), 0.1270152225332156, [0.02030955631369269, 0.0006234397795396599, 0.022838995607427765, 0.0015792557981660117, 0.019171242512164528, 0.11153157681983193], [0.9188997552200732, 1.0225320269406628, 1.1907712250514653, 1.0598919269061233, 1.1391030226178354, 1.1516137949016843]
Training Loss (progress: 0.32), 0.12809824317111532, [0.020397522567963774, 0.0006570737663736212, 0.023014485461323814, 0.0016015786719992477, 0.019182334410238353, 0.11176775098211833], [0.9186563387714786, 1.022397373951756, 1.1910302114458517, 1.059666392690324, 1.1394333278902458, 1.1518260482977112]
Training Loss (progress: 0.40), 0.1257888745973213, [0.02037135711390513, 0.0006548218535327358, 0.02310659913393638, 0.00158998096781312, 0.01927255998471751, 0.1119676957786824], [0.918373164996734, 1.0222715907483646, 1.191361696450186, 1.0596505400202079, 1.1397015062969373, 1.152131660907483]
Training Loss (progress: 0.48), 0.12931065149882573, [0.02049214519763155, 0.0007020522658528518, 0.02342117518714834, 0.0015968919414844152, 0.019390756257119796, 0.11204501612341802], [0.9182743959629438, 1.0220142429437986, 1.1919821961827923, 1.0595543070766469, 1.1403050728244046, 1.152439338337878]
Training Loss (progress: 0.56), 0.12619811204022538, [0.02044870864988093, 0.0006606886948859973, 0.023642985682451496, 0.0016187469107767872, 0.019512889471550703, 0.11223221996143799], [0.9179244057746188, 1.0218628981366709, 1.192414406389221, 1.0594622188662737, 1.140664450771342, 1.1526961549196895]
Training Loss (progress: 0.64), 0.1235588998811455, [0.020590279987592004, 0.0006794193474193462, 0.023652373284949967, 0.0015865076095519153, 0.0195298055923052, 0.11233669085289506], [0.9177557643257599, 1.0217701894436084, 1.1925956673082005, 1.0594417766524034, 1.140933617717651, 1.1528977314330275]
Training Loss (progress: 0.72), 0.1202001285668689, [0.020636897952363148, 0.000635832587935126, 0.024035616049890584, 0.0016076427831361545, 0.019636441824060893, 0.11241638607756536], [0.9174780825109884, 1.0215661869611272, 1.193180474672952, 1.0592533824386847, 1.1414162588911327, 1.153070541429948]
Training Loss (progress: 0.80), 0.11882717594878733, [0.020662197097169308, 0.0006528899688442424, 0.02439054267816366, 0.0015835452757435242, 0.019573231682657302, 0.11256901418240059], [0.9172210996027931, 1.0213182615252308, 1.193819563755645, 1.0590885612295442, 1.1416635813320077, 1.1533162849992866]
Training Loss (progress: 0.88), 0.12058760829839645, [0.020710790241278747, 0.0006821805231886411, 0.024621266867914757, 0.0015725311925386953, 0.019667009895687468, 0.11269947515824666], [0.9170442990845996, 1.0212617482286084, 1.1944079283981166, 1.0590619216414618, 1.1419596326055175, 1.1536801983037628]
Training Loss (progress: 0.96), 0.13057896786696058, [0.02072607479602385, 0.0006596203636697572, 0.024880496285556036, 0.0015711792561777576, 0.019779773377538168, 0.11296197889672298], [0.9167734046379339, 1.0210745806878418, 1.1949578203286118, 1.0589833095507382, 1.1424110002112904, 1.153983973325085]
Evaluation on validation dataset:
Step 25, mean loss 0.025884570799685046
Step 50, mean loss 0.0161793355141092
Step 75, mean loss 0.02802349168600067
Step 100, mean loss 0.02783413144439576
Step 125, mean loss 0.04127717080782186
Step 150, mean loss 0.038574316622745033
Step 175, mean loss 0.06761437541486001
Step 200, mean loss 0.16953620784939963
Step 225, mean loss 0.12074636308451808
Unrolled forward losses 1.7620410770108847
Unrolled forward base losses 2.565701273852575
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.11388178307373192, [0.02073316228125184, 0.0006281442461266881, 0.02496944395935805, 0.0015994949167035916, 0.01979907604211251, 0.11301584386297069], [0.9166722746658119, 1.020945979638125, 1.1950094637129636, 1.0590259203197483, 1.1425645206921606, 1.1541253750651244]
Training Loss (progress: 0.08), 0.11094289559840266, [0.020739369703751304, 0.0006548836475186547, 0.024990135678714137, 0.0015850215331306963, 0.01984536319620298, 0.11318639239135676], [0.9163534170923657, 1.0208353507678303, 1.1953418905044968, 1.0589751717556488, 1.142764880381216, 1.1543884981590562]
Training Loss (progress: 0.16), 0.1244411631856838, [0.02076170913965786, 0.0006400668077163749, 0.02517755662410081, 0.0015831138363226283, 0.01985268091465826, 0.11329826119871952], [0.9160493148414294, 1.0207336793866526, 1.195829532869274, 1.0588234588026024, 1.1428842159852421, 1.1545772850038776]
Training Loss (progress: 0.24), 0.13203491555527916, [0.02070389932316575, 0.0006807419106362226, 0.025505523246256278, 0.0015686385772515853, 0.019906578567705297, 0.11341628411208997], [0.9157657358675331, 1.0204599015795304, 1.1963188074922086, 1.0588060630665415, 1.143353746362194, 1.154875155880842]
Training Loss (progress: 0.32), 0.12363658867929704, [0.020768048842225096, 0.0006822407539883069, 0.025620965816557588, 0.0015756403829192097, 0.020006824993140628, 0.11350172249268399], [0.91563174771106, 1.0202954410835892, 1.1967175066827698, 1.0585285943951726, 1.143847418426415, 1.155024167455607]
Training Loss (progress: 0.40), 0.13551656667066647, [0.020784205711773366, 0.0006857792033719126, 0.025854116518718828, 0.0015912348096551359, 0.020130887398356927, 0.11363207447927717], [0.9153324011850761, 1.0200876418285223, 1.197032021004677, 1.058480965936008, 1.144163798110115, 1.1553482597719111]
Training Loss (progress: 0.48), 0.12473872065081867, [0.020940150292423222, 0.0006625811847086176, 0.026132103887948982, 0.0015972179546057804, 0.02007870402952006, 0.11378719175518168], [0.9151120604637564, 1.0199193573769503, 1.1976138815293935, 1.058485844644487, 1.144466371395192, 1.155557732287587]
Training Loss (progress: 0.56), 0.12254735494754536, [0.020947851968148606, 0.0007097349245005465, 0.026444922436149095, 0.0015653141626761635, 0.02020168676830585, 0.11402256841763672], [0.9147436258726406, 1.0198520681386425, 1.1981392104278503, 1.0583010398024675, 1.1449056893110199, 1.155949256815995]
Training Loss (progress: 0.64), 0.12901825918815302, [0.021004775490011084, 0.0006811335368465917, 0.02665275978515546, 0.001576214299245203, 0.02022905858615997, 0.11424017488028963], [0.9145394502516222, 1.0197098218146987, 1.1985660376366778, 1.0582595532835122, 1.145047452215716, 1.1562074285038113]
Training Loss (progress: 0.72), 0.11942114570596005, [0.02099463110891374, 0.0006668779133432372, 0.027031431526437062, 0.0015650372074449382, 0.020316481376600026, 0.11433256978530922], [0.914289458537007, 1.0194937954302796, 1.199166926016593, 1.058251271881548, 1.1454618946038029, 1.1564355787331455]
Training Loss (progress: 0.80), 0.11244242262703925, [0.021109836387022112, 0.0006717137834143492, 0.02736442939449726, 0.0015766628264211632, 0.020383501620889522, 0.1144694566558978], [0.9141161334185507, 1.0193859546204935, 1.1995248626784525, 1.058022987847033, 1.1457518624013971, 1.1565618194866676]
Training Loss (progress: 0.88), 0.14121342078695617, [0.02112955100446847, 0.0007011880759525199, 0.027643068391910653, 0.0016098077629002271, 0.020586174155004498, 0.11470950200482873], [0.9138280510557326, 1.0191623840507038, 1.1998067018138159, 1.0579014575913126, 1.1462821352988584, 1.1569608498602901]
Training Loss (progress: 0.96), 0.12222401805478458, [0.021136077928024726, 0.000667216221842759, 0.027803119979690195, 0.0015743134360012162, 0.02058089670694987, 0.11488162390204999], [0.9134913439228887, 1.0188489395746045, 1.2001673035044393, 1.0578246662776611, 1.1464979433376714, 1.1572302091733593]
Evaluation on validation dataset:
Step 25, mean loss 0.025326554755451883
Step 50, mean loss 0.0151847753134781
Step 75, mean loss 0.025231661129430313
Step 100, mean loss 0.03152844214235692
Step 125, mean loss 0.03845169439941457
Step 150, mean loss 0.0362389076909487
Step 175, mean loss 0.05965328394270863
Step 200, mean loss 0.20389040196038644
Step 225, mean loss 0.128867680995911
Unrolled forward losses 1.6360992981583649
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.67175e-01, 3.68704e-03, 1.05304e+00, 1.03561e-01, 9.59380e-01, 1.12094e+00
Node: 01 (pos: 0.010): 8.09147e-01, 2.05427e-02, 1.09606e+00, 2.10656e-01, 1.01310e+00, 1.13194e+00
Node: 02 (pos: 0.020): 8.45190e-01, 8.37536e-02, 1.13256e+00, 3.76601e-01, 1.05928e+00, 1.14102e+00
Node: 03 (pos: 0.030): 8.74330e-01, 2.49872e-01, 1.16179e+00, 5.91726e-01, 1.09665e+00, 1.14814e+00
Node: 04 (pos: 0.040): 8.95757e-01, 5.45505e-01, 1.18313e+00, 8.17132e-01, 1.12415e+00, 1.15325e+00
Node: 05 (pos: 0.051): 9.08865e-01, 8.71461e-01, 1.19612e+00, 9.91733e-01, 1.14098e+00, 1.15632e+00
-
Node: 07 (pos: 0.071): 9.08865e-01, 8.71461e-01, 1.19612e+00, 9.91733e-01, 1.14098e+00, 1.15632e+00
Node: 08 (pos: 0.081): 8.95757e-01, 5.45505e-01, 1.18313e+00, 8.17132e-01, 1.12415e+00, 1.15325e+00
Node: 09 (pos: 0.091): 8.74330e-01, 2.49872e-01, 1.16179e+00, 5.91726e-01, 1.09665e+00, 1.14814e+00
Node: 10 (pos: 0.101): 8.45190e-01, 8.37536e-02, 1.13256e+00, 3.76601e-01, 1.05928e+00, 1.14102e+00
Node: 11 (pos: 0.111): 8.09147e-01, 2.05427e-02, 1.09606e+00, 2.10656e-01, 1.01310e+00, 1.13194e+00
Node: 12 (pos: 0.121): 7.67175e-01, 3.68704e-03, 1.05304e+00, 1.03561e-01, 9.59380e-01, 1.12094e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.08865e-01, 8.71461e-01, 1.19612e+00, 9.91733e-01, 1.14098e+00, 1.15632e+00
Node: 58 (pos: 0.586): 8.95757e-01, 5.45505e-01, 1.18313e+00, 8.17132e-01, 1.12415e+00, 1.15325e+00
Node: 59 (pos: 0.596): 8.74330e-01, 2.49872e-01, 1.16179e+00, 5.91726e-01, 1.09665e+00, 1.14814e+00
Node: 60 (pos: 0.606): 8.45190e-01, 8.37536e-02, 1.13256e+00, 3.76601e-01, 1.05928e+00, 1.14102e+00
Node: 61 (pos: 0.616): 8.09147e-01, 2.05427e-02, 1.09606e+00, 2.10656e-01, 1.01310e+00, 1.13194e+00
Node: 50 (pos: 0.505): 7.67175e-01, 3.68704e-03, 1.05304e+00, 1.03561e-01, 9.59380e-01, 1.12094e+00
-
Node: 51 (pos: 0.515): 8.09147e-01, 2.05427e-02, 1.09606e+00, 2.10656e-01, 1.01310e+00, 1.13194e+00
Node: 52 (pos: 0.525): 8.45190e-01, 8.37536e-02, 1.13256e+00, 3.76601e-01, 1.05928e+00, 1.14102e+00
Node: 53 (pos: 0.535): 8.74330e-01, 2.49872e-01, 1.16179e+00, 5.91726e-01, 1.09665e+00, 1.14814e+00
Node: 54 (pos: 0.545): 8.95757e-01, 5.45505e-01, 1.18313e+00, 8.17132e-01, 1.12415e+00, 1.15325e+00
Node: 55 (pos: 0.556): 9.08865e-01, 8.71461e-01, 1.19612e+00, 9.91733e-01, 1.14098e+00, 1.15632e+00
Node: 62 (pos: 0.626): 7.67175e-01, 3.68704e-03, 1.05304e+00, 1.03561e-01, 9.59380e-01, 1.12094e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.02272315946045792
Step 50, mean loss 0.011325033261305528
Step 75, mean loss 0.01909398827752313
Step 100, mean loss 0.0217266288106593
Step 125, mean loss 0.03809414384238744
Step 150, mean loss 0.03266036867247807
Step 175, mean loss 0.05682966247889172
Step 200, mean loss 0.06879391028718458
Step 225, mean loss 0.08043156535063814
Unrolled forward losses 1.3259375243546823
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.1111403945134467, [0.021105359412816004, 0.0006561212378426998, 0.028055979423277266, 0.0016012854183734958, 0.020588850775392922, 0.11489819057899327], [0.9132884482366941, 1.0187906507371594, 1.2005932699757813, 1.0578365289757505, 1.1466817699901328, 1.157340347262796]
Training Loss (progress: 0.08), 0.11879323084773094, [0.021258472685021457, 0.0006828354396756218, 0.028175598104108888, 0.0016113643891182863, 0.020679266798880457, 0.11504803399847077], [0.9134046592989007, 1.0188063621257242, 1.2008662915066177, 1.0578816344050754, 1.1470193416894323, 1.1576068244021667]
Training Loss (progress: 0.16), 0.1181907006697269, [0.021233566867650686, 0.000653532415263458, 0.028406906531582034, 0.0016032033907931677, 0.02077449603695885, 0.11520110867452221], [0.9133080374744178, 1.0187321106738239, 1.2012331323870156, 1.0579116746406858, 1.1473511880783602, 1.1578465698533031]
Training Loss (progress: 0.24), 0.11288790225665635, [0.02126633830916927, 0.0006719199851466458, 0.028473357616607395, 0.0015908280725473116, 0.020778901689512574, 0.11535732571161532], [0.9132656714472511, 1.0186775671049455, 1.2014932145650183, 1.0579017212874664, 1.1474800400703238, 1.1580539137590569]
Training Loss (progress: 0.32), 0.11539799634359281, [0.02124572972708728, 0.0006745780946707747, 0.028627653431068822, 0.0016082206144111349, 0.020892370054714542, 0.11543561633740342], [0.9131978004798506, 1.0186064722830774, 1.2017967388815072, 1.0579342920999835, 1.1476763068596139, 1.1582766826995812]
Training Loss (progress: 0.40), 0.10807089307791233, [0.021283982447745552, 0.0006571647958268159, 0.0286529070477881, 0.0016032309573091185, 0.020835468514422615, 0.11552450715617998], [0.9131082408534765, 1.0186195191461738, 1.201982298631088, 1.0579077289255037, 1.1478609511643627, 1.1584912417439148]
Training Loss (progress: 0.48), 0.10585101963083311, [0.021285664542609276, 0.0006679539038179236, 0.028936580339693353, 0.0016157728810061406, 0.02092366475539575, 0.11562372956340462], [0.9130794923772204, 1.0185539359032785, 1.2023350385258047, 1.0579161697951038, 1.148089891510469, 1.1587113256167978]
Training Loss (progress: 0.56), 0.10971749346404137, [0.021263109328708025, 0.0006624521105650536, 0.029095242037643076, 0.0016010804672733415, 0.020910516444487135, 0.11573132409531024], [0.9129883451877592, 1.0185201120156213, 1.2026250981119082, 1.0578912491170958, 1.1483327254302376, 1.1589282399306327]
Training Loss (progress: 0.64), 0.1104112245029404, [0.02132374810012066, 0.0006756776515230035, 0.029232052422714185, 0.0015955916592265455, 0.020959211959890207, 0.11584532413617779], [0.9129716499494825, 1.0184681868853798, 1.2029511547796345, 1.057917736443849, 1.1485184099457684, 1.1591276352493436]
Training Loss (progress: 0.72), 0.10766551526500182, [0.021341883646052672, 0.0006620606968484731, 0.02919212338434131, 0.0015944694959063792, 0.020979504754148347, 0.11586435473282765], [0.9129017293930803, 1.018411226335073, 1.2030469463676068, 1.0578793953086914, 1.1486928252517732, 1.1592369403983849]
Training Loss (progress: 0.80), 0.1149780782313679, [0.02135469451513901, 0.0006809693095655287, 0.029336308400147004, 0.0015921161344392954, 0.021025956334941674, 0.11589358555844262], [0.9128484926620404, 1.0183488429748746, 1.203266065329465, 1.0579333641440067, 1.148897145464417, 1.1593605977726613]
Training Loss (progress: 0.88), 0.10933455246191501, [0.021371802356058074, 0.0006762959846312035, 0.029517523450437247, 0.0016098763229671768, 0.021083008250917048, 0.11597842766948008], [0.9127643719557983, 1.0182840826209059, 1.203615239259304, 1.0579169325021878, 1.1491430326553886, 1.1596103904395807]
Training Loss (progress: 0.96), 0.09833443161404887, [0.02134494413354407, 0.0006603117461377935, 0.02960403753537575, 0.0016057485077657552, 0.021151786192764054, 0.11612239197992225], [0.9126505699915742, 1.0181600009847687, 1.203830910754127, 1.0579435216654478, 1.1493239312936776, 1.159828648758201]
Evaluation on validation dataset:
Step 25, mean loss 0.023030674568560408
Step 50, mean loss 0.015028386257092689
Step 75, mean loss 0.0232722996920997
Step 100, mean loss 0.026817901014842938
Step 125, mean loss 0.03660093801994914
Step 150, mean loss 0.034797753122147904
Step 175, mean loss 0.05907384444308383
Step 200, mean loss 0.1807796282206863
Step 225, mean loss 0.12024110407781899
Unrolled forward losses 1.5368005014202166
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.68409e-01, 4.11531e-03, 1.06346e+00, 1.03600e-01, 9.66198e-01, 1.12378e+00
Node: 01 (pos: 0.010): 8.09869e-01, 2.21681e-02, 1.10453e+00, 2.10714e-01, 1.01884e+00, 1.13470e+00
Node: 02 (pos: 0.020): 8.45449e-01, 8.79196e-02, 1.13932e+00, 3.76674e-01, 1.06404e+00, 1.14370e+00
Node: 03 (pos: 0.030): 8.74200e-01, 2.56729e-01, 1.16713e+00, 5.91803e-01, 1.10058e+00, 1.15076e+00
Node: 04 (pos: 0.040): 8.95333e-01, 5.51945e-01, 1.18741e+00, 8.17200e-01, 1.12745e+00, 1.15582e+00
Node: 05 (pos: 0.051): 9.08257e-01, 8.73672e-01, 1.19975e+00, 9.91788e-01, 1.14388e+00, 1.15887e+00
-
Node: 07 (pos: 0.071): 9.08257e-01, 8.73672e-01, 1.19975e+00, 9.91788e-01, 1.14388e+00, 1.15887e+00
Node: 08 (pos: 0.081): 8.95333e-01, 5.51945e-01, 1.18741e+00, 8.17200e-01, 1.12745e+00, 1.15582e+00
Node: 09 (pos: 0.091): 8.74200e-01, 2.56729e-01, 1.16713e+00, 5.91803e-01, 1.10058e+00, 1.15076e+00
Node: 10 (pos: 0.101): 8.45449e-01, 8.79196e-02, 1.13932e+00, 3.76674e-01, 1.06404e+00, 1.14370e+00
Node: 11 (pos: 0.111): 8.09869e-01, 2.21681e-02, 1.10453e+00, 2.10714e-01, 1.01884e+00, 1.13470e+00
Node: 12 (pos: 0.121): 7.68409e-01, 4.11531e-03, 1.06346e+00, 1.03600e-01, 9.66198e-01, 1.12378e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.08257e-01, 8.73672e-01, 1.19975e+00, 9.91788e-01, 1.14388e+00, 1.15887e+00
Node: 58 (pos: 0.586): 8.95333e-01, 5.51945e-01, 1.18741e+00, 8.17200e-01, 1.12745e+00, 1.15582e+00
Node: 59 (pos: 0.596): 8.74200e-01, 2.56729e-01, 1.16713e+00, 5.91803e-01, 1.10058e+00, 1.15076e+00
Node: 60 (pos: 0.606): 8.45449e-01, 8.79196e-02, 1.13932e+00, 3.76674e-01, 1.06404e+00, 1.14370e+00
Node: 61 (pos: 0.616): 8.09869e-01, 2.21681e-02, 1.10453e+00, 2.10714e-01, 1.01884e+00, 1.13470e+00
Node: 50 (pos: 0.505): 7.68409e-01, 4.11531e-03, 1.06346e+00, 1.03600e-01, 9.66198e-01, 1.12378e+00
-
Node: 51 (pos: 0.515): 8.09869e-01, 2.21681e-02, 1.10453e+00, 2.10714e-01, 1.01884e+00, 1.13470e+00
Node: 52 (pos: 0.525): 8.45449e-01, 8.79196e-02, 1.13932e+00, 3.76674e-01, 1.06404e+00, 1.14370e+00
Node: 53 (pos: 0.535): 8.74200e-01, 2.56729e-01, 1.16713e+00, 5.91803e-01, 1.10058e+00, 1.15076e+00
Node: 54 (pos: 0.545): 8.95333e-01, 5.51945e-01, 1.18741e+00, 8.17200e-01, 1.12745e+00, 1.15582e+00
Node: 55 (pos: 0.556): 9.08257e-01, 8.73672e-01, 1.19975e+00, 9.91788e-01, 1.14388e+00, 1.15887e+00
Node: 62 (pos: 0.626): 7.68409e-01, 4.11531e-03, 1.06346e+00, 1.03600e-01, 9.66198e-01, 1.12378e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.02062428039379858
Step 50, mean loss 0.010555341258551211
Step 75, mean loss 0.017141755157291015
Step 100, mean loss 0.022292713850589303
Step 125, mean loss 0.032890289487070304
Step 150, mean loss 0.0329859359905246
Step 175, mean loss 0.05298536790251516
Step 200, mean loss 0.06925419396284994
Step 225, mean loss 0.07934983921777164
Unrolled forward losses 1.324284885599524
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.10987324985112372, [0.021350537019204963, 0.0006962215232434179, 0.029527895800672388, 0.0015964466524445258, 0.021164045944541986, 0.11615808670954185], [0.912576995393821, 1.018183877533071, 1.2038405755300825, 1.0579374927800453, 1.1494247523898602, 1.1599163878127594]
Training Loss (progress: 0.08), 0.11325448883695594, [0.02137914289187425, 0.0006638924898593754, 0.02977508203635472, 0.001597924179459226, 0.021270088547639798, 0.116224785005871], [0.9125770852084595, 1.0181581000768454, 1.2042191610800281, 1.0579141409541688, 1.1497227398320309, 1.160099731949798]
Training Loss (progress: 0.16), 0.10197425009750087, [0.021381230640878976, 0.0006471468555443454, 0.02996130952505493, 0.0016011883378218318, 0.021277826064665793, 0.11636348142438309], [0.9124915384277759, 1.0180692930355062, 1.2044006917986572, 1.0579089971579294, 1.1497735977914698, 1.160302619019393]
Training Loss (progress: 0.24), 0.09696460957172474, [0.021347809511423222, 0.0006774121677212198, 0.03002232942348651, 0.0015950317730460658, 0.02137155991038053, 0.1164195999860153], [0.9124042694656611, 1.0179737314984376, 1.2046128357970474, 1.057848650319143, 1.1500813103103928, 1.1604744175508588]
Training Loss (progress: 0.32), 0.1108630025280547, [0.021375898622156685, 0.0006822733008364467, 0.03012475909788215, 0.0016111586127860772, 0.02138595567474103, 0.11650034544974493], [0.9123314705159886, 1.017966051674973, 1.2049443032506089, 1.0578606072342405, 1.1502664997283722, 1.1606415512082093]
Training Loss (progress: 0.40), 0.11253171727497016, [0.021375694100183244, 0.0006669693391802042, 0.0302170094149402, 0.0015949930664980398, 0.021457020549849423, 0.11656206756567851], [0.912255848137978, 1.0179029103648058, 1.2051349402908935, 1.0578336153455234, 1.1504711829518413, 1.1608086224932452]
Training Loss (progress: 0.48), 0.10773665026007131, [0.02139132582117192, 0.0006506024886071094, 0.030426243569543193, 0.0016048068110104782, 0.02148636297548841, 0.11660374469377308], [0.9121894596359414, 1.0179109851202, 1.2054262915964205, 1.0579106671572223, 1.1506519614695958, 1.1609306687557461]
Training Loss (progress: 0.56), 0.1024789297013396, [0.021423206057846755, 0.0006783033831350407, 0.030530767571109146, 0.0016021922716330248, 0.021497419842847306, 0.11669363982515506], [0.9121383382559509, 1.0177699444163153, 1.205677753728066, 1.0579213595645518, 1.1508035673384067, 1.1611078187714177]
Training Loss (progress: 0.64), 0.11354036835469858, [0.021401538833481306, 0.0006661352509160323, 0.0306372030166381, 0.0015832848356457055, 0.021664261302329242, 0.11677289026335662], [0.9120159397631123, 1.0176932894049104, 1.2058853125893476, 1.05784597269604, 1.1511663531523109, 1.1613224036342693]
Training Loss (progress: 0.72), 0.0993155494957756, [0.021466320890730948, 0.0006865331249594608, 0.030849594060092324, 0.0016011196671794914, 0.021645915422572912, 0.11690446259661313], [0.9119650569454448, 1.0176905818199937, 1.206191021186838, 1.0578698361724959, 1.1512599484281982, 1.1615424574805386]
Training Loss (progress: 0.80), 0.10391708631415958, [0.02144499888790513, 0.0006743877524128769, 0.03091107914915898, 0.001580837769615945, 0.021642246529826437, 0.11698077614805914], [0.9118584370684324, 1.017672528142656, 1.2064140870352933, 1.057811004653862, 1.1514681430464038, 1.1617126614293345]
Training Loss (progress: 0.88), 0.11481234316926668, [0.021478190367879898, 0.0006824237456670193, 0.03111244858380583, 0.001589180269002405, 0.02163350729448269, 0.11707818703700297], [0.9118116699847287, 1.017652193417348, 1.2066107283541663, 1.0577846950843006, 1.1514504129761103, 1.1618864410264662]
Training Loss (progress: 0.96), 0.11731858015911405, [0.021466656502073838, 0.0006846425322682244, 0.03128200249043468, 0.0016006069470399977, 0.021721522312522182, 0.11710486436597457], [0.9116710167339718, 1.0175683852711122, 1.2069623147851545, 1.0578350702626418, 1.15175961908494, 1.1619720516207996]
Evaluation on validation dataset:
Step 25, mean loss 0.02226905303020879
Step 50, mean loss 0.014124569310487153
Step 75, mean loss 0.022883604313037143
Step 100, mean loss 0.0279250755728596
Step 125, mean loss 0.03497079869207764
Step 150, mean loss 0.032574500015671935
Step 175, mean loss 0.052565366107888395
Step 200, mean loss 0.1702271258466305
Step 225, mean loss 0.10875925230050801
Unrolled forward losses 1.5001758256738413
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.68259e-01, 4.05392e-03, 1.07321e+00, 1.06956e-01, 9.72925e-01, 1.12623e+00
Node: 01 (pos: 0.010): 8.09504e-01, 2.19335e-02, 1.11244e+00, 2.15424e-01, 1.02441e+00, 1.13707e+00
Node: 02 (pos: 0.020): 8.44891e-01, 8.73024e-02, 1.14559e+00, 3.82026e-01, 1.06856e+00, 1.14601e+00
Node: 03 (pos: 0.030): 8.73481e-01, 2.55640e-01, 1.17206e+00, 5.96489e-01, 1.10420e+00, 1.15301e+00
Node: 04 (pos: 0.040): 8.94492e-01, 5.50703e-01, 1.19135e+00, 8.20020e-01, 1.13039e+00, 1.15804e+00
Node: 05 (pos: 0.051): 9.07341e-01, 8.72750e-01, 1.20307e+00, 9.92561e-01, 1.14640e+00, 1.16107e+00
-
Node: 07 (pos: 0.071): 9.07341e-01, 8.72750e-01, 1.20307e+00, 9.92561e-01, 1.14640e+00, 1.16107e+00
Node: 08 (pos: 0.081): 8.94492e-01, 5.50703e-01, 1.19135e+00, 8.20020e-01, 1.13039e+00, 1.15804e+00
Node: 09 (pos: 0.091): 8.73481e-01, 2.55640e-01, 1.17206e+00, 5.96489e-01, 1.10420e+00, 1.15301e+00
Node: 10 (pos: 0.101): 8.44891e-01, 8.73024e-02, 1.14559e+00, 3.82026e-01, 1.06856e+00, 1.14601e+00
Node: 11 (pos: 0.111): 8.09504e-01, 2.19335e-02, 1.11244e+00, 2.15424e-01, 1.02441e+00, 1.13707e+00
Node: 12 (pos: 0.121): 7.68259e-01, 4.05392e-03, 1.07321e+00, 1.06956e-01, 9.72925e-01, 1.12623e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.07341e-01, 8.72750e-01, 1.20307e+00, 9.92561e-01, 1.14640e+00, 1.16107e+00
Node: 58 (pos: 0.586): 8.94492e-01, 5.50703e-01, 1.19135e+00, 8.20020e-01, 1.13039e+00, 1.15804e+00
Node: 59 (pos: 0.596): 8.73481e-01, 2.55640e-01, 1.17206e+00, 5.96489e-01, 1.10420e+00, 1.15301e+00
Node: 60 (pos: 0.606): 8.44891e-01, 8.73024e-02, 1.14559e+00, 3.82026e-01, 1.06856e+00, 1.14601e+00
Node: 61 (pos: 0.616): 8.09504e-01, 2.19335e-02, 1.11244e+00, 2.15424e-01, 1.02441e+00, 1.13707e+00
Node: 50 (pos: 0.505): 7.68259e-01, 4.05392e-03, 1.07321e+00, 1.06956e-01, 9.72925e-01, 1.12623e+00
-
Node: 51 (pos: 0.515): 8.09504e-01, 2.19335e-02, 1.11244e+00, 2.15424e-01, 1.02441e+00, 1.13707e+00
Node: 52 (pos: 0.525): 8.44891e-01, 8.73024e-02, 1.14559e+00, 3.82026e-01, 1.06856e+00, 1.14601e+00
Node: 53 (pos: 0.535): 8.73481e-01, 2.55640e-01, 1.17206e+00, 5.96489e-01, 1.10420e+00, 1.15301e+00
Node: 54 (pos: 0.545): 8.94492e-01, 5.50703e-01, 1.19135e+00, 8.20020e-01, 1.13039e+00, 1.15804e+00
Node: 55 (pos: 0.556): 9.07341e-01, 8.72750e-01, 1.20307e+00, 9.92561e-01, 1.14640e+00, 1.16107e+00
Node: 62 (pos: 0.626): 7.68259e-01, 4.05392e-03, 1.07321e+00, 1.06956e-01, 9.72925e-01, 1.12623e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.019713405538789794
Step 50, mean loss 0.009945542540149618
Step 75, mean loss 0.016754209539925062
Step 100, mean loss 0.02133691792614474
Step 125, mean loss 0.03328710130076724
Step 150, mean loss 0.03171925995684147
Step 175, mean loss 0.050412155567816086
Step 200, mean loss 0.06506908451625662
Step 225, mean loss 0.07338382169336367
Unrolled forward losses 1.2515316666589276
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.11274924830200786, [0.021498354351587295, 0.0006587287337568096, 0.0313155232804011, 0.0016125516141214339, 0.02175938808662902, 0.11719649760102353], [0.9117003772820936, 1.017495908333917, 1.207029798588694, 1.0578216833249392, 1.1517850796872062, 1.1621122838906375]
Training Loss (progress: 0.08), 0.10874561738390141, [0.02147086065517792, 0.0006762640653940732, 0.031349871840593437, 0.0016184654519383968, 0.021789607479588455, 0.11725670102367608], [0.911574207641906, 1.0174484115341051, 1.207109057834236, 1.0577931506652658, 1.1520313487119558, 1.1622732707829884]
Training Loss (progress: 0.16), 0.11759448912389486, [0.021499851400218623, 0.0006891693800900909, 0.03153274216095122, 0.0016093930843350532, 0.021870897640039737, 0.11732387689723399], [0.9115518942087243, 1.0173602184228119, 1.2075103955427025, 1.0577200325766667, 1.152298163466669, 1.16243196654064]
Training Loss (progress: 0.24), 0.11316946282852663, [0.02151995434737145, 0.00067763046983445, 0.03164197668091624, 0.0016113421037625298, 0.0219041104458724, 0.1173981366787652], [0.911449881318106, 1.0173094182493985, 1.2076771089396803, 1.0577035661007395, 1.152460344633836, 1.162618847889174]
Training Loss (progress: 0.32), 0.10929260470199785, [0.021560763730583115, 0.0006718556138001818, 0.031679059962972336, 0.0016073300243896042, 0.021928086007278864, 0.11743690352876777], [0.9114022188388081, 1.017287922165979, 1.2078684222434586, 1.0576558037236596, 1.1525874184162503, 1.1627046745787695]
Training Loss (progress: 0.40), 0.10762210496071839, [0.021490029036376028, 0.000673531910511534, 0.031794958384771324, 0.0016196656380794738, 0.021931500631684524, 0.11747280286054972], [0.9112400383572755, 1.0172927105043055, 1.2081218660298656, 1.0576922760549, 1.1527380569741703, 1.162874310439336]
Training Loss (progress: 0.48), 0.10775255278879715, [0.021499464714229057, 0.0006730015922403337, 0.03195125851702681, 0.001595187393260775, 0.02200431875521894, 0.11751034904538397], [0.9111722804253561, 1.0171667519786805, 1.2082306137443717, 1.0576017150463675, 1.1529544512734093, 1.1629763783847364]
Training Loss (progress: 0.56), 0.09805329457581717, [0.02155345373734687, 0.0006862930543499393, 0.03226713312894187, 0.0016056916545578624, 0.022000597860611572, 0.11756711233071432], [0.9111481123021357, 1.0171322267303151, 1.208634155981469, 1.0576506242387838, 1.1530862294888464, 1.1630893216175227]
Training Loss (progress: 0.64), 0.10679930057043978, [0.02156427662693369, 0.0006627958435314165, 0.03239673836887505, 0.0016050551725688544, 0.02207840521043543, 0.11766708490996892], [0.9110990023262283, 1.017124126986548, 1.2088636035461027, 1.0576248273765911, 1.1532933085610324, 1.163288809312883]
Training Loss (progress: 0.72), 0.10807903684373552, [0.02152800347300053, 0.0006686682275609197, 0.032540931569574244, 0.001618136716671265, 0.022171052398083188, 0.11778486267353823], [0.9109661305978085, 1.017048614054022, 1.2090133700608439, 1.0576528862906271, 1.1535609309276647, 1.163490197423281]
Training Loss (progress: 0.80), 0.10890856153709008, [0.021551802440006724, 0.0006838685840974087, 0.032693321103109164, 0.0016025984774872056, 0.02220389938380695, 0.11788580246837368], [0.910906320821436, 1.0170046496440672, 1.209239681635571, 1.057611784916197, 1.1537335161938609, 1.1637384629889498]
Training Loss (progress: 0.88), 0.10864163349244502, [0.021579675198353722, 0.0006685412735768423, 0.0328220178141775, 0.0015933841828911011, 0.022222165041405232, 0.11791064231664596], [0.9108309385173738, 1.0168970946505957, 1.2093969038424937, 1.0575721823576985, 1.1538529977784988, 1.1638644479156175]
Training Loss (progress: 0.96), 0.11001926683866847, [0.021586161420744314, 0.0006785897442475834, 0.032958194900957256, 0.0016017227027946281, 0.022270353235550256, 0.11798310450737326], [0.9107578062585068, 1.0168661534425443, 1.2096148805216933, 1.0575350884396317, 1.1540571903379586, 1.1639916545808908]
Evaluation on validation dataset:
Step 25, mean loss 0.020275263677376733
Step 50, mean loss 0.013759014583326585
Step 75, mean loss 0.022873031613922137
Step 100, mean loss 0.026302835915375272
Step 125, mean loss 0.03404698866636004
Step 150, mean loss 0.03386130518174319
Step 175, mean loss 0.05326874892720761
Step 200, mean loss 0.17594438775743887
Step 225, mean loss 0.11152899619349518
Unrolled forward losses 1.5581890166899246
Unrolled forward base losses 2.565701273852575
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.1018189160509849, [0.021600291779641182, 0.000655108320025476, 0.03283433328483073, 0.0016134448304749376, 0.02228322171053951, 0.1180408228726803], [0.910759472409335, 1.0168040397710356, 1.2096075499477592, 1.0575659599004903, 1.1541550861474437, 1.1640951643516408]
Training Loss (progress: 0.08), 0.11884648606051315, [0.021600982362099366, 0.0006802274149840364, 0.03303405968373917, 0.0016072935339883532, 0.022344157816862557, 0.11816552278916731], [0.9106696510225787, 1.0167726533261996, 1.2098623938607869, 1.0576049526060782, 1.154320171323691, 1.164317978404243]
Training Loss (progress: 0.16), 0.10361699020721159, [0.021636203226114988, 0.0006795648935091234, 0.03319404042039296, 0.0015796270397092382, 0.022393805819819187, 0.11818211757728085], [0.910613748236651, 1.0166892149457751, 1.210102816117046, 1.0576001261776948, 1.1545002063776657, 1.164391025518808]
Training Loss (progress: 0.24), 0.11015311792902234, [0.02161589190233892, 0.0006892095642864231, 0.033306038175374156, 0.0016042984726770428, 0.02240857628762101, 0.11829717915324933], [0.910513242604404, 1.016634766565962, 1.2102621069104804, 1.0575381106552904, 1.154635072783527, 1.1645447920277108]
Training Loss (progress: 0.32), 0.10823036756412888, [0.021567552978112484, 0.0006778755473599986, 0.03354309680068706, 0.0016203831912260812, 0.022388315836096483, 0.1184104324356419], [0.9103438668986064, 1.01659203264883, 1.2105800942786427, 1.0575136151563442, 1.1547118854542682, 1.1647429084751206]
Training Loss (progress: 0.40), 0.11667109940495923, [0.021609952091131885, 0.0006704319336676137, 0.033708881126321616, 0.0016159402900301353, 0.02246580827617097, 0.11850691817007335], [0.9102917879082338, 1.01650348189175, 1.210829597832561, 1.0574849512002786, 1.1549996024528897, 1.1649151946165544]
Training Loss (progress: 0.48), 0.10758841533602549, [0.021665919718187106, 0.0006839300197460991, 0.033671496064423385, 0.0016163844206155528, 0.022513296664520344, 0.11849764903138704], [0.9102731821685945, 1.0163885589244837, 1.2109480988497603, 1.0574445266402095, 1.1552484255172322, 1.164967669019565]
Training Loss (progress: 0.56), 0.10321387604090157, [0.021656844933839212, 0.0006858820009468776, 0.0338567969754059, 0.0016022429780306311, 0.022547593203476407, 0.11862064795120995], [0.9101836047825396, 1.016413205013727, 1.2111301023241015, 1.0574830368672175, 1.1553465644755083, 1.165240929645801]
Training Loss (progress: 0.64), 0.10750428308560203, [0.021572716117108103, 0.0006747306275709646, 0.033886209695768575, 0.001617963968939646, 0.022602856621663742, 0.1186660945451577], [0.9100249347878145, 1.0163635200023267, 1.211287839879722, 1.057523777357852, 1.1555873805726526, 1.1653825807017613]
Training Loss (progress: 0.72), 0.1037456532225348, [0.021610621819624084, 0.0006669109037185652, 0.03421076716131546, 0.001604809548174388, 0.022692385006921244, 0.1187317758215176], [0.9099713515881616, 1.016340975508348, 1.21170584164948, 1.0574251631739262, 1.1558197738533025, 1.1654634878503964]
Training Loss (progress: 0.80), 0.11328120938013977, [0.02163767728380889, 0.0006699190086039224, 0.03436894080301306, 0.0016101658580983903, 0.022746924981582935, 0.11887275320402789], [0.9099050712788748, 1.0162546467590898, 1.2119550168055526, 1.0573893097619491, 1.155976159039752, 1.165654792981782]
Training Loss (progress: 0.88), 0.10769710616122158, [0.02162332861224806, 0.0006760636608945983, 0.03456487095498821, 0.0015924741025265974, 0.022781768637489125, 0.11888080989585247], [0.9098004825920918, 1.0161768687405321, 1.2122668535801382, 1.0573967799290678, 1.1562020939712516, 1.1657823195654904]
Training Loss (progress: 0.96), 0.103523107882042, [0.021637016464581948, 0.000665768607653036, 0.034636261865481875, 0.001590828942402147, 0.022825907737583383, 0.11891478126483264], [0.9097366806071956, 1.0161519291667531, 1.2123890896006402, 1.0573920416107119, 1.1563700790181652, 1.1658745054441733]
Evaluation on validation dataset:
Step 25, mean loss 0.021456026034228717
Step 50, mean loss 0.014005639880429972
Step 75, mean loss 0.022207776512462182
Step 100, mean loss 0.028856068999569995
Step 125, mean loss 0.034823994333915614
Step 150, mean loss 0.03416495576934164
Step 175, mean loss 0.055377047677115994
Step 200, mean loss 0.15103891020510513
Step 225, mean loss 0.11314034485359949
Unrolled forward losses 1.5595042839297053
Unrolled forward base losses 2.565701273852575
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00), 0.09975143847313947, [0.02165391093842597, 0.0006913576340913925, 0.034693207049016114, 0.0016145687406125112, 0.02286489893112506, 0.11900492688841544], [0.9097219218202631, 1.0161544748631282, 1.2124940378969764, 1.0574049055636636, 1.1565020505170247, 1.1660448296775794]
Training Loss (progress: 0.08), 0.10289620523934252, [0.021627735131082515, 0.0006774394153387305, 0.0347990325719693, 0.0016051052054083053, 0.022902817057327975, 0.11909037853211955], [0.9095832707598629, 1.0161235564143904, 1.2127618179601527, 1.0573983102431044, 1.1566415170009812, 1.1662302975741987]
Training Loss (progress: 0.16), 0.10694266453962241, [0.021628494706365877, 0.0006776263006465273, 0.034968986532212996, 0.0015917258796795334, 0.022933138732553093, 0.11914179401339878], [0.9095216189606354, 1.0160570205278834, 1.2129977178049218, 1.0573906279553391, 1.156848053020676, 1.1663407173070748]
Training Loss (progress: 0.24), 0.10867762345184019, [0.021643498652410643, 0.0006844675741894536, 0.03522640848094533, 0.0015959217503656046, 0.022965722404706304, 0.11918642156825639], [0.9094897496668443, 1.015962096408756, 1.2132701627106486, 1.057337958433615, 1.157004015811604, 1.1664630163043372]
Training Loss (progress: 0.32), 0.111454058112459, [0.02167645124706552, 0.0006726053753833855, 0.03525561669268, 0.0016082214926154887, 0.02299159032513271, 0.11924490640365933], [0.9093918810508251, 1.0158721156343122, 1.2133496640276402, 1.057271225133746, 1.157152898877443, 1.166630822986296]
Training Loss (progress: 0.40), 0.1064531246221249, [0.021700154122075908, 0.0006995052172055089, 0.03534625089876044, 0.0015935814634151565, 0.023055535069507645, 0.11937684070678856], [0.9092964067988166, 1.0158178138761687, 1.2135363288496983, 1.0572771506415481, 1.1572979675325272, 1.1668192121284897]
Training Loss (progress: 0.48), 0.10109149180131241, [0.02173987392667368, 0.0006875515318487068, 0.03557953866256256, 0.0015967956512651384, 0.02306046624002872, 0.11940489176337188], [0.9092520007413286, 1.0156743084641708, 1.2137990277048822, 1.0572742902938106, 1.1575289615328257, 1.1669491778972516]
Training Loss (progress: 0.56), 0.10638232103479911, [0.021730030107020928, 0.0006770278063747916, 0.03578916315417803, 0.0016215352151961952, 0.02310659844716677, 0.11944882732469823], [0.9091537479379972, 1.0156439687008842, 1.213985877709704, 1.0573799658370895, 1.157735395110427, 1.167126656814693]
Training Loss (progress: 0.64), 0.10851915122857875, [0.02171798567944106, 0.0006820256722678835, 0.03592495350682005, 0.0015931202660729017, 0.02316018512836409, 0.11954441503945583], [0.9090399840097737, 1.0155776103779455, 1.214238078175274, 1.0572970393775383, 1.157912310001286, 1.1672811781678811]
Training Loss (progress: 0.72), 0.11227836115705295, [0.02174849210880658, 0.0006917048802874353, 0.03602377484731436, 0.001592560163364405, 0.023157926296764355, 0.11959094704964664], [0.9089582454903081, 1.0155245506105035, 1.214444270310373, 1.0572543422352207, 1.15801969410759, 1.1674282967944605]
Training Loss (progress: 0.80), 0.10538925238871498, [0.021791773664019853, 0.0006650391993956478, 0.036150052426303696, 0.001607080244035922, 0.023254409292442726, 0.1197202750154065], [0.9088980294070594, 1.0155246717380508, 1.214611108944098, 1.0572213037179148, 1.1582603634827413, 1.167587891065994]
Training Loss (progress: 0.88), 0.10368315126332037, [0.021734263061330736, 0.0006810419605795673, 0.03627331025057343, 0.0015974255435384393, 0.023289789455372672, 0.11977826153906045], [0.908738081647635, 1.0154167048572136, 1.214729251626441, 1.0571858938958179, 1.158422936167179, 1.1677146976114614]
Training Loss (progress: 0.96), 0.1018350163186876, [0.021779020059110213, 0.0006949789824561021, 0.03632228960146478, 0.0016113571286218691, 0.02335440230936484, 0.1198489166637017], [0.9086982760030959, 1.0153744302560643, 1.2149451693383198, 1.0571997876752526, 1.1586216351259986, 1.1678667002336676]
Evaluation on validation dataset:
Step 25, mean loss 0.02078635385437546
Step 50, mean loss 0.01403173869080326
Step 75, mean loss 0.022618721952766232
Step 100, mean loss 0.02725100737745602
Step 125, mean loss 0.03519090776253165
Step 150, mean loss 0.03461402563219233
Step 175, mean loss 0.05512493874698696
Step 200, mean loss 0.17146214078487443
Step 225, mean loss 0.11330884271223879
Unrolled forward losses 1.612859250908714
Unrolled forward base losses 2.565701273852575
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00), 0.1038196238277803, [0.021779711749006413, 0.000682711509338296, 0.036419683002121735, 0.0016050553753746946, 0.023414236809643624, 0.1198823649027824], [0.9086890811476718, 1.0152888431636522, 1.2150713966545663, 1.0571847949636246, 1.158737697372539, 1.1679102375389918]
Training Loss (progress: 0.08), 0.10348141807181845, [0.02178590064222504, 0.0006767771835989788, 0.036434317819190896, 0.0015966627553426617, 0.02340539769813901, 0.11991526555512594], [0.9086423343450049, 1.0152702426281648, 1.215149203111136, 1.0571515662246065, 1.1587691155309425, 1.1679941863890324]
Training Loss (progress: 0.16), 0.10242834695906637, [0.021769538857594257, 0.0006883735134686176, 0.036531822094477105, 0.0016062490006278243, 0.02342957002769831, 0.1199291146201341], [0.9086132034285959, 1.0152500311256367, 1.2152428114471439, 1.0571849362636083, 1.1588756493180983, 1.1680845754062703]
Training Loss (progress: 0.24), 0.09350198655212016, [0.021770325237348176, 0.000682028965696783, 0.03659912000908034, 0.001591734386257518, 0.023461421884960355, 0.11998982638008882], [0.9085844546574123, 1.0152217544465942, 1.2153402005863085, 1.0571642118104136, 1.1589652060820306, 1.168205771131496]
Training Loss (progress: 0.32), 0.10992233433577005, [0.021763965220234736, 0.0006709989091522072, 0.036763136665534074, 0.0016103148490674048, 0.023488826704578924, 0.1200384177682025], [0.9085410167315735, 1.015198707602757, 1.2155006796571584, 1.0571311313074478, 1.1590855716342718, 1.1682848334062332]
Training Loss (progress: 0.40), 0.1037883378312176, [0.02175114850216608, 0.0006783794613733269, 0.0368131961854407, 0.0016115553601339734, 0.02348746904143461, 0.12007463501277617], [0.9084909157910578, 1.015189940827618, 1.2156080417662047, 1.057155802870598, 1.1591352503608, 1.1683908598300041]
Training Loss (progress: 0.48), 0.1058371118171952, [0.021742290545060817, 0.0006742022393712269, 0.036831351084209034, 0.0016081506240955795, 0.02352582610805221, 0.12008677377921864], [0.9084534672304748, 1.0151676537492893, 1.2156764373348876, 1.0571418617596564, 1.159255093665962, 1.1684430660219023]
Training Loss (progress: 0.56), 0.10279613703952145, [0.02174283275576517, 0.0006914222786656524, 0.03687332684994247, 0.001602580321399408, 0.023560959640649824, 0.12012857264696589], [0.9084487579071084, 1.0151846613936804, 1.2157255403325855, 1.057164911890027, 1.159378603266381, 1.1685258989517886]
Training Loss (progress: 0.64), 0.11026329907915154, [0.021766986605228248, 0.0006766084904062277, 0.03689390977991394, 0.0015970179835198834, 0.0235812014812364, 0.12018416588238216], [0.9084467054371088, 1.0151406556724136, 1.2158175902078394, 1.0571293360560245, 1.1594451195885793, 1.1686307591134468]
Training Loss (progress: 0.72), 0.09929984025524222, [0.021777459258242662, 0.000679126799870216, 0.03699557257049472, 0.0015964668337610864, 0.02362426152512697, 0.12023655195363235], [0.9084270792057726, 1.0151499456398845, 1.2159628720904059, 1.0571232220283728, 1.1595549356066304, 1.1687355612606638]
Training Loss (progress: 0.80), 0.10585765122178066, [0.0217886760030216, 0.0006782923496087404, 0.03700355806473853, 0.001605947713187667, 0.023587864884720932, 0.12024820540965231], [0.9084041090511858, 1.0151547138792933, 1.2160331298057565, 1.0571575769150134, 1.1595810726278353, 1.168792382159158]
Training Loss (progress: 0.88), 0.11713250198674698, [0.02179678652501424, 0.0006751268254150523, 0.03712071645033417, 0.0016088784419338571, 0.023621735752510538, 0.12027137604068273], [0.9083976008724528, 1.0151286414432406, 1.2161588836398252, 1.0571582897251501, 1.1596891816710417, 1.1688746375534904]
Training Loss (progress: 0.96), 0.09764611473076497, [0.021779925424134164, 0.0006745715471430812, 0.037186592703543624, 0.0016020177085993004, 0.023642089172347617, 0.12031358857704], [0.9083527189645736, 1.0150898694216226, 1.2162221832272926, 1.0571695683514102, 1.1597555880351724, 1.1689801914256919]
Evaluation on validation dataset:
Step 25, mean loss 0.02190060801156868
Step 50, mean loss 0.013000037434223571
Step 75, mean loss 0.020957392028454258
Step 100, mean loss 0.0248934703196733
Step 125, mean loss 0.031583737270955876
Step 150, mean loss 0.031322175384947644
Step 175, mean loss 0.05187791256448316
Step 200, mean loss 0.16539284918075192
Step 225, mean loss 0.10920796365507535
Unrolled forward losses 1.4733753679265442
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.67507e-01, 4.60510e-03, 1.10191e+00, 1.07105e-01, 9.93025e-01, 1.13386e+00
Node: 01 (pos: 0.010): 8.08059e-01, 2.39463e-02, 1.13566e+00, 2.15592e-01, 1.04127e+00, 1.14449e+00
Node: 02 (pos: 0.020): 8.42826e-01, 9.22690e-02, 1.16404e+00, 3.82134e-01, 1.08247e+00, 1.15326e+00
Node: 03 (pos: 0.030): 8.70898e-01, 2.63446e-01, 1.18660e+00, 5.96426e-01, 1.11565e+00, 1.16012e+00
Node: 04 (pos: 0.040): 8.91521e-01, 5.57372e-01, 1.20298e+00, 8.19706e-01, 1.13996e+00, 1.16505e+00
Node: 05 (pos: 0.051): 9.04128e-01, 8.73812e-01, 1.21292e+00, 9.92016e-01, 1.15481e+00, 1.16802e+00
-
Node: 07 (pos: 0.071): 9.04128e-01, 8.73812e-01, 1.21292e+00, 9.92016e-01, 1.15481e+00, 1.16802e+00
Node: 08 (pos: 0.081): 8.91521e-01, 5.57372e-01, 1.20298e+00, 8.19706e-01, 1.13996e+00, 1.16505e+00
Node: 09 (pos: 0.091): 8.70898e-01, 2.63446e-01, 1.18660e+00, 5.96426e-01, 1.11565e+00, 1.16012e+00
Node: 10 (pos: 0.101): 8.42826e-01, 9.22690e-02, 1.16404e+00, 3.82134e-01, 1.08247e+00, 1.15326e+00
Node: 11 (pos: 0.111): 8.08059e-01, 2.39463e-02, 1.13566e+00, 2.15592e-01, 1.04127e+00, 1.14449e+00
Node: 12 (pos: 0.121): 7.67507e-01, 4.60510e-03, 1.10191e+00, 1.07105e-01, 9.93025e-01, 1.13386e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.04128e-01, 8.73812e-01, 1.21292e+00, 9.92016e-01, 1.15481e+00, 1.16802e+00
Node: 58 (pos: 0.586): 8.91521e-01, 5.57372e-01, 1.20298e+00, 8.19706e-01, 1.13996e+00, 1.16505e+00
Node: 59 (pos: 0.596): 8.70898e-01, 2.63446e-01, 1.18660e+00, 5.96426e-01, 1.11565e+00, 1.16012e+00
Node: 60 (pos: 0.606): 8.42826e-01, 9.22690e-02, 1.16404e+00, 3.82134e-01, 1.08247e+00, 1.15326e+00
Node: 61 (pos: 0.616): 8.08059e-01, 2.39463e-02, 1.13566e+00, 2.15592e-01, 1.04127e+00, 1.14449e+00
Node: 50 (pos: 0.505): 7.67507e-01, 4.60510e-03, 1.10191e+00, 1.07105e-01, 9.93025e-01, 1.13386e+00
-
Node: 51 (pos: 0.515): 8.08059e-01, 2.39463e-02, 1.13566e+00, 2.15592e-01, 1.04127e+00, 1.14449e+00
Node: 52 (pos: 0.525): 8.42826e-01, 9.22690e-02, 1.16404e+00, 3.82134e-01, 1.08247e+00, 1.15326e+00
Node: 53 (pos: 0.535): 8.70898e-01, 2.63446e-01, 1.18660e+00, 5.96426e-01, 1.11565e+00, 1.16012e+00
Node: 54 (pos: 0.545): 8.91521e-01, 5.57372e-01, 1.20298e+00, 8.19706e-01, 1.13996e+00, 1.16505e+00
Node: 55 (pos: 0.556): 9.04128e-01, 8.73812e-01, 1.21292e+00, 9.92016e-01, 1.15481e+00, 1.16802e+00
Node: 62 (pos: 0.626): 7.67507e-01, 4.60510e-03, 1.10191e+00, 1.07105e-01, 9.93025e-01, 1.13386e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.019622200872802247
Step 50, mean loss 0.008873203130211475
Step 75, mean loss 0.014504615568716735
Step 100, mean loss 0.019531893093430903
Step 125, mean loss 0.02999339891274382
Step 150, mean loss 0.03002374149789522
Step 175, mean loss 0.047376420772059336
Step 200, mean loss 0.058454790024997746
Step 225, mean loss 0.07215326573416814
Unrolled forward losses 1.2318380784183156
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00), 0.10335226752306163, [0.021788929273549223, 0.0006748699256714689, 0.03719546569119138, 0.0016068022195420244, 0.023657747973227266, 0.12032952083824447], [0.9083547192142022, 1.015082590636094, 1.2162464204467671, 1.057136923690225, 1.1598061014076926, 1.1690228806611]
Training Loss (progress: 0.08), 0.10527306972991335, [0.021775479576086387, 0.0006803516682439664, 0.037258293496442114, 0.0016131648125730408, 0.02368838674410039, 0.12034716075198247], [0.9083085222174913, 1.0150948728840494, 1.2163836801985417, 1.0571602984652226, 1.1599197348068135, 1.1691341291385977]
Training Loss (progress: 0.16), 0.09950646688962976, [0.021772706794472685, 0.000682704291527858, 0.03731517415581715, 0.0016046773787472919, 0.023700305184986347, 0.1203862740598098], [0.9082704557911176, 1.0150678773921484, 1.2165099136091397, 1.057171864895034, 1.160034465908117, 1.1692123279177125]
Training Loss (progress: 0.24), 0.10605896416062567, [0.021799992911201344, 0.0006766705216527042, 0.03736056376446411, 0.0016099042761334845, 0.023744381852778825, 0.12041702859651433], [0.9082764040218896, 1.0150759454451, 1.21661111252766, 1.0571478090583801, 1.16014572943471, 1.169276355988583]
Training Loss (progress: 0.32), 0.10878558565148327, [0.021821445687660035, 0.000679973730820678, 0.03744997112378633, 0.0016115524267808572, 0.023771049150857017, 0.1205109140324901], [0.908269569728694, 1.01503135064425, 1.216754689279155, 1.057109761590234, 1.1602119376156004, 1.1693956806932853]
Training Loss (progress: 0.40), 0.10766945674148332, [0.02180474238171223, 0.0006717073519571231, 0.03751085840022773, 0.001611942487636805, 0.023810920224661282, 0.12055779140249066], [0.9082220700156686, 1.015011877660893, 1.2168661826508291, 1.0570964460918792, 1.1603220806049925, 1.1694858944306998]
Training Loss (progress: 0.48), 0.10151214745623545, [0.02180407672954097, 0.0006738503168909534, 0.037579265092952535, 0.001611000471353178, 0.02380470108358299, 0.12059216280557505], [0.9081902456343759, 1.015016824861967, 1.2169799412191795, 1.0571380168837285, 1.1603806791002125, 1.1695692771422568]
Training Loss (progress: 0.56), 0.11128129307720233, [0.02182527868724609, 0.0006836234053066374, 0.037611241321716435, 0.0015980808747761454, 0.02385404359283264, 0.12062596648628206], [0.9081875615536924, 1.0150075517419381, 1.2170568697834625, 1.0571571541395377, 1.1604858798741482, 1.1696510876933175]
Training Loss (progress: 0.64), 0.09223352324157884, [0.02184406175992473, 0.0006831706208525671, 0.03768468303232604, 0.0016005804490958713, 0.023830007871060942, 0.1206747988450217], [0.9081668770665494, 1.0149856681733178, 1.2171503523680753, 1.0571716738355081, 1.1605034823077172, 1.1697376581682877]
Training Loss (progress: 0.72), 0.10017653044165874, [0.02181323870224731, 0.0006839686476658585, 0.03773555609513574, 0.0015972836192568994, 0.023867812845569204, 0.12068829894198439], [0.908103287990429, 1.0149819958952793, 1.2172421200303436, 1.0571475910688766, 1.1606002517550775, 1.1698097455823466]
Training Loss (progress: 0.80), 0.10440929076933836, [0.021832672780952044, 0.0006880095754278169, 0.03778083051772226, 0.001604788754563304, 0.02388707742086728, 0.12072434672130511], [0.9080773549017385, 1.014990094068876, 1.2173581770663997, 1.057150189903009, 1.1606868063474256, 1.169877331204533]
Training Loss (progress: 0.88), 0.09723550744972553, [0.02182107158304972, 0.0006741537132897066, 0.03787226011213914, 0.0016206526612601536, 0.023901684910489412, 0.12077898455012911], [0.9080227984123413, 1.0149520326525474, 1.2175011711285475, 1.0571447769870856, 1.160792748962353, 1.1700064383040614]
Training Loss (progress: 0.96), 0.10540599313165457, [0.021836605563292463, 0.0006792501347902589, 0.03786452529462593, 0.0016007640110551476, 0.023933611069935586, 0.12080464434013916], [0.9080306657836565, 1.014913510054709, 1.2175081245894872, 1.057111479229551, 1.1608814348156056, 1.1700777780183065]
Evaluation on validation dataset:
Step 25, mean loss 0.019409613465729017
Step 50, mean loss 0.01288640617023892
Step 75, mean loss 0.022073404267096405
Step 100, mean loss 0.026124639444128538
Step 125, mean loss 0.033825594742816385
Step 150, mean loss 0.03134839219366574
Step 175, mean loss 0.051510762122635896
Step 200, mean loss 0.16650513818401116
Step 225, mean loss 0.1084071404763017
Unrolled forward losses 1.5214676278115968
Unrolled forward base losses 2.565701273852575
Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00), 0.09923858793732211, [0.02182210413495303, 0.0006842045209256185, 0.03787083076348472, 0.0015980807885587443, 0.02394250135407796, 0.12083227322214561], [0.9079918789750092, 1.0149239765672688, 1.2175177690838959, 1.0570983486585466, 1.1609110030066736, 1.1701310160672063]
Training Loss (progress: 0.08), 0.10122746660269488, [0.021837010712810606, 0.0006917335408568933, 0.038008777676329736, 0.0016034307448994831, 0.023973069248424333, 0.1208559138909568], [0.9079744360595263, 1.0149124225552228, 1.217718955035485, 1.057150250982653, 1.1610201862626894, 1.1702184605473562]
Training Loss (progress: 0.16), 0.10283962472309569, [0.021869962562559346, 0.0006835648448807511, 0.03807877942486781, 0.0016016955565536998, 0.023986717794980357, 0.12089745802478843], [0.9079801590611722, 1.0148723840122593, 1.2178135313521783, 1.0571469272548935, 1.1611127051711778, 1.1702970595765412]
Training Loss (progress: 0.24), 0.09542652775224571, [0.02182853944851689, 0.0006826556448735762, 0.03817591197907584, 0.0016069590926270356, 0.02401252946264372, 0.12093483146668671], [0.9079256684153455, 1.0148690285395274, 1.217907443396437, 1.0571482827258865, 1.161214398863757, 1.1703925636920862]
Training Loss (progress: 0.32), 0.11103520081890819, [0.02182287315318892, 0.0006868775858204057, 0.03823244714917319, 0.0016083818900056092, 0.024021216314242554, 0.12095530634698098], [0.9078979622900514, 1.0148312067235332, 1.2179857048879095, 1.0571478783004238, 1.1612865431040942, 1.1704352062140262]
Training Loss (progress: 0.40), 0.10276463939012434, [0.0218315500121986, 0.0006770292606724041, 0.03829609920780985, 0.0016007553369260566, 0.024036332170629282, 0.12098571714289114], [0.9078696675864225, 1.0148075606050107, 1.2180951285089965, 1.0571456939123736, 1.1613973535891007, 1.1705285412112825]
Training Loss (progress: 0.48), 0.1108810386922194, [0.021840895675124768, 0.0006703212950048656, 0.038350714056977604, 0.0016133175343365184, 0.02404823650278422, 0.12104521953098472], [0.9078541795687822, 1.0147872970794842, 1.2181864290151803, 1.0571374058043537, 1.1614691737562195, 1.1706147738428463]
Training Loss (progress: 0.56), 0.0957788079526558, [0.021853413165824474, 0.0006814090047452365, 0.03838083062938628, 0.0015947202322474882, 0.024080613173948728, 0.12105211104332952], [0.9078509566203757, 1.0147782844684294, 1.2182112942026113, 1.057127920512973, 1.1615381640858877, 1.170654713292473]
Training Loss (progress: 0.64), 0.1073106335010949, [0.021815467971653745, 0.0006924118857658681, 0.03848178075954415, 0.0016046173048209723, 0.0240893225899043, 0.1211062942889692], [0.9077641352197998, 1.0147573370638054, 1.2183518809426763, 1.0571663033187884, 1.1616303271285122, 1.1707672338851278]
Training Loss (progress: 0.72), 0.11161449852943535, [0.021831298730002755, 0.0006805475772233519, 0.03853171334993331, 0.0016203399240633722, 0.024115172228317147, 0.12115225607136486], [0.9077600724537517, 1.0147461021758573, 1.2184361469965992, 1.057148592865679, 1.1617520225654592, 1.170838090558621]
Training Loss (progress: 0.80), 0.09228090042846157, [0.021862788762741493, 0.0006831025250934675, 0.038508643205178356, 0.0015937175438688978, 0.02414376726663046, 0.12115092192952526], [0.9077477764065951, 1.0147036455625051, 1.2184714650612005, 1.057137509438837, 1.1618266774791985, 1.1708908741964024]
Training Loss (progress: 0.88), 0.10554831425881481, [0.02186622434660163, 0.0006849273440073741, 0.03865495866819203, 0.0016003577497368546, 0.02417424462524614, 0.12120016302380743], [0.9077276126085737, 1.0146925605350854, 1.2186532566912, 1.0571294832122244, 1.1619332724598412, 1.1709828874859174]
Training Loss (progress: 0.96), 0.11066301001704756, [0.021857107216428397, 0.0006842226126731789, 0.038678003526801105, 0.001598985351384549, 0.02418769691057833, 0.12121466401205665], [0.9076746155269055, 1.0146848764015641, 1.2186817205189229, 1.0571170401381695, 1.1620314351792826, 1.171042656338956]
Evaluation on validation dataset:
Step 25, mean loss 0.018610991098980217
Step 50, mean loss 0.012606853311836603
Step 75, mean loss 0.02031593800053124
Step 100, mean loss 0.025849567161412585
Step 125, mean loss 0.03203599304962254
Step 150, mean loss 0.030809166918879506
Step 175, mean loss 0.05065975317786299
Step 200, mean loss 0.1607632189340647
Step 225, mean loss 0.10707420568989505
Unrolled forward losses 1.475356951425319
Unrolled forward base losses 2.565701273852575
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00), 0.11078193534271671, [0.021867440658438216, 0.0006889975647094009, 0.03874009667496645, 0.001605573458850429, 0.024182421564946082, 0.1212453923393075], [0.9076760293902099, 1.0146716207261495, 1.218751888534017, 1.0571278849681942, 1.162063071952426, 1.1711160060189785]
Training Loss (progress: 0.08), 0.10284786086319038, [0.021850918511370148, 0.0006886018969674614, 0.038808106520938364, 0.0016061909886639727, 0.024213580228082423, 0.12126229069436063], [0.9076356938778802, 1.0146311815924602, 1.218839503029156, 1.0571093178580044, 1.16215473348297, 1.1711688208481985]
Training Loss (progress: 0.16), 0.12068921105136969, [0.021851258209163324, 0.0006890304325781444, 0.038942107804931236, 0.0016012641714426197, 0.024215004516781786, 0.12127946652580139], [0.9076084164951413, 1.0146315764049414, 1.2190027929778986, 1.0571268356220704, 1.1622243383043496, 1.1712225417157407]
Training Loss (progress: 0.24), 0.094577765498576, [0.021841688956073434, 0.0006939346993954511, 0.03895056743061908, 0.0015987649587219258, 0.024225636316589756, 0.12130865506309603], [0.9075573605361115, 1.0146186623811655, 1.21906495141358, 1.057106303675094, 1.1622805958925349, 1.1713027525059725]
Training Loss (progress: 0.32), 0.10484245594838806, [0.021873549757092138, 0.0006814756933489461, 0.03901199576964029, 0.0016070168787083326, 0.02425839350548715, 0.12133463844981704], [0.9075566700004706, 1.014600119733262, 1.2191606764904777, 1.0571527961514167, 1.1623747960620052, 1.1713912075133026]
Training Loss (progress: 0.40), 0.10492723943612674, [0.021862705212216033, 0.0006916650620737227, 0.03904616439763739, 0.0015967030760410182, 0.024255760875701124, 0.12136858676664825], [0.907500484212221, 1.0145713733821247, 1.2192270317436373, 1.057108086874491, 1.1624307159379312, 1.1714659164629226]
Training Loss (progress: 0.48), 0.1038162169028555, [0.021879562844433764, 0.0006791341695136751, 0.03911244138135413, 0.0016026941967959565, 0.024276448464846816, 0.12140490848033304], [0.9074759997589421, 1.0145261985564473, 1.219337487174832, 1.0571225751883808, 1.1624980706771912, 1.171533475586596]
Training Loss (progress: 0.56), 0.10521508459845291, [0.021878827500375845, 0.0006895870954893177, 0.03916840098524376, 0.0016097959017776812, 0.02432793972923758, 0.1214593734534091], [0.9074624921432861, 1.014545543858181, 1.219443476982982, 1.057140083293727, 1.162612663329088, 1.1716161725114782]
Training Loss (progress: 0.64), 0.10459779921785907, [0.021884670556340628, 0.0006827663707313666, 0.03928285688012459, 0.001613542401662184, 0.02433889248899303, 0.12150627359492795], [0.9074340595950118, 1.0145015707519616, 1.219572512515534, 1.0571433216289066, 1.1627165176326775, 1.1717386493402941]
Training Loss (progress: 0.72), 0.09763531839684572, [0.021888969043850216, 0.0006917341030092985, 0.03928391765865887, 0.0016017462978161988, 0.02437362498519724, 0.1215462936940427], [0.9074030624195926, 1.0144805996124164, 1.2196304937050682, 1.0571430564681092, 1.1628150761130485, 1.1718324744970448]
Training Loss (progress: 0.80), 0.09034053193771562, [0.021880458413492444, 0.0006803945970000638, 0.03932405136537551, 0.001604234445449293, 0.024413824762489416, 0.12158531376816015], [0.9073577268146168, 1.0144668903957068, 1.2196787039352612, 1.0571360473831561, 1.1629258464729284, 1.1719140440322287]
Training Loss (progress: 0.88), 0.09946416047343583, [0.021877388235869877, 0.0006898899496545474, 0.039361225627014315, 0.0016102171431962603, 0.024426404364531827, 0.12161052902128115], [0.9073307102794128, 1.014444461384644, 1.2197311279501228, 1.0571197568238178, 1.16296428544779, 1.1719847049266237]
Training Loss (progress: 0.96), 0.09516035738797005, [0.02187606306866555, 0.000676350213702488, 0.039484087503078524, 0.0015932694456496069, 0.02444178308078879, 0.1216588869415031], [0.9073208960897732, 1.0144335941802416, 1.2198869790339393, 1.0570952881972704, 1.1630478339735646, 1.1720633116422643]
Evaluation on validation dataset:
Step 25, mean loss 0.0175656620912856
Step 50, mean loss 0.012572678640347296
Step 75, mean loss 0.02125040887016867
Step 100, mean loss 0.024135792451981185
Step 125, mean loss 0.03118243873115119
Step 150, mean loss 0.029986138475307372
Step 175, mean loss 0.04959105022352223
Step 200, mean loss 0.16068557149592455
Step 225, mean loss 0.10675318805747334
Unrolled forward losses 1.4620216494772167
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.67177e-01, 4.46854e-03, 1.11149e+00, 1.07078e-01, 1.00081e+00, 1.13720e+00
Node: 01 (pos: 0.010): 8.07529e-01, 2.34463e-02, 1.14355e+00, 2.15553e-01, 1.04784e+00, 1.14774e+00
Node: 02 (pos: 0.020): 8.42118e-01, 9.10102e-02, 1.17048e+00, 3.82085e-01, 1.08795e+00, 1.15644e+00
Node: 03 (pos: 0.030): 8.70042e-01, 2.61345e-01, 1.19185e+00, 5.96377e-01, 1.12021e+00, 1.16325e+00
Node: 04 (pos: 0.040): 8.90552e-01, 5.55194e-01, 1.20736e+00, 8.19663e-01, 1.14383e+00, 1.16814e+00
Node: 05 (pos: 0.051): 9.03090e-01, 8.72536e-01, 1.21676e+00, 9.91983e-01, 1.15825e+00, 1.17108e+00
-
Node: 07 (pos: 0.071): 9.03090e-01, 8.72536e-01, 1.21676e+00, 9.91983e-01, 1.15825e+00, 1.17108e+00
Node: 08 (pos: 0.081): 8.90552e-01, 5.55194e-01, 1.20736e+00, 8.19663e-01, 1.14383e+00, 1.16814e+00
Node: 09 (pos: 0.091): 8.70042e-01, 2.61345e-01, 1.19185e+00, 5.96377e-01, 1.12021e+00, 1.16325e+00
Node: 10 (pos: 0.101): 8.42118e-01, 9.10102e-02, 1.17048e+00, 3.82085e-01, 1.08795e+00, 1.15644e+00
Node: 11 (pos: 0.111): 8.07529e-01, 2.34463e-02, 1.14355e+00, 2.15553e-01, 1.04784e+00, 1.14774e+00
Node: 12 (pos: 0.121): 7.67177e-01, 4.46854e-03, 1.11149e+00, 1.07078e-01, 1.00081e+00, 1.13720e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.03090e-01, 8.72536e-01, 1.21676e+00, 9.91983e-01, 1.15825e+00, 1.17108e+00
Node: 58 (pos: 0.586): 8.90552e-01, 5.55194e-01, 1.20736e+00, 8.19663e-01, 1.14383e+00, 1.16814e+00
Node: 59 (pos: 0.596): 8.70042e-01, 2.61345e-01, 1.19185e+00, 5.96377e-01, 1.12021e+00, 1.16325e+00
Node: 60 (pos: 0.606): 8.42118e-01, 9.10102e-02, 1.17048e+00, 3.82085e-01, 1.08795e+00, 1.15644e+00
Node: 61 (pos: 0.616): 8.07529e-01, 2.34463e-02, 1.14355e+00, 2.15553e-01, 1.04784e+00, 1.14774e+00
Node: 50 (pos: 0.505): 7.67177e-01, 4.46854e-03, 1.11149e+00, 1.07078e-01, 1.00081e+00, 1.13720e+00
-
Node: 51 (pos: 0.515): 8.07529e-01, 2.34463e-02, 1.14355e+00, 2.15553e-01, 1.04784e+00, 1.14774e+00
Node: 52 (pos: 0.525): 8.42118e-01, 9.10102e-02, 1.17048e+00, 3.82085e-01, 1.08795e+00, 1.15644e+00
Node: 53 (pos: 0.535): 8.70042e-01, 2.61345e-01, 1.19185e+00, 5.96377e-01, 1.12021e+00, 1.16325e+00
Node: 54 (pos: 0.545): 8.90552e-01, 5.55194e-01, 1.20736e+00, 8.19663e-01, 1.14383e+00, 1.16814e+00
Node: 55 (pos: 0.556): 9.03090e-01, 8.72536e-01, 1.21676e+00, 9.91983e-01, 1.15825e+00, 1.17108e+00
Node: 62 (pos: 0.626): 7.67177e-01, 4.46854e-03, 1.11149e+00, 1.07078e-01, 1.00081e+00, 1.13720e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.015801745531901158
Step 50, mean loss 0.008549962667143187
Step 75, mean loss 0.01426741181843464
Step 100, mean loss 0.019101482885795933
Step 125, mean loss 0.030347570580742986
Step 150, mean loss 0.02904047134988076
Step 175, mean loss 0.04768468309193305
Step 200, mean loss 0.059448082949231776
Step 225, mean loss 0.07249437209866252
Unrolled forward losses 1.2275190531912497
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time5191557.tar

Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00), 0.10212024111934682, [0.02188634745402673, 0.0006853620672865464, 0.03946546087578986, 0.0016027000495834238, 0.024447054424469542, 0.12162585522842118], [0.9072973380485423, 1.0144430617311395, 1.2199124170944038, 1.057123383221608, 1.1630951856758949, 1.1720607797325433]
Training Loss (progress: 0.08), 0.10742059183593643, [0.021872753829104776, 0.0006891473704880858, 0.039521724847569654, 0.001596471135519855, 0.024459338471851996, 0.12166904532470614], [0.9072625098368354, 1.014438566325533, 1.2199984384211837, 1.0571224796150716, 1.163196951928248, 1.1721672561882608]
Training Loss (progress: 0.16), 0.10602822664411289, [0.021874528116830404, 0.0006863528887314246, 0.03955351092268028, 0.0016092973702599028, 0.024488422792572826, 0.12171054348778092], [0.9072358365565392, 1.0144338531957497, 1.2200492148867517, 1.0570665935684425, 1.1632677510770955, 1.1722509976728743]
Training Loss (progress: 0.24), 0.10135474279181773, [0.021880212503565317, 0.000681444851868554, 0.03960988849874447, 0.0016118257922598676, 0.02450200744647354, 0.1217293942616841], [0.907204154811132, 1.0144097824073928, 1.220153832133781, 1.0570873612166698, 1.163360334082744, 1.1723172599566207]
Training Loss (progress: 0.32), 0.10338745872027187, [0.021874282307724445, 0.0006687318987071044, 0.039686705040221266, 0.0016096694069532456, 0.024529634373315293, 0.12173779564715609], [0.9071628440753526, 1.0143950000903637, 1.2202525790133492, 1.0571035309672014, 1.1634857895670911, 1.1723921986693022]
Training Loss (progress: 0.40), 0.10064476250353821, [0.02189615534213392, 0.0006880467791663204, 0.039790857598211465, 0.0016051102775776315, 0.02453820677253639, 0.12179577264454483], [0.9071617443108726, 1.0143588487166684, 1.2203764277018376, 1.0571191948334082, 1.1635380103250048, 1.1725046122182772]
Training Loss (progress: 0.48), 0.09475003853511133, [0.021886459117422194, 0.0006832675612815667, 0.03986605173603169, 0.0016172220868551066, 0.024560949004300177, 0.12182151488517515], [0.9071174657728933, 1.0143246929856702, 1.2205169547082724, 1.0570934067059679, 1.1636280986210825, 1.172587278382403]
Training Loss (progress: 0.56), 0.10759620960832636, [0.021897906649060453, 0.0006891957528640759, 0.03994587191849832, 0.0016067993590314915, 0.024577693095072847, 0.12183480133175097], [0.907079941123586, 1.0143173661421159, 1.2206034550320204, 1.057061225747139, 1.1636854361319497, 1.1726316239108188]
