Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_norm+mlp+net_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time531216.tar
Number of parameters: 1035593.0
Epoch 0
Starting epoch 0...
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.92929e-02, -6.77086e-02, -1.45306e-01, -1.23059e-02, -1.34638e-01, 1.21806e-02
Node: 01 (pos: 0.010): 7.99421e-02, -6.66879e-02, -1.44456e-01, -1.28321e-02, -1.33193e-01, 1.14505e-02
Node: 02 (pos: 0.020): 8.05922e-02, -6.56681e-02, -1.43591e-01, -1.33504e-02, -1.31740e-01, 1.07179e-02
Node: 03 (pos: 0.030): 8.12432e-02, -6.46493e-02, -1.42712e-01, -1.38607e-02, -1.30276e-01, 9.98291e-03
Node: 04 (pos: 0.040): 8.18950e-02, -6.36314e-02, -1.41818e-01, -1.43631e-02, -1.28804e-01, 9.24545e-03
Node: 05 (pos: 0.051): 8.25478e-02, -6.26145e-02, -1.40911e-01, -1.48577e-02, -1.27322e-01, 8.50555e-03
-
Node: 07 (pos: 0.071): 8.25478e-02, -6.26145e-02, -1.40911e-01, -1.48577e-02, -1.27322e-01, 8.50555e-03
Node: 08 (pos: 0.081): 8.18950e-02, -6.36314e-02, -1.41818e-01, -1.43631e-02, -1.28804e-01, 9.24545e-03
Node: 09 (pos: 0.091): 8.12432e-02, -6.46493e-02, -1.42712e-01, -1.38607e-02, -1.30276e-01, 9.98291e-03
Node: 10 (pos: 0.101): 8.05922e-02, -6.56681e-02, -1.43591e-01, -1.33504e-02, -1.31740e-01, 1.07179e-02
Node: 11 (pos: 0.111): 7.99421e-02, -6.66879e-02, -1.44456e-01, -1.28321e-02, -1.33193e-01, 1.14505e-02
Node: 12 (pos: 0.121): 7.92929e-02, -6.77086e-02, -1.45306e-01, -1.23059e-02, -1.34638e-01, 1.21806e-02

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 5.33265e-01, 6.32265e-01, 1.21069e-01, 9.84970e-01, 1.63206e-01, 9.85273e-01
Node: 01 (pos: 0.010): 5.27781e-01, 6.40998e-01, 1.24091e-01, 9.83668e-01, 1.69645e-01, 9.86974e-01
Node: 02 (pos: 0.020): 5.22302e-01, 6.49709e-01, 1.27221e-01, 9.82335e-01, 1.76307e-01, 9.88578e-01
Node: 03 (pos: 0.030): 5.16828e-01, 6.58393e-01, 1.30463e-01, 9.80971e-01, 1.83197e-01, 9.90084e-01
Node: 04 (pos: 0.040): 5.11361e-01, 6.67047e-01, 1.33822e-01, 9.79581e-01, 1.90320e-01, 9.91489e-01
Node: 05 (pos: 0.051): 5.05901e-01, 6.75665e-01, 1.37300e-01, 9.78167e-01, 1.97681e-01, 9.92792e-01
-
Node: 07 (pos: 0.071): 5.05901e-01, 6.75665e-01, 1.37300e-01, 9.78167e-01, 1.97681e-01, 9.92792e-01
Node: 08 (pos: 0.081): 5.11361e-01, 6.67047e-01, 1.33822e-01, 9.79581e-01, 1.90320e-01, 9.91489e-01
Node: 09 (pos: 0.091): 5.16828e-01, 6.58393e-01, 1.30463e-01, 9.80971e-01, 1.83197e-01, 9.90084e-01
Node: 10 (pos: 0.101): 5.22302e-01, 6.49709e-01, 1.27221e-01, 9.82335e-01, 1.76307e-01, 9.88578e-01
Node: 11 (pos: 0.111): 5.27781e-01, 6.40998e-01, 1.24091e-01, 9.83668e-01, 1.69645e-01, 9.86974e-01
Node: 12 (pos: 0.121): 5.33265e-01, 6.32265e-01, 1.21069e-01, 9.84970e-01, 1.63206e-01, 9.85273e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 8.25478e-02, -6.26145e-02, -1.40911e-01, -1.48577e-02, -1.27322e-01, 8.50555e-03
Node: 01 (pos: 0.010): 8.18950e-02, -6.36314e-02, -1.41818e-01, -1.43631e-02, -1.28804e-01, 9.24545e-03
Node: 02 (pos: 0.020): 8.12432e-02, -6.46493e-02, -1.42712e-01, -1.38607e-02, -1.30276e-01, 9.98291e-03
Node: 03 (pos: 0.030): 8.05922e-02, -6.56681e-02, -1.43591e-01, -1.33504e-02, -1.31740e-01, 1.07179e-02
Node: 04 (pos: 0.040): 7.99421e-02, -6.66879e-02, -1.44456e-01, -1.28321e-02, -1.33193e-01, 1.14505e-02
Node: 05 (pos: 0.051): 7.92929e-02, -6.77086e-02, -1.45306e-01, -1.23059e-02, -1.34638e-01, 1.21806e-02
-
Node: 07 (pos: 0.071): 7.99421e-02, -6.66879e-02, -1.44456e-01, -1.28321e-02, -1.33193e-01, 1.14505e-02
Node: 08 (pos: 0.081): 8.05922e-02, -6.56681e-02, -1.43591e-01, -1.33504e-02, -1.31740e-01, 1.07179e-02
Node: 09 (pos: 0.091): 8.12432e-02, -6.46493e-02, -1.42712e-01, -1.38607e-02, -1.30276e-01, 9.98291e-03
Node: 10 (pos: 0.101): 8.18950e-02, -6.36314e-02, -1.41818e-01, -1.43631e-02, -1.28804e-01, 9.24545e-03
Node: 11 (pos: 0.111): 8.25478e-02, -6.26145e-02, -1.40911e-01, -1.48577e-02, -1.27322e-01, 8.50555e-03
Node: 12 (pos: 0.121): 7.92929e-02, -6.77086e-02, -1.45306e-01, -1.23059e-02, -1.34638e-01, 1.21806e-02

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 5.05901e-01, 6.75665e-01, 1.37300e-01, 9.78167e-01, 1.97681e-01, 9.92792e-01
Node: 58 (pos: 0.586): 5.11361e-01, 6.67047e-01, 1.33822e-01, 9.79581e-01, 1.90320e-01, 9.91489e-01
Node: 59 (pos: 0.596): 5.16828e-01, 6.58393e-01, 1.30463e-01, 9.80971e-01, 1.83197e-01, 9.90084e-01
Node: 60 (pos: 0.606): 5.22302e-01, 6.49709e-01, 1.27221e-01, 9.82335e-01, 1.76307e-01, 9.88578e-01
Node: 61 (pos: 0.616): 5.27781e-01, 6.40998e-01, 1.24091e-01, 9.83668e-01, 1.69645e-01, 9.86974e-01
Node: 50 (pos: 0.505): 5.33265e-01, 6.32265e-01, 1.21069e-01, 9.84970e-01, 1.63206e-01, 9.85273e-01
-
Node: 51 (pos: 0.515): 5.27781e-01, 6.40998e-01, 1.24091e-01, 9.83668e-01, 1.69645e-01, 9.86974e-01
Node: 52 (pos: 0.525): 5.22302e-01, 6.49709e-01, 1.27221e-01, 9.82335e-01, 1.76307e-01, 9.88578e-01
Node: 53 (pos: 0.535): 5.16828e-01, 6.58393e-01, 1.30463e-01, 9.80971e-01, 1.83197e-01, 9.90084e-01
Node: 54 (pos: 0.545): 5.11361e-01, 6.67047e-01, 1.33822e-01, 9.79581e-01, 1.90320e-01, 9.91489e-01
Node: 55 (pos: 0.556): 5.05901e-01, 6.75665e-01, 1.37300e-01, 9.78167e-01, 1.97681e-01, 9.92792e-01
Node: 62 (pos: 0.626): 5.33265e-01, 6.32265e-01, 1.21069e-01, 9.84970e-01, 1.63206e-01, 9.85273e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.2034837236841573, -4.430599222621507e-09, -6.444182665980495e-10, 1.4457435274990034e-08, -4.222936094051203e-08, 2.926305941932976e-08, 6.429300875650636e-08, [0.013001908386356283, 0.010228875750594465, 0.016280070137842242, 0.012957787862429472, 0.013104910021375971, 0.011763245818680611]
Training Loss (progress: 0.08), 0.26384931366203046, -1.1142190847644119e-08, 2.7523160012055984e-08, 2.223583672210183e-07, 8.646888797894259e-07, -6.551602181897179e-08, 9.72890364310294e-08, [0.018836049372231208, 0.011683711353644185, 0.01404670794549952, 0.025350395461989108, 0.014135555721120296, 0.028794326795810822]
Training Loss (progress: 0.16), 0.19955899201563795, 1.546613707360618e-08, 1.0508257476185909e-07, 2.0343588879806067e-07, 3.35122089993496e-07, -8.838206068082487e-08, -2.8675904997168734e-07, [0.02748266919624173, 0.009396038334755268, 0.012509210097536121, 0.039831514611736174, 0.0159778660294382, 0.041039741709206684]
Training Loss (progress: 0.24), 0.1736468966376395, -4.507146443521452e-08, 1.0643532646376431e-08, -2.7494936994036057e-07, -1.908933242863608e-07, 4.523830061972011e-09, 4.469161799564687e-09, [0.0372354360720297, 0.005856215943867006, 0.011487753035861262, 0.052405580371286786, 0.01738379985432983, 0.045899358731627565]
Training Loss (progress: 0.32), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.40), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.48), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.56), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.64), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
