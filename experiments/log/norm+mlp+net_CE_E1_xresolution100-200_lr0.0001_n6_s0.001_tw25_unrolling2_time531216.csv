Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_norm+mlp+net_CE_E1_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time531216.tar
Number of parameters: 1035593.0
Epoch 0
Starting epoch 0...
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.22053e-02, 6.49377e-02, -1.12239e-01, -5.37360e-02, -2.58326e-02, 1.29089e-02
Node: 01 (pos: 0.010): 1.25428e-02, 6.46047e-02, -1.13208e-01, -5.37283e-02, -2.72489e-02, 1.17724e-02
Node: 02 (pos: 0.020): 1.28852e-02, 6.42633e-02, -1.14183e-01, -5.37320e-02, -2.86576e-02, 1.06434e-02
Node: 03 (pos: 0.030): 1.32328e-02, 6.39135e-02, -1.15163e-01, -5.37469e-02, -3.00586e-02, 9.52181e-03
Node: 04 (pos: 0.040): 1.35853e-02, 6.35555e-02, -1.16148e-01, -5.37731e-02, -3.14520e-02, 8.40765e-03
Node: 05 (pos: 0.051): 1.39430e-02, 6.31891e-02, -1.17138e-01, -5.38106e-02, -3.28379e-02, 7.30093e-03
-
Node: 07 (pos: 0.071): 1.39430e-02, 6.31891e-02, -1.17138e-01, -5.38106e-02, -3.28379e-02, 7.30093e-03
Node: 08 (pos: 0.081): 1.35853e-02, 6.35555e-02, -1.16148e-01, -5.37731e-02, -3.14520e-02, 8.40765e-03
Node: 09 (pos: 0.091): 1.32328e-02, 6.39135e-02, -1.15163e-01, -5.37469e-02, -3.00586e-02, 9.52181e-03
Node: 10 (pos: 0.101): 1.28852e-02, 6.42633e-02, -1.14183e-01, -5.37320e-02, -2.86576e-02, 1.06434e-02
Node: 11 (pos: 0.111): 1.25428e-02, 6.46047e-02, -1.13208e-01, -5.37283e-02, -2.72489e-02, 1.17724e-02
Node: 12 (pos: 0.121): 1.22053e-02, 6.49377e-02, -1.12239e-01, -5.37360e-02, -2.58326e-02, 1.29089e-02

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 8.61595e-01, 1.47443e-02, 3.38031e-06, 5.57124e-02, 5.13080e-01, 8.46505e-01
Node: 01 (pos: 0.010): 8.54430e-01, 1.53943e-02, 2.71674e-06, 5.57580e-02, 4.75920e-01, 8.70585e-01
Node: 02 (pos: 0.020): 8.47021e-01, 1.60866e-02, 2.17668e-06, 5.57361e-02, 4.39878e-01, 8.92899e-01
Node: 03 (pos: 0.030): 8.39368e-01, 1.68241e-02, 1.73857e-06, 5.56468e-02, 4.05142e-01, 9.13324e-01
Node: 04 (pos: 0.040): 8.31469e-01, 1.76098e-02, 1.38432e-06, 5.54902e-02, 3.71864e-01, 9.31752e-01
Node: 05 (pos: 0.051): 8.23324e-01, 1.84469e-02, 1.09880e-06, 5.52668e-02, 3.40164e-01, 9.48092e-01
-
Node: 07 (pos: 0.071): 8.23324e-01, 1.84469e-02, 1.09880e-06, 5.52668e-02, 3.40164e-01, 9.48092e-01
Node: 08 (pos: 0.081): 8.31469e-01, 1.76098e-02, 1.38432e-06, 5.54902e-02, 3.71864e-01, 9.31752e-01
Node: 09 (pos: 0.091): 8.39368e-01, 1.68241e-02, 1.73857e-06, 5.56468e-02, 4.05142e-01, 9.13324e-01
Node: 10 (pos: 0.101): 8.47021e-01, 1.60866e-02, 2.17668e-06, 5.57361e-02, 4.39878e-01, 8.92899e-01
Node: 11 (pos: 0.111): 8.54430e-01, 1.53943e-02, 2.71674e-06, 5.57580e-02, 4.75920e-01, 8.70585e-01
Node: 12 (pos: 0.121): 8.61595e-01, 1.47443e-02, 3.38031e-06, 5.57124e-02, 5.13080e-01, 8.46505e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 1.39430e-02, 6.31891e-02, -1.17138e-01, -5.38106e-02, -3.28379e-02, 7.30093e-03
Node: 01 (pos: 0.010): 1.35853e-02, 6.35555e-02, -1.16148e-01, -5.37731e-02, -3.14520e-02, 8.40765e-03
Node: 02 (pos: 0.020): 1.32328e-02, 6.39135e-02, -1.15163e-01, -5.37469e-02, -3.00586e-02, 9.52181e-03
Node: 03 (pos: 0.030): 1.28852e-02, 6.42633e-02, -1.14183e-01, -5.37320e-02, -2.86576e-02, 1.06434e-02
Node: 04 (pos: 0.040): 1.25428e-02, 6.46047e-02, -1.13208e-01, -5.37283e-02, -2.72489e-02, 1.17724e-02
Node: 05 (pos: 0.051): 1.22053e-02, 6.49377e-02, -1.12239e-01, -5.37360e-02, -2.58326e-02, 1.29089e-02
-
Node: 07 (pos: 0.071): 1.25428e-02, 6.46047e-02, -1.13208e-01, -5.37283e-02, -2.72489e-02, 1.17724e-02
Node: 08 (pos: 0.081): 1.28852e-02, 6.42633e-02, -1.14183e-01, -5.37320e-02, -2.86576e-02, 1.06434e-02
Node: 09 (pos: 0.091): 1.32328e-02, 6.39135e-02, -1.15163e-01, -5.37469e-02, -3.00586e-02, 9.52181e-03
Node: 10 (pos: 0.101): 1.35853e-02, 6.35555e-02, -1.16148e-01, -5.37731e-02, -3.14520e-02, 8.40765e-03
Node: 11 (pos: 0.111): 1.39430e-02, 6.31891e-02, -1.17138e-01, -5.38106e-02, -3.28379e-02, 7.30093e-03
Node: 12 (pos: 0.121): 1.22053e-02, 6.49377e-02, -1.12239e-01, -5.37360e-02, -2.58326e-02, 1.29089e-02

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 8.23324e-01, 1.84469e-02, 1.09880e-06, 5.52668e-02, 3.40164e-01, 9.48092e-01
Node: 58 (pos: 0.586): 8.31469e-01, 1.76098e-02, 1.38432e-06, 5.54902e-02, 3.71864e-01, 9.31752e-01
Node: 59 (pos: 0.596): 8.39368e-01, 1.68241e-02, 1.73857e-06, 5.56468e-02, 4.05142e-01, 9.13324e-01
Node: 60 (pos: 0.606): 8.47021e-01, 1.60866e-02, 2.17668e-06, 5.57361e-02, 4.39878e-01, 8.92899e-01
Node: 61 (pos: 0.616): 8.54430e-01, 1.53943e-02, 2.71674e-06, 5.57580e-02, 4.75920e-01, 8.70585e-01
Node: 50 (pos: 0.505): 8.61595e-01, 1.47443e-02, 3.38031e-06, 5.57124e-02, 5.13080e-01, 8.46505e-01
-
Node: 51 (pos: 0.515): 8.54430e-01, 1.53943e-02, 2.71674e-06, 5.57580e-02, 4.75920e-01, 8.70585e-01
Node: 52 (pos: 0.525): 8.47021e-01, 1.60866e-02, 2.17668e-06, 5.57361e-02, 4.39878e-01, 8.92899e-01
Node: 53 (pos: 0.535): 8.39368e-01, 1.68241e-02, 1.73857e-06, 5.56468e-02, 4.05142e-01, 9.13324e-01
Node: 54 (pos: 0.545): 8.31469e-01, 1.76098e-02, 1.38432e-06, 5.54902e-02, 3.71864e-01, 9.31752e-01
Node: 55 (pos: 0.556): 8.23324e-01, 1.84469e-02, 1.09880e-06, 5.52668e-02, 3.40164e-01, 9.48092e-01
Node: 62 (pos: 0.626): 8.61595e-01, 1.47443e-02, 3.38031e-06, 5.57124e-02, 5.13080e-01, 8.46505e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.3592188763750397, 4.177757236926899e-09, 3.363026148796411e-09, -3.529097066020706e-08, 8.454267808966607e-09, 5.002927356546247e-09, 2.157351972636202e-08, [0.0036104432770725664, 0.0038923845580288614, 0.0005802188251688819, 0.001934279527966951, 0.0009443224012539575, 0.0011759261987750276]
Training Loss (progress: 0.08), 0.2815696191880915, 3.3328523614567465e-08, 3.738689083365589e-08, -3.7511631300458394e-07, -1.7067398343176551e-06, 3.747057771907223e-07, 2.781878227969535e-07, [0.007879271878983183, 0.015242179484111232, 0.0005887542745067093, 0.005305888564823933, 0.008175544892353724, 0.0058313458189895775]
Training Loss (progress: 0.16), 0.21154384955878092, 4.8902474340038355e-08, -6.547497635690811e-08, 1.0426241278151266e-07, 1.1821576491618533e-06, 2.902463169681306e-08, -2.9505275951198874e-07, [0.01044068559962431, 0.019416861064523897, 0.000623717175184218, 0.006317049262538644, 0.012564285358233267, 0.010318120533444643]
Training Loss (progress: 0.24), 0.18732115907732186, 9.815949824573991e-08, -1.5964870422728614e-07, 6.143809388224863e-07, 5.156608994005077e-06, -4.939477783462067e-07, -4.463370009557987e-07, [0.015413353124158388, 0.02293890003002019, 0.0005937315333917633, 0.007110322616149978, 0.020519321155264173, 0.01164001009043148]
Training Loss (progress: 0.32), 0.15574841949142593, -8.062037326517955e-09, -9.392026969441317e-09, -1.2193275602183542e-07, -1.695327414933051e-06, 3.5609033802232823e-07, 2.1391734649494216e-07, [0.019366788845674887, 0.027895169346932595, 0.0005584197085888696, 0.008345442368905812, 0.028270409172361368, 0.01683647885142666]
Training Loss (progress: 0.40), 0.1529155949962771, 1.8180495825623584e-08, 1.5690203997875376e-08, 1.1603183394820015e-07, -5.6716616612222335e-06, -2.006528961287975e-08, -1.5996122156382528e-07, [0.027606215007777806, 0.03171414392682469, 0.000531146054367577, 0.009113520625557578, 0.03883622551528482, 0.0236482927735418]
Training Loss (progress: 0.48), 0.14189674119527945, 9.32963006936252e-09, 1.9490655855283387e-08, -3.801906195743535e-07, -4.8212185882416074e-06, 4.244109740318296e-07, -8.806240336339973e-08, [0.04050429524185742, 0.03792553394362561, 0.0005244669847403761, 0.010207706704666008, 0.04462962797894495, 0.029849890897360707]
Training Loss (progress: 0.56), 0.12825526968460133, -6.893799926679567e-08, 6.83542073469179e-08, -3.809873646997525e-07, -7.803372537660557e-06, 4.843964939689641e-07, 1.3382881418230982e-06, [0.04679160944077617, 0.04880846479378641, 0.0004553044099616507, 0.012579717484857344, 0.05462812539421283, 0.03402676200652319]
Training Loss (progress: 0.64), 0.13319738538909726, -3.1945029176332445e-08, -2.7597723408816198e-08, 1.9260664184339215e-07, 8.726168098946249e-06, -2.282761380211057e-07, -9.672872259968063e-07, [0.05407324514798175, 0.058989213515339, 0.0005147301859439005, 0.01747464297373062, 0.06059060131860392, 0.04093341432591525]
Training Loss (progress: 0.72), 0.11935212579082576, -1.7291332748125478e-08, 3.0801504530392193e-08, 2.0334706027632444e-07, 1.0084492631868028e-05, -3.124510100825379e-07, -7.587266594765565e-07, [0.06771984957767237, 0.06596560756076338, 0.0004910591004378239, 0.02233218402382183, 0.07205772903744487, 0.05256829250952623]
Training Loss (progress: 0.80), 0.11286525146051682, 9.286883996998055e-08, -1.8022451558809802e-07, 4.6228363119961244e-07, 8.307043040193295e-06, -2.65571394647293e-07, -3.0971441043906344e-07, [0.07476923940997138, 0.07422726593590483, 0.0004992874499867866, 0.026813790198434273, 0.08140117682706545, 0.0635181728628395]
Training Loss (progress: 0.88), 0.10423675379230815, -4.4206351280096956e-08, 7.237027400626963e-10, -2.998910720875775e-08, -3.7316936237302623e-06, 1.3062927683487103e-07, 1.3729043620267721e-06, [0.08391339373731345, 0.08339654511257862, 0.0005098020093770632, 0.033046071832967545, 0.08736545610686744, 0.0703576058041374]
Training Loss (progress: 0.96), 0.10236536157660885, -3.488988067766813e-08, 1.244283997701912e-07, -4.534593984570608e-07, -1.1200092546396014e-05, 3.559262314016673e-07, 3.236845727772878e-07, [0.10094979083617042, 0.09430720455139151, 0.00048782291224319543, 0.03891323903159418, 0.09372403648753534, 0.079028501795427]
Evaluation on validation dataset:
