Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar
Number of parameters: 1031651.0
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
Node: 01 (pos: 0.010): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 02 (pos: 0.020): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 03 (pos: 0.030): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 04 (pos: 0.040): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 05 (pos: 0.051): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
-
Node: 07 (pos: 0.071): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 08 (pos: 0.081): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 09 (pos: 0.091): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 10 (pos: 0.101): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 11 (pos: 0.111): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 12 (pos: 0.121): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 58 (pos: 0.586): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 59 (pos: 0.596): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 60 (pos: 0.606): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 61 (pos: 0.616): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 50 (pos: 0.505): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
-
Node: 51 (pos: 0.515): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 52 (pos: 0.525): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 53 (pos: 0.535): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 54 (pos: 0.545): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 55 (pos: 0.556): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 62 (pos: 0.626): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.3991162148042937, [0.012529366418965298, 0.020858730520717676, 0.020990830491440883, 0.02044757408565243, 0.022398360573006346, 0.02208333999330901]
Training Loss (progress: 0.08), 0.24545892772932196, [0.009962547364670676, 0.005254786645808124, 0.003085725683485976, 0.0008877570058216701, 0.01336088167898159, 0.01128110037274024]
Training Loss (progress: 0.16), 0.20200585058032622, [0.006291740401388045, 0.007516893236941215, 0.0044001900352216, 0.0008603187792237673, 0.015473745833775492, 0.015548566533909177]
Training Loss (progress: 0.24), 0.15922920726717382, [0.004396146234940888, 0.009147973093395621, 0.004955230397322956, 0.0007016755949322342, 0.015530632375254922, 0.018829033386426167]
Training Loss (progress: 0.32), 0.14662262512142007, [0.0035325704299956425, 0.010500353677972247, 0.0052908415467428135, 0.0007458099686235731, 0.015403825861848235, 0.02200008110150092]
Training Loss (progress: 0.40), 0.1349400215443998, [0.002840192073755583, 0.01115989089123035, 0.005558929601396266, 0.0007597823600870477, 0.014994826526931904, 0.024307150759920463]
Training Loss (progress: 0.48), 0.12827763911423987, [0.0022996433561537767, 0.011379068184724797, 0.005143812445413667, 0.0007594814535998844, 0.014222193016814294, 0.026613377355796775]
Training Loss (progress: 0.56), 0.12645901993206388, [0.002351801999112214, 0.01135625353632753, 0.005141069739917614, 0.000792609934568303, 0.014001137631296241, 0.028988525613465158]
Training Loss (progress: 0.64), 0.11479631992170788, [0.0022071851045958827, 0.012679449785470568, 0.005559035212403969, 0.0008388934032720259, 0.013791003719965894, 0.030969622491274464]
Training Loss (progress: 0.72), 0.10733254803061447, [0.002018932082470618, 0.012646621405044465, 0.005331753226179827, 0.0007802657657033612, 0.013372066135375945, 0.032718680569401755]
Training Loss (progress: 0.80), 0.10337407838304273, [0.002008719056978653, 0.013571738981590644, 0.005731268910903134, 0.0007761819205852237, 0.013561380359374653, 0.03427010394507714]
Training Loss (progress: 0.88), 0.10292615319427433, [0.0017668530839178116, 0.014000010066318687, 0.005347920153804113, 0.0007989635616499586, 0.013022019950119111, 0.03513612599632798]
Training Loss (progress: 0.96), 0.10075128090212372, [0.0017527152051132675, 0.014175549656665022, 0.005111057007055053, 0.0007890563768442242, 0.013482861646491413, 0.036617779147308994]
Evaluation on validation dataset:
Step 25, mean loss 0.09058057288273773
Step 50, mean loss 0.08532594972968999
Step 75, mean loss 0.11458860278139854
Step 100, mean loss 0.25852309138903046
Step 125, mean loss 0.18777928261630666
Step 150, mean loss 0.15708045825977596
Step 175, mean loss 0.2683860150670936
Step 200, mean loss 0.42299047502544773
Step 225, mean loss 0.45862386488292145
Unrolled forward losses 16.003252988258033
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 8.76046e-02, 7.79260e-01, 5.15247e-01, 1.43344e-02, 7.61192e-01, 9.04703e-01
Node: 01 (pos: 0.010): 1.84350e-01, 8.40968e-01, 6.30973e-01, 5.24454e-02, 8.27379e-01, 9.32816e-01
Node: 02 (pos: 0.020): 3.38854e-01, 8.95074e-01, 7.44744e-01, 1.51570e-01, 8.85790e-01, 9.56466e-01
Node: 03 (pos: 0.030): 5.44041e-01, 9.39551e-01, 8.47235e-01, 3.46015e-01, 9.34057e-01, 9.75274e-01
Node: 04 (pos: 0.040): 7.62962e-01, 9.72668e-01, 9.28970e-01, 6.23955e-01, 9.70136e-01, 9.88934e-01
Node: 05 (pos: 0.051): 9.34600e-01, 9.93096e-01, 9.81749e-01, 8.88768e-01, 9.92449e-01, 9.97222e-01
-
Node: 07 (pos: 0.071): 9.34600e-01, 9.93096e-01, 9.81749e-01, 8.88768e-01, 9.92449e-01, 9.97222e-01
Node: 08 (pos: 0.081): 7.62962e-01, 9.72668e-01, 9.28970e-01, 6.23955e-01, 9.70136e-01, 9.88934e-01
Node: 09 (pos: 0.091): 5.44041e-01, 9.39551e-01, 8.47235e-01, 3.46015e-01, 9.34057e-01, 9.75274e-01
Node: 10 (pos: 0.101): 3.38854e-01, 8.95074e-01, 7.44744e-01, 1.51570e-01, 8.85790e-01, 9.56466e-01
Node: 11 (pos: 0.111): 1.84350e-01, 8.40968e-01, 6.30973e-01, 5.24454e-02, 8.27379e-01, 9.32816e-01
Node: 12 (pos: 0.121): 8.76046e-02, 7.79260e-01, 5.15247e-01, 1.43344e-02, 7.61192e-01, 9.04703e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.34600e-01, 9.93096e-01, 9.81749e-01, 8.88768e-01, 9.92449e-01, 9.97222e-01
Node: 58 (pos: 0.586): 7.62962e-01, 9.72668e-01, 9.28970e-01, 6.23955e-01, 9.70136e-01, 9.88934e-01
Node: 59 (pos: 0.596): 5.44041e-01, 9.39551e-01, 8.47235e-01, 3.46015e-01, 9.34057e-01, 9.75274e-01
Node: 60 (pos: 0.606): 3.38854e-01, 8.95074e-01, 7.44744e-01, 1.51570e-01, 8.85790e-01, 9.56466e-01
Node: 61 (pos: 0.616): 1.84350e-01, 8.40968e-01, 6.30973e-01, 5.24454e-02, 8.27379e-01, 9.32816e-01
Node: 50 (pos: 0.505): 8.76046e-02, 7.79260e-01, 5.15247e-01, 1.43344e-02, 7.61192e-01, 9.04703e-01
-
Node: 51 (pos: 0.515): 1.84350e-01, 8.40968e-01, 6.30973e-01, 5.24454e-02, 8.27379e-01, 9.32816e-01
Node: 52 (pos: 0.525): 3.38854e-01, 8.95074e-01, 7.44744e-01, 1.51570e-01, 8.85790e-01, 9.56466e-01
Node: 53 (pos: 0.535): 5.44041e-01, 9.39551e-01, 8.47235e-01, 3.46015e-01, 9.34057e-01, 9.75274e-01
Node: 54 (pos: 0.545): 7.62962e-01, 9.72668e-01, 9.28970e-01, 6.23955e-01, 9.70136e-01, 9.88934e-01
Node: 55 (pos: 0.556): 9.34600e-01, 9.93096e-01, 9.81749e-01, 8.88768e-01, 9.92449e-01, 9.97222e-01
Node: 62 (pos: 0.626): 8.76046e-02, 7.79260e-01, 5.15247e-01, 1.43344e-02, 7.61192e-01, 9.04703e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.0789065334400712
Step 50, mean loss 0.07068456955338415
Step 75, mean loss 0.10124221416012952
Step 100, mean loss 0.12776713414933488
Step 125, mean loss 0.32394359386507166
Step 150, mean loss 0.16964683550601423
Step 175, mean loss 0.33124068763253045
Step 200, mean loss 0.39503760174299346
Step 225, mean loss 0.37932219303273407
Unrolled forward losses 14.277634956783846
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.21835847911145334, [0.0017264499342080325, 0.014501323521503698, 0.005901861704576591, 0.0007567335021996305, 0.013463123024883821, 0.036822748898746524]
Training Loss (progress: 0.08), 0.19294153328508487, [0.0019132838175045064, 0.01809792869770781, 0.007070127650348316, 0.0006995192137823403, 0.015050804222329208, 0.04163659499223107]
Training Loss (progress: 0.16), 0.19327782645987915, [0.0018507778684663007, 0.021166593411280212, 0.00763435606156814, 0.0007897294638854218, 0.016393073432024653, 0.04536830629912939]
Training Loss (progress: 0.24), 0.1965907967141384, [0.001937241718672569, 0.023628131697200936, 0.008191113920808733, 0.0008504208266915301, 0.017325746249005024, 0.04836066428701329]
Training Loss (progress: 0.32), 0.20777271681513906, [0.0019412192981134741, 0.027699084264923186, 0.00857086098309431, 0.000790096987175603, 0.018040837370182892, 0.049944697257776886]
Training Loss (progress: 0.40), 0.1759429354539631, [0.0019988674816501157, 0.029712266643876776, 0.009126422852619945, 0.0008322126177226133, 0.018188603302819434, 0.05249249859275909]
Training Loss (progress: 0.48), 0.18862836161073263, [0.0020916231162916832, 0.03367998094131932, 0.009418953381556216, 0.0008327558350455035, 0.019052156159061452, 0.05479149027893711]
Training Loss (progress: 0.56), 0.17659984730733547, [0.0018862079831063238, 0.03661131317778021, 0.010407218144837544, 0.0008690694408439498, 0.019825025630859448, 0.057478502346226985]
Training Loss (progress: 0.64), 0.18461101039510674, [0.0018498715718044218, 0.03976599910353225, 0.011669657729688176, 0.0008083435956043567, 0.02011162449994723, 0.059106412128293]
Training Loss (progress: 0.72), 0.16720233717256525, [0.001865243325419657, 0.04110282941310962, 0.011529390334311298, 0.0008049969357415294, 0.02052697067685978, 0.06151528980974161]
Training Loss (progress: 0.80), 0.18141552162022528, [0.0018304560460874265, 0.04391073561155637, 0.012445520386428915, 0.0007952856328538727, 0.020568465907822898, 0.06269348303155915]
Training Loss (progress: 0.88), 0.16176412640672716, [0.0018326519552552354, 0.046510504488031415, 0.012537348250271651, 0.0008709829604268831, 0.020215479979752683, 0.06421577918627416]
Training Loss (progress: 0.96), 0.14888020022913884, [0.0019622110694033834, 0.047532654892431046, 0.013126658846770735, 0.0007613251171317496, 0.020331549456818344, 0.06534867969033249]
Evaluation on validation dataset:
Step 25, mean loss 0.08904228615460844
Step 50, mean loss 0.07617363894064139
Step 75, mean loss 0.10197547882441946
Step 100, mean loss 0.18039248521247675
Step 125, mean loss 0.16574497002695265
Step 150, mean loss 0.17425354988514427
Step 175, mean loss 0.3051618139117479
Step 200, mean loss 0.5011443075462879
Step 225, mean loss 0.43659962702970745
Unrolled forward losses 5.4922819749791865
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.64398e-01, 9.27014e-01, 7.49656e-01, 8.26538e-03, 8.34303e-01, 9.45350e-01
Node: 01 (pos: 0.010): 2.85419e-01, 9.48732e-01, 8.18651e-01, 3.57812e-02, 8.81787e-01, 9.61724e-01
Node: 02 (pos: 0.020): 4.48239e-01, 9.66878e-01, 8.79798e-01, 1.18669e-01, 9.22641e-01, 9.75332e-01
Node: 03 (pos: 0.030): 6.36758e-01, 9.81232e-01, 9.30498e-01, 3.01520e-01, 9.55721e-01, 9.86048e-01
Node: 04 (pos: 0.040): 8.18234e-01, 9.91615e-01, 9.68491e-01, 5.86928e-01, 9.80072e-01, 9.93775e-01
Node: 05 (pos: 0.051): 9.51085e-01, 9.97897e-01, 9.92028e-01, 8.75278e-01, 9.94980e-01, 9.98440e-01
-
Node: 07 (pos: 0.071): 9.51085e-01, 9.97897e-01, 9.92028e-01, 8.75278e-01, 9.94980e-01, 9.98440e-01
Node: 08 (pos: 0.081): 8.18234e-01, 9.91615e-01, 9.68491e-01, 5.86928e-01, 9.80072e-01, 9.93775e-01
Node: 09 (pos: 0.091): 6.36758e-01, 9.81232e-01, 9.30498e-01, 3.01520e-01, 9.55721e-01, 9.86048e-01
Node: 10 (pos: 0.101): 4.48239e-01, 9.66878e-01, 8.79798e-01, 1.18669e-01, 9.22641e-01, 9.75332e-01
Node: 11 (pos: 0.111): 2.85419e-01, 9.48732e-01, 8.18651e-01, 3.57812e-02, 8.81787e-01, 9.61724e-01
Node: 12 (pos: 0.121): 1.64398e-01, 9.27014e-01, 7.49656e-01, 8.26538e-03, 8.34303e-01, 9.45350e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.51085e-01, 9.97897e-01, 9.92028e-01, 8.75278e-01, 9.94980e-01, 9.98440e-01
Node: 58 (pos: 0.586): 8.18234e-01, 9.91615e-01, 9.68491e-01, 5.86928e-01, 9.80072e-01, 9.93775e-01
Node: 59 (pos: 0.596): 6.36758e-01, 9.81232e-01, 9.30498e-01, 3.01520e-01, 9.55721e-01, 9.86048e-01
Node: 60 (pos: 0.606): 4.48239e-01, 9.66878e-01, 8.79798e-01, 1.18669e-01, 9.22641e-01, 9.75332e-01
Node: 61 (pos: 0.616): 2.85419e-01, 9.48732e-01, 8.18651e-01, 3.57812e-02, 8.81787e-01, 9.61724e-01
Node: 50 (pos: 0.505): 1.64398e-01, 9.27014e-01, 7.49656e-01, 8.26538e-03, 8.34303e-01, 9.45350e-01
-
Node: 51 (pos: 0.515): 2.85419e-01, 9.48732e-01, 8.18651e-01, 3.57812e-02, 8.81787e-01, 9.61724e-01
Node: 52 (pos: 0.525): 4.48239e-01, 9.66878e-01, 8.79798e-01, 1.18669e-01, 9.22641e-01, 9.75332e-01
Node: 53 (pos: 0.535): 6.36758e-01, 9.81232e-01, 9.30498e-01, 3.01520e-01, 9.55721e-01, 9.86048e-01
Node: 54 (pos: 0.545): 8.18234e-01, 9.91615e-01, 9.68491e-01, 5.86928e-01, 9.80072e-01, 9.93775e-01
Node: 55 (pos: 0.556): 9.51085e-01, 9.97897e-01, 9.92028e-01, 8.75278e-01, 9.94980e-01, 9.98440e-01
Node: 62 (pos: 0.626): 1.64398e-01, 9.27014e-01, 7.49656e-01, 8.26538e-03, 8.34303e-01, 9.45350e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.0703663927344107
Step 50, mean loss 0.05877914431067259
Step 75, mean loss 0.08411019986305118
Step 100, mean loss 0.11520199165363036
Step 125, mean loss 0.26064618405264967
Step 150, mean loss 0.17641116731679526
Step 175, mean loss 0.2739856718570827
Step 200, mean loss 0.22379299838416003
Step 225, mean loss 0.3088762337100083
Unrolled forward losses 4.0461883368334295
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.2508972505774386, [0.001936346552307577, 0.048498155031432254, 0.012954286755152325, 0.0008011834680212894, 0.020283561731801132, 0.06563053039442551]
Training Loss (progress: 0.08), 0.23297400292783613, [0.0020100997075837273, 0.04970532151124681, 0.013878517929777507, 0.0008011808673780225, 0.021091488015339225, 0.06755195939808264]
Training Loss (progress: 0.16), 0.22316543223646187, [0.0018976950354639202, 0.05048505610027637, 0.0142798361005025, 0.0008368135449414746, 0.021385473637015588, 0.06867636158469236]
Training Loss (progress: 0.24), 0.19028688332742477, [0.0018704222263210557, 0.051578247602822556, 0.014977109704761267, 0.0008108219100697037, 0.02149586062476852, 0.06987812819597015]
Training Loss (progress: 0.32), 0.22710836873359694, [0.001889673585360161, 0.05223524814822546, 0.015483616109107073, 0.0008354762653140775, 0.02156949067315827, 0.0707080003513327]
Training Loss (progress: 0.40), 0.21003187740057588, [0.0018838013300198945, 0.053351599720645015, 0.016091447849342636, 0.0007686627234001214, 0.022138239356951084, 0.07131949462409405]
Training Loss (progress: 0.48), 0.19446864569438244, [0.0018964977124748498, 0.0550833936670425, 0.016629675408005168, 0.0007517085606941765, 0.022423791857019273, 0.07238198433554857]
Training Loss (progress: 0.56), 0.1944489122496125, [0.0018734247548234917, 0.05523606887895668, 0.016411591869560364, 0.0007940801751033659, 0.022867406200364992, 0.07301789461477594]
Training Loss (progress: 0.64), 0.2110917645434911, [0.0018789029650264212, 0.05641191189960434, 0.016862161810768547, 0.0008326880142142004, 0.02293551819495893, 0.07382750379705959]
Training Loss (progress: 0.72), 0.20915505108212756, [0.0018055831620261795, 0.05738350160944004, 0.017459237989282783, 0.0007995124069000866, 0.023310740295107813, 0.07476076910184462]
Training Loss (progress: 0.80), 0.18358802056250365, [0.0020293760444256553, 0.058323989308394816, 0.01832945769884894, 0.0008207893037754123, 0.02385019515230208, 0.07530156046911321]
Training Loss (progress: 0.88), 0.20359044841721652, [0.0019372084925829793, 0.05888061158717631, 0.019030064025571596, 0.0008071257678279899, 0.024026989287313616, 0.07563697569351459]
Training Loss (progress: 0.96), 0.18397895281016147, [0.0019665882784271286, 0.05971212244057352, 0.01894352419584535, 0.0008049856295303631, 0.024447535757025945, 0.0763911829621762]
Evaluation on validation dataset:
Step 25, mean loss 0.07471291604131028
Step 50, mean loss 0.04078034887334604
Step 75, mean loss 0.0504937755473416
Step 100, mean loss 0.06987195924543313
Step 125, mean loss 0.08844868103469902
Step 150, mean loss 0.08356938640006405
Step 175, mean loss 0.14671712266915624
Step 200, mean loss 0.3258973127202376
Step 225, mean loss 0.2617712941171757
Unrolled forward losses 3.356613537166224
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.54627e-01, 9.40897e-01, 8.26400e-01, 9.45429e-03, 8.61670e-01, 9.53320e-01
Node: 01 (pos: 0.010): 2.73529e-01, 9.58576e-01, 8.75978e-01, 3.92814e-02, 9.01775e-01, 9.67347e-01
Node: 02 (pos: 0.020): 4.36197e-01, 9.73287e-01, 9.18746e-01, 1.25974e-01, 9.35972e-01, 9.78978e-01
Node: 03 (pos: 0.030): 6.27078e-01, 9.84885e-01, 9.53449e-01, 3.11822e-01, 9.63464e-01, 9.88120e-01
Node: 04 (pos: 0.040): 8.12682e-01, 9.93254e-01, 9.79037e-01, 5.95758e-01, 9.83594e-01, 9.94702e-01
Node: 05 (pos: 0.051): 9.49468e-01, 9.98309e-01, 9.94717e-01, 8.78552e-01, 9.95873e-01, 9.98673e-01
-
Node: 07 (pos: 0.071): 9.49468e-01, 9.98309e-01, 9.94717e-01, 8.78552e-01, 9.95873e-01, 9.98673e-01
Node: 08 (pos: 0.081): 8.12682e-01, 9.93254e-01, 9.79037e-01, 5.95758e-01, 9.83594e-01, 9.94702e-01
Node: 09 (pos: 0.091): 6.27078e-01, 9.84885e-01, 9.53449e-01, 3.11822e-01, 9.63464e-01, 9.88120e-01
Node: 10 (pos: 0.101): 4.36197e-01, 9.73287e-01, 9.18746e-01, 1.25974e-01, 9.35972e-01, 9.78978e-01
Node: 11 (pos: 0.111): 2.73529e-01, 9.58576e-01, 8.75978e-01, 3.92814e-02, 9.01775e-01, 9.67347e-01
Node: 12 (pos: 0.121): 1.54627e-01, 9.40897e-01, 8.26400e-01, 9.45429e-03, 8.61670e-01, 9.53320e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.49468e-01, 9.98309e-01, 9.94717e-01, 8.78552e-01, 9.95873e-01, 9.98673e-01
Node: 58 (pos: 0.586): 8.12682e-01, 9.93254e-01, 9.79037e-01, 5.95758e-01, 9.83594e-01, 9.94702e-01
Node: 59 (pos: 0.596): 6.27078e-01, 9.84885e-01, 9.53449e-01, 3.11822e-01, 9.63464e-01, 9.88120e-01
Node: 60 (pos: 0.606): 4.36197e-01, 9.73287e-01, 9.18746e-01, 1.25974e-01, 9.35972e-01, 9.78978e-01
Node: 61 (pos: 0.616): 2.73529e-01, 9.58576e-01, 8.75978e-01, 3.92814e-02, 9.01775e-01, 9.67347e-01
Node: 50 (pos: 0.505): 1.54627e-01, 9.40897e-01, 8.26400e-01, 9.45429e-03, 8.61670e-01, 9.53320e-01
-
Node: 51 (pos: 0.515): 2.73529e-01, 9.58576e-01, 8.75978e-01, 3.92814e-02, 9.01775e-01, 9.67347e-01
Node: 52 (pos: 0.525): 4.36197e-01, 9.73287e-01, 9.18746e-01, 1.25974e-01, 9.35972e-01, 9.78978e-01
Node: 53 (pos: 0.535): 6.27078e-01, 9.84885e-01, 9.53449e-01, 3.11822e-01, 9.63464e-01, 9.88120e-01
Node: 54 (pos: 0.545): 8.12682e-01, 9.93254e-01, 9.79037e-01, 5.95758e-01, 9.83594e-01, 9.94702e-01
Node: 55 (pos: 0.556): 9.49468e-01, 9.98309e-01, 9.94717e-01, 8.78552e-01, 9.95873e-01, 9.98673e-01
Node: 62 (pos: 0.626): 1.54627e-01, 9.40897e-01, 8.26400e-01, 9.45429e-03, 8.61670e-01, 9.53320e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.06207291369306876
Step 50, mean loss 0.03157068531641731
Step 75, mean loss 0.041854390358212853
Step 100, mean loss 0.054520328539642324
Step 125, mean loss 0.13105731524745684
Step 150, mean loss 0.08052505254946679
Step 175, mean loss 0.14513547821936815
Step 200, mean loss 0.13512912269661187
Step 225, mean loss 0.16019720198294024
Unrolled forward losses 2.4562049074250822
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.1864549532948687, [0.001941127993781023, 0.06055759506624388, 0.01934268341518305, 0.0008000791159189935, 0.02467226275361708, 0.07692881199851219]
Training Loss (progress: 0.08), 0.19121473153290686, [0.0019001090020093335, 0.061206416237057075, 0.019474037820584923, 0.0008114987459814338, 0.02508917716490009, 0.07721961344054995]
Training Loss (progress: 0.16), 0.19837022140992366, [0.0018711616652768261, 0.062319538567900755, 0.019449527012566005, 0.0008451527648854545, 0.025190395196844108, 0.07748918371250767]
Training Loss (progress: 0.24), 0.19847820185473652, [0.0018917716216048338, 0.06304456266466638, 0.020267829382271448, 0.000828680706956575, 0.025700243381015186, 0.07824553141962025]
Training Loss (progress: 0.32), 0.17328094767860183, [0.0019081706450642698, 0.06391140559960297, 0.019974897528471782, 0.0008079342016805223, 0.02596489156247087, 0.07847906754436626]
Training Loss (progress: 0.40), 0.17243818477147732, [0.001872174065288284, 0.06488915953230127, 0.02047313780607689, 0.0008240003012103714, 0.026389576635670344, 0.07851898883311118]
Training Loss (progress: 0.48), 0.18527475418880496, [0.0019529762807168382, 0.06591680754463038, 0.020565369193998685, 0.0007939873533286175, 0.026558596458830696, 0.07884373454331291]
Training Loss (progress: 0.56), 0.18946205440773342, [0.0017430026371969937, 0.06663848073500135, 0.020585034463442717, 0.0008341010742729321, 0.026740149597132612, 0.0793601160375594]
Training Loss (progress: 0.64), 0.18326351170078703, [0.0018492123355931963, 0.06756363177692606, 0.021599288616507492, 0.0008465141628551122, 0.026899127725971375, 0.07988547911178734]
Training Loss (progress: 0.72), 0.18946563931791977, [0.001835338713950306, 0.06848852521765403, 0.021874829570501093, 0.0008372290971809323, 0.027298652223681278, 0.07982598171366412]
Training Loss (progress: 0.80), 0.1858499806996208, [0.0019207846635794362, 0.06974814759593743, 0.021733517570832733, 0.0008414632085533742, 0.027517681696357325, 0.0802914428680162]
Training Loss (progress: 0.88), 0.170368014731836, [0.0019294765815357812, 0.07010847775719996, 0.022023682499332436, 0.0007902917414376562, 0.027768421749178505, 0.08058304671395658]
Training Loss (progress: 0.96), 0.19867523147900304, [0.0019557333401176162, 0.07117420300633238, 0.021973474767512637, 0.0008632098418648872, 0.02820917940326078, 0.08112654770317156]
Evaluation on validation dataset:
Step 25, mean loss 0.06882461399179568
Step 50, mean loss 0.031702285021435024
Step 75, mean loss 0.04860489218866054
Step 100, mean loss 0.06941293187277392
Step 125, mean loss 0.07267951259206862
Step 150, mean loss 0.08420502031575225
Step 175, mean loss 0.13246417786861076
Step 200, mean loss 0.2874762823670848
Step 225, mean loss 0.21618108871911462
Unrolled forward losses 3.678869764575044
Unrolled forward base losses 2.565701273852575
Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.1740911979398742, [0.0018975296317819163, 0.07205932321558563, 0.022134901434474465, 0.0007996282826528659, 0.028294053762696062, 0.08137579582565026]
Training Loss (progress: 0.08), 0.1643158418201943, [0.0019633244668822974, 0.07272668409098121, 0.022539726291464817, 0.0008411324279029449, 0.028436613529782287, 0.08136156710998783]
Training Loss (progress: 0.16), 0.18130229697254482, [0.0018071356649141223, 0.07355517005868273, 0.022516273811515598, 0.0008648353643084602, 0.028795244260216582, 0.08197307377886595]
Training Loss (progress: 0.24), 0.20102669106369342, [0.0018619499599389282, 0.07436092989760382, 0.022760069861991954, 0.0008681115692906887, 0.028923432358928424, 0.08219852491669252]
Training Loss (progress: 0.32), 0.19474963881161797, [0.0018545694096459217, 0.07545327756239148, 0.022875462536806838, 0.0008455518358418759, 0.029598239415356307, 0.0823772633971152]
Training Loss (progress: 0.40), 0.17484807093698443, [0.0018388455106046013, 0.07594223421585054, 0.02312229847905906, 0.0008928690780131196, 0.029748827139804535, 0.08284676793373871]
Training Loss (progress: 0.48), 0.19832240761555273, [0.0019172203648586581, 0.07695656234030665, 0.024010397458242922, 0.0008548676559826566, 0.029944807333874206, 0.08278498834935392]
Training Loss (progress: 0.56), 0.1786856313593952, [0.0019113572774893414, 0.07763433665056425, 0.02430810014796103, 0.0007523477180460907, 0.03010205369878288, 0.08329183781891174]
Training Loss (progress: 0.64), 0.1572614569298129, [0.0018269610563700087, 0.07856297836572995, 0.02453183673437521, 0.0007760248455182062, 0.03038053506330772, 0.0834067430791555]
Training Loss (progress: 0.72), 0.15681902042973703, [0.0018768185181362934, 0.0790678751859586, 0.02478232200314779, 0.0008619123686248243, 0.030730409352954582, 0.08373649849615968]
Training Loss (progress: 0.80), 0.16977015439166185, [0.0018682886754536044, 0.07985497490193365, 0.024480869902453593, 0.0007830377983883192, 0.031076930486738895, 0.08422909277567607]
Training Loss (progress: 0.88), 0.1854266249893771, [0.001818859474789991, 0.0810685859413533, 0.025052427970932992, 0.0008469541733761283, 0.031244585028457676, 0.0844793419887769]
Training Loss (progress: 0.96), 0.1751809875628894, [0.001914592413309274, 0.08202073380007599, 0.025153724121226607, 0.0008187687734254065, 0.031316259965716915, 0.08434179211259286]
Evaluation on validation dataset:
Step 25, mean loss 0.06072574287498811
Step 50, mean loss 0.02917782443448755
Step 75, mean loss 0.03763514385390917
Step 100, mean loss 0.054869507479191205
Step 125, mean loss 0.051750285997830325
Step 150, mean loss 0.05583258141374657
Step 175, mean loss 0.10402105248501811
Step 200, mean loss 0.27213945682024904
Step 225, mean loss 0.1954865007018285
Unrolled forward losses 2.9554269420777155
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.49219e-01, 9.56426e-01, 8.64934e-01, 1.10463e-02, 8.90841e-01, 9.57578e-01
Node: 01 (pos: 0.010): 2.66850e-01, 9.69535e-01, 9.04145e-01, 4.37647e-02, 9.22866e-01, 9.70346e-01
Node: 02 (pos: 0.020): 4.29349e-01, 9.80394e-01, 9.37546e-01, 1.34995e-01, 9.49924e-01, 9.80919e-01
Node: 03 (pos: 0.030): 6.21521e-01, 9.88924e-01, 9.64375e-01, 3.24194e-01, 9.71516e-01, 9.89222e-01
Node: 04 (pos: 0.040): 8.09474e-01, 9.95062e-01, 9.84007e-01, 6.06150e-01, 9.87239e-01, 9.95195e-01
Node: 05 (pos: 0.051): 9.48529e-01, 9.98763e-01, 9.95977e-01, 8.82358e-01, 9.96794e-01, 9.98797e-01
-
Node: 07 (pos: 0.071): 9.48529e-01, 9.98763e-01, 9.95977e-01, 8.82358e-01, 9.96794e-01, 9.98797e-01
Node: 08 (pos: 0.081): 8.09474e-01, 9.95062e-01, 9.84007e-01, 6.06150e-01, 9.87239e-01, 9.95195e-01
Node: 09 (pos: 0.091): 6.21521e-01, 9.88924e-01, 9.64375e-01, 3.24194e-01, 9.71516e-01, 9.89222e-01
Node: 10 (pos: 0.101): 4.29349e-01, 9.80394e-01, 9.37546e-01, 1.34995e-01, 9.49924e-01, 9.80919e-01
Node: 11 (pos: 0.111): 2.66850e-01, 9.69535e-01, 9.04145e-01, 4.37647e-02, 9.22866e-01, 9.70346e-01
Node: 12 (pos: 0.121): 1.49219e-01, 9.56426e-01, 8.64934e-01, 1.10463e-02, 8.90841e-01, 9.57578e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.48529e-01, 9.98763e-01, 9.95977e-01, 8.82358e-01, 9.96794e-01, 9.98797e-01
Node: 58 (pos: 0.586): 8.09474e-01, 9.95062e-01, 9.84007e-01, 6.06150e-01, 9.87239e-01, 9.95195e-01
Node: 59 (pos: 0.596): 6.21521e-01, 9.88924e-01, 9.64375e-01, 3.24194e-01, 9.71516e-01, 9.89222e-01
Node: 60 (pos: 0.606): 4.29349e-01, 9.80394e-01, 9.37546e-01, 1.34995e-01, 9.49924e-01, 9.80919e-01
Node: 61 (pos: 0.616): 2.66850e-01, 9.69535e-01, 9.04145e-01, 4.37647e-02, 9.22866e-01, 9.70346e-01
Node: 50 (pos: 0.505): 1.49219e-01, 9.56426e-01, 8.64934e-01, 1.10463e-02, 8.90841e-01, 9.57578e-01
-
Node: 51 (pos: 0.515): 2.66850e-01, 9.69535e-01, 9.04145e-01, 4.37647e-02, 9.22866e-01, 9.70346e-01
Node: 52 (pos: 0.525): 4.29349e-01, 9.80394e-01, 9.37546e-01, 1.34995e-01, 9.49924e-01, 9.80919e-01
Node: 53 (pos: 0.535): 6.21521e-01, 9.88924e-01, 9.64375e-01, 3.24194e-01, 9.71516e-01, 9.89222e-01
Node: 54 (pos: 0.545): 8.09474e-01, 9.95062e-01, 9.84007e-01, 6.06150e-01, 9.87239e-01, 9.95195e-01
Node: 55 (pos: 0.556): 9.48529e-01, 9.98763e-01, 9.95977e-01, 8.82358e-01, 9.96794e-01, 9.98797e-01
Node: 62 (pos: 0.626): 1.49219e-01, 9.56426e-01, 8.64934e-01, 1.10463e-02, 8.90841e-01, 9.57578e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.04877327968103473
Step 50, mean loss 0.021711198647834773
Step 75, mean loss 0.02899092462269042
Step 100, mean loss 0.035215976347625785
Step 125, mean loss 0.07482295112780563
Step 150, mean loss 0.0588350050463215
Step 175, mean loss 0.10780393536928697
Step 200, mean loss 0.09065993180974258
Step 225, mean loss 0.119945559860482
Unrolled forward losses 2.189641244467083
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.14614120097439928, [0.00181130726397614, 0.08244453120955011, 0.025358986402980684, 0.0008301433434051592, 0.03176143105381016, 0.08477939807186838]
Training Loss (progress: 0.08), 0.1423602423977543, [0.0018350021436858096, 0.08284266394113149, 0.025480186717976496, 0.0008497386439493903, 0.03188724352180792, 0.08493109403879098]
Training Loss (progress: 0.16), 0.14235117855515023, [0.0018633023297403148, 0.08319353116709594, 0.025571689903818185, 0.0008483931486875005, 0.03208209574858804, 0.08511461204878142]
Training Loss (progress: 0.24), 0.1345168008325134, [0.0018512117067725515, 0.08332701399869569, 0.025503217160565837, 0.0008036701321581883, 0.03220040641505062, 0.08537136174666206]
Training Loss (progress: 0.32), 0.15195346241537114, [0.0018300891385682977, 0.08410998429192626, 0.025677787584270056, 0.0008168427604178613, 0.0323666491613049, 0.08554773827289057]
Training Loss (progress: 0.40), 0.13242830162580535, [0.0018196283431539657, 0.08431356429969046, 0.026016067097325627, 0.0008314159320238288, 0.032432325275779994, 0.08581315721978727]
Training Loss (progress: 0.48), 0.15512294585587202, [0.0018152849366567575, 0.08460365290896309, 0.026083768872097613, 0.0008212871473395616, 0.032652380300542745, 0.08597760546909453]
Training Loss (progress: 0.56), 0.14768203589778844, [0.001838108081086547, 0.08505642054476653, 0.026170338435775743, 0.0008169337487289325, 0.0328946066631909, 0.08614594667335167]
Training Loss (progress: 0.64), 0.13984917756840431, [0.0018585523418930526, 0.0856124336157977, 0.02671825038076793, 0.000821334939791239, 0.03317622006387707, 0.08628287506901347]
Training Loss (progress: 0.72), 0.14142750887245456, [0.001859427877820239, 0.08596582967247433, 0.02664962152326912, 0.0008243515113907511, 0.03326429340953005, 0.086416578257854]
Training Loss (progress: 0.80), 0.14262637723014843, [0.0017981685483873515, 0.08624178039660861, 0.026775934622158718, 0.0008463012880889636, 0.03347749084859558, 0.08665545762037725]
Training Loss (progress: 0.88), 0.13582959556830135, [0.0018402393718051269, 0.08662643998558654, 0.027018155418180076, 0.0008505533404993232, 0.03358634941662762, 0.08678067652105624]
Training Loss (progress: 0.96), 0.14336607159710338, [0.0018814687732483595, 0.08665249003728528, 0.02691126139828933, 0.0008373021931637479, 0.03368083833881741, 0.08705457647909715]
Evaluation on validation dataset:
Step 25, mean loss 0.05728269688698267
Step 50, mean loss 0.024571623004441205
Step 75, mean loss 0.03576486040730084
Step 100, mean loss 0.04589352129332504
Step 125, mean loss 0.05398664777236907
Step 150, mean loss 0.05316933531932865
Step 175, mean loss 0.08889662911396531
Step 200, mean loss 0.2265844340433383
Step 225, mean loss 0.17990789278023497
Unrolled forward losses 2.2018903751906165
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.33170e-01, 9.58586e-01, 8.73228e-01, 1.35379e-02, 8.96922e-01, 9.58658e-01
Node: 01 (pos: 0.010): 2.46575e-01, 9.71055e-01, 9.10157e-01, 5.04042e-02, 9.27237e-01, 9.71106e-01
Node: 02 (pos: 0.020): 4.08176e-01, 9.81377e-01, 9.41531e-01, 1.47767e-01, 9.52801e-01, 9.81410e-01
Node: 03 (pos: 0.030): 6.04090e-01, 9.89482e-01, 9.66678e-01, 3.41105e-01, 9.73170e-01, 9.89500e-01
Node: 04 (pos: 0.040): 7.99304e-01, 9.95311e-01, 9.85051e-01, 6.20004e-01, 9.87985e-01, 9.95320e-01
Node: 05 (pos: 0.051): 9.45536e-01, 9.98826e-01, 9.96242e-01, 8.87358e-01, 9.96983e-01, 9.98828e-01
-
Node: 07 (pos: 0.071): 9.45536e-01, 9.98826e-01, 9.96242e-01, 8.87358e-01, 9.96983e-01, 9.98828e-01
Node: 08 (pos: 0.081): 7.99304e-01, 9.95311e-01, 9.85051e-01, 6.20004e-01, 9.87985e-01, 9.95320e-01
Node: 09 (pos: 0.091): 6.04090e-01, 9.89482e-01, 9.66678e-01, 3.41105e-01, 9.73170e-01, 9.89500e-01
Node: 10 (pos: 0.101): 4.08176e-01, 9.81377e-01, 9.41531e-01, 1.47767e-01, 9.52801e-01, 9.81410e-01
Node: 11 (pos: 0.111): 2.46575e-01, 9.71055e-01, 9.10157e-01, 5.04042e-02, 9.27237e-01, 9.71106e-01
Node: 12 (pos: 0.121): 1.33170e-01, 9.58586e-01, 8.73228e-01, 1.35379e-02, 8.96922e-01, 9.58658e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45536e-01, 9.98826e-01, 9.96242e-01, 8.87358e-01, 9.96983e-01, 9.98828e-01
Node: 58 (pos: 0.586): 7.99304e-01, 9.95311e-01, 9.85051e-01, 6.20004e-01, 9.87985e-01, 9.95320e-01
Node: 59 (pos: 0.596): 6.04090e-01, 9.89482e-01, 9.66678e-01, 3.41105e-01, 9.73170e-01, 9.89500e-01
Node: 60 (pos: 0.606): 4.08176e-01, 9.81377e-01, 9.41531e-01, 1.47767e-01, 9.52801e-01, 9.81410e-01
Node: 61 (pos: 0.616): 2.46575e-01, 9.71055e-01, 9.10157e-01, 5.04042e-02, 9.27237e-01, 9.71106e-01
Node: 50 (pos: 0.505): 1.33170e-01, 9.58586e-01, 8.73228e-01, 1.35379e-02, 8.96922e-01, 9.58658e-01
-
Node: 51 (pos: 0.515): 2.46575e-01, 9.71055e-01, 9.10157e-01, 5.04042e-02, 9.27237e-01, 9.71106e-01
Node: 52 (pos: 0.525): 4.08176e-01, 9.81377e-01, 9.41531e-01, 1.47767e-01, 9.52801e-01, 9.81410e-01
Node: 53 (pos: 0.535): 6.04090e-01, 9.89482e-01, 9.66678e-01, 3.41105e-01, 9.73170e-01, 9.89500e-01
Node: 54 (pos: 0.545): 7.99304e-01, 9.95311e-01, 9.85051e-01, 6.20004e-01, 9.87985e-01, 9.95320e-01
Node: 55 (pos: 0.556): 9.45536e-01, 9.98826e-01, 9.96242e-01, 8.87358e-01, 9.96983e-01, 9.98828e-01
Node: 62 (pos: 0.626): 1.33170e-01, 9.58586e-01, 8.73228e-01, 1.35379e-02, 8.96922e-01, 9.58658e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.047302001901072574
Step 50, mean loss 0.02061841188995643
Step 75, mean loss 0.0283738343878401
Step 100, mean loss 0.03783804870571519
Step 125, mean loss 0.06906421836181809
Step 150, mean loss 0.05303054697938893
Step 175, mean loss 0.08596396797519566
Step 200, mean loss 0.09635907728882917
Step 225, mean loss 0.11280041434137894
Unrolled forward losses 1.8435697258641195
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.1284671967954905, [0.001811836055279623, 0.0868447543009654, 0.027174629622357286, 0.0008356886065741409, 0.03369519645092279, 0.08697802614676539]
Training Loss (progress: 0.08), 0.13574072074053192, [0.0018069798729376404, 0.08732980982271676, 0.027438503228666655, 0.0008484972292628532, 0.03389693462369733, 0.08703183882968214]
Training Loss (progress: 0.16), 0.1195514850574902, [0.001778577084321786, 0.08791386746229428, 0.02733506799078536, 0.0008468940436691261, 0.03408698468912965, 0.08721773063830264]
Training Loss (progress: 0.24), 0.14714057844732797, [0.001838585980541124, 0.08816044633996283, 0.027484658743412393, 0.0008397100878655171, 0.03427198986747594, 0.08742139472160226]
Training Loss (progress: 0.32), 0.12795280176480717, [0.0018355030685434366, 0.08859944280323358, 0.02758690885453921, 0.0008482295114018409, 0.034390019809465364, 0.0875164809945466]
Training Loss (progress: 0.40), 0.1365449611625382, [0.0018331213880550222, 0.08887500722896823, 0.027825592468864672, 0.0008355640737336305, 0.034624398198896664, 0.08773109756846716]
Training Loss (progress: 0.48), 0.13440378098261818, [0.0018139109142865076, 0.08952212522955347, 0.027971512613223573, 0.0008210600574147468, 0.03475198354220531, 0.08784695524072171]
Training Loss (progress: 0.56), 0.13663923684347634, [0.001859274359501812, 0.08991196970744501, 0.02836246326883816, 0.0008229357803734804, 0.034848529372532806, 0.08800479534815041]
Training Loss (progress: 0.64), 0.12696998387151526, [0.0018081968761801427, 0.09016109405848212, 0.02848953797693732, 0.0008369634950535554, 0.034985802532844765, 0.08811073937673221]
Training Loss (progress: 0.72), 0.1364497491812681, [0.0018871794392390584, 0.09042430770846802, 0.028623136547247095, 0.0008401536168551438, 0.035256663851158734, 0.088315972705806]
Training Loss (progress: 0.80), 0.1213587590716489, [0.0018091024800561517, 0.09088339083191016, 0.028581866198840476, 0.000830866801463433, 0.035422635390209956, 0.08831123843338216]
Training Loss (progress: 0.88), 0.14397892741824983, [0.0018731336536560658, 0.09146050278296164, 0.028747612844860406, 0.000824272628739516, 0.03559704753861361, 0.08843868519492623]
Training Loss (progress: 0.96), 0.13167098522638554, [0.0018080001094260944, 0.09171277041618826, 0.028876267486625112, 0.0008772386923873338, 0.03572024084898694, 0.08865582203513797]
Evaluation on validation dataset:
Step 25, mean loss 0.07758617507933532
Step 50, mean loss 0.03005804440533015
Step 75, mean loss 0.03236510450717684
Step 100, mean loss 0.04265953817215233
Step 125, mean loss 0.04487491627519079
Step 150, mean loss 0.04270767191205588
Step 175, mean loss 0.08185889617191597
Step 200, mean loss 0.19769274054784508
Step 225, mean loss 0.16212192309647966
Unrolled forward losses 2.2484820926910443
Unrolled forward base losses 2.565701273852575
Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.12006993625582615, [0.0017924559257513158, 0.0919889436171803, 0.02902845554316908, 0.0008755782064532784, 0.035741592856012055, 0.0886705014036401]
Training Loss (progress: 0.08), 0.14149139785456263, [0.0018631761933203615, 0.09238438630432319, 0.029163435110379336, 0.0008234638075141449, 0.035955283708308465, 0.08875265058996254]
Training Loss (progress: 0.16), 0.12016288189850753, [0.0018284247653051198, 0.09278759143474477, 0.02940860983210132, 0.0008425244091332984, 0.03607222219965009, 0.08890305377096717]
Training Loss (progress: 0.24), 0.13608239027808391, [0.0018632726675686404, 0.09299650564752923, 0.02957497159290356, 0.0008197621048408103, 0.03627347543272534, 0.0891643486741796]
Training Loss (progress: 0.32), 0.12212950268457468, [0.00184224747744161, 0.09347385905453678, 0.029757412849083696, 0.0008293276918101336, 0.03624083695709472, 0.08930011908492629]
Training Loss (progress: 0.40), 0.13326690394153917, [0.0018081204072004041, 0.09391585442418769, 0.029789356408081155, 0.0008382063684227449, 0.03658398445824429, 0.08941864473301701]
Training Loss (progress: 0.48), 0.12466329071970947, [0.001837141695722817, 0.09420288494772963, 0.029861328242768125, 0.000824087784058254, 0.03666653677012408, 0.08938034876604255]
Training Loss (progress: 0.56), 0.12815400408469682, [0.0018341244819531776, 0.09446914409642741, 0.030276471714507945, 0.0008580403028129527, 0.03678474707301561, 0.0895086017659314]
Training Loss (progress: 0.64), 0.12638174814970854, [0.0018210242711855075, 0.09490882077408605, 0.030398896407680856, 0.0008621799143128149, 0.03682903706724419, 0.08961928161339923]
Training Loss (progress: 0.72), 0.12688784077668805, [0.0017844752232154173, 0.09541165127521402, 0.030554931530895398, 0.000838886912898314, 0.037142470264360065, 0.08980581870752086]
Training Loss (progress: 0.80), 0.1214717995888403, [0.0018247165869265303, 0.0955779219259204, 0.030369485277629105, 0.0008438315995982668, 0.037201383144916375, 0.08996259271568136]
Training Loss (progress: 0.88), 0.14369923606759222, [0.0018027065787762982, 0.0960812473877762, 0.030599073176185695, 0.0008369213415895561, 0.037378012817269074, 0.09010287374733508]
Training Loss (progress: 0.96), 0.13296293391577763, [0.0018563422924308074, 0.09644408740150479, 0.030911951622803198, 0.0008321536755833253, 0.03750901594664423, 0.09014352589624856]
Evaluation on validation dataset:
Step 25, mean loss 0.04004830480651868
Step 50, mean loss 0.016763522187233267
Step 75, mean loss 0.02964220353549136
Step 100, mean loss 0.0440440104600548
Step 125, mean loss 0.04335219239934192
Step 150, mean loss 0.04526634585632408
Step 175, mean loss 0.083939032399639
Step 200, mean loss 0.19193832218593834
Step 225, mean loss 0.15383036812419779
Unrolled forward losses 1.8224114247497236
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.35284e-01, 9.62689e-01, 8.88806e-01, 1.36393e-02, 9.06741e-01, 9.60095e-01
Node: 01 (pos: 0.010): 2.49287e-01, 9.73939e-01, 9.21403e-01, 5.06661e-02, 9.34274e-01, 9.72116e-01
Node: 02 (pos: 0.020): 4.11043e-01, 9.83242e-01, 9.48959e-01, 1.48258e-01, 9.57423e-01, 9.82064e-01
Node: 03 (pos: 0.030): 6.06473e-01, 9.90539e-01, 9.70961e-01, 3.41742e-01, 9.75822e-01, 9.89871e-01
Node: 04 (pos: 0.040): 8.00704e-01, 9.95784e-01, 9.86988e-01, 6.20519e-01, 9.89181e-01, 9.95485e-01
Node: 05 (pos: 0.051): 9.45950e-01, 9.98944e-01, 9.96731e-01, 8.87542e-01, 9.97284e-01, 9.98869e-01
-
Node: 07 (pos: 0.071): 9.45950e-01, 9.98944e-01, 9.96731e-01, 8.87542e-01, 9.97284e-01, 9.98869e-01
Node: 08 (pos: 0.081): 8.00704e-01, 9.95784e-01, 9.86988e-01, 6.20519e-01, 9.89181e-01, 9.95485e-01
Node: 09 (pos: 0.091): 6.06473e-01, 9.90539e-01, 9.70961e-01, 3.41742e-01, 9.75822e-01, 9.89871e-01
Node: 10 (pos: 0.101): 4.11043e-01, 9.83242e-01, 9.48959e-01, 1.48258e-01, 9.57423e-01, 9.82064e-01
Node: 11 (pos: 0.111): 2.49287e-01, 9.73939e-01, 9.21403e-01, 5.06661e-02, 9.34274e-01, 9.72116e-01
Node: 12 (pos: 0.121): 1.35284e-01, 9.62689e-01, 8.88806e-01, 1.36393e-02, 9.06741e-01, 9.60095e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45950e-01, 9.98944e-01, 9.96731e-01, 8.87542e-01, 9.97284e-01, 9.98869e-01
Node: 58 (pos: 0.586): 8.00704e-01, 9.95784e-01, 9.86988e-01, 6.20519e-01, 9.89181e-01, 9.95485e-01
Node: 59 (pos: 0.596): 6.06473e-01, 9.90539e-01, 9.70961e-01, 3.41742e-01, 9.75822e-01, 9.89871e-01
Node: 60 (pos: 0.606): 4.11043e-01, 9.83242e-01, 9.48959e-01, 1.48258e-01, 9.57423e-01, 9.82064e-01
Node: 61 (pos: 0.616): 2.49287e-01, 9.73939e-01, 9.21403e-01, 5.06661e-02, 9.34274e-01, 9.72116e-01
Node: 50 (pos: 0.505): 1.35284e-01, 9.62689e-01, 8.88806e-01, 1.36393e-02, 9.06741e-01, 9.60095e-01
-
Node: 51 (pos: 0.515): 2.49287e-01, 9.73939e-01, 9.21403e-01, 5.06661e-02, 9.34274e-01, 9.72116e-01
Node: 52 (pos: 0.525): 4.11043e-01, 9.83242e-01, 9.48959e-01, 1.48258e-01, 9.57423e-01, 9.82064e-01
Node: 53 (pos: 0.535): 6.06473e-01, 9.90539e-01, 9.70961e-01, 3.41742e-01, 9.75822e-01, 9.89871e-01
Node: 54 (pos: 0.545): 8.00704e-01, 9.95784e-01, 9.86988e-01, 6.20519e-01, 9.89181e-01, 9.95485e-01
Node: 55 (pos: 0.556): 9.45950e-01, 9.98944e-01, 9.96731e-01, 8.87542e-01, 9.97284e-01, 9.98869e-01
Node: 62 (pos: 0.626): 1.35284e-01, 9.62689e-01, 8.88806e-01, 1.36393e-02, 9.06741e-01, 9.60095e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03325025820986012
Step 50, mean loss 0.013007068557187456
Step 75, mean loss 0.02279000063265802
Step 100, mean loss 0.03255776203444492
Step 125, mean loss 0.06277387519084454
Step 150, mean loss 0.049013109478565006
Step 175, mean loss 0.07758355167526867
Step 200, mean loss 0.08246998486560898
Step 225, mean loss 0.08864649832612123
Unrolled forward losses 1.4896518082485355
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.1378637912392797, [0.0018664251339088381, 0.09680886443101361, 0.03123157187153653, 0.0008426848428263464, 0.03763540639799085, 0.09017996068254892]
Training Loss (progress: 0.08), 0.12385453792264162, [0.001772009235974626, 0.09708875604160831, 0.03135731560749346, 0.0008565352688851103, 0.037643978483797434, 0.0902842033719305]
Training Loss (progress: 0.16), 0.13528821548150552, [0.0018541626231493996, 0.09763552311065989, 0.031514980739256786, 0.0008349722750685652, 0.037768572781218916, 0.0903608576900307]
Training Loss (progress: 0.24), 0.13562841738223977, [0.001832833967230372, 0.09766696517946624, 0.031618531140702705, 0.0008437418175255709, 0.038106059268236694, 0.0904307315433738]
Training Loss (progress: 0.32), 0.12830957493319178, [0.0018402159136901499, 0.09830865789254861, 0.031867897795083484, 0.0008211967453354459, 0.038145602522216694, 0.09070693154163906]
Training Loss (progress: 0.40), 0.1265282171329831, [0.0018181662123923446, 0.0985944604980544, 0.0318201253718802, 0.0008851423579447997, 0.03837413775305035, 0.09078307972434538]
Training Loss (progress: 0.48), 0.12654502928006617, [0.0018391206532989372, 0.0989870603381855, 0.03202755231012734, 0.0008466016862073592, 0.038559097790452405, 0.09095020021879942]
Training Loss (progress: 0.56), 0.12463301073474617, [0.0018281222286802867, 0.09961005484756771, 0.03235412940386106, 0.0008299665000617876, 0.03870080590284672, 0.0911225413411986]
Training Loss (progress: 0.64), 0.14401411201416678, [0.0018163438325482578, 0.099811463825761, 0.03262535060655748, 0.0008395504870753691, 0.0388961900897497, 0.09125204531590084]
Training Loss (progress: 0.72), 0.12202087415374609, [0.0018414757717460123, 0.1002697721489616, 0.032903974547191434, 0.0008534494118300379, 0.03904962469704991, 0.09133493414449806]
Training Loss (progress: 0.80), 0.1258495625226509, [0.0017801216999303471, 0.10056385445386098, 0.03309821210064176, 0.0008586990103619396, 0.03907731802789725, 0.09143342796227638]
Training Loss (progress: 0.88), 0.12384742212963085, [0.0018073484879568768, 0.10109592326151111, 0.03328298769930569, 0.0008293586172345036, 0.03931552026884269, 0.09154057177519676]
Training Loss (progress: 0.96), 0.1249556321600499, [0.0018146671187313792, 0.1014947618389641, 0.033346884518501686, 0.00085860446204274, 0.03944732662039928, 0.09163474834166122]
Evaluation on validation dataset:
Step 25, mean loss 0.03640397058685249
Step 50, mean loss 0.01683571894979686
Step 75, mean loss 0.030347785389886424
Step 100, mean loss 0.04106260024727042
Step 125, mean loss 0.04096667333717563
Step 150, mean loss 0.04388018857916201
Step 175, mean loss 0.07551397919830331
Step 200, mean loss 0.2008063838039674
Step 225, mean loss 0.14989751254840117
Unrolled forward losses 1.9503727423848338
Unrolled forward base losses 2.565701273852575
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.1291465276074441, [0.001832531092192826, 0.10171119262445895, 0.033690828851683105, 0.0008489620569095681, 0.03956181456744418, 0.09171003550637256]
Training Loss (progress: 0.08), 0.13160893650267974, [0.0018086403769184078, 0.10205006205971075, 0.03357456464737683, 0.0008849759010910149, 0.03972634134793707, 0.09183565163691912]
Training Loss (progress: 0.16), 0.12456559289433401, [0.0017895291489943561, 0.10252558876452453, 0.03381087386261136, 0.0008294398282070266, 0.0397816860710216, 0.09194130348338621]
Training Loss (progress: 0.24), 0.13540884998519528, [0.0018197486973890874, 0.10285387521450902, 0.034039073427226486, 0.0008683281738686809, 0.039926217588827195, 0.09214778438830758]
Training Loss (progress: 0.32), 0.10831153667976358, [0.0018365249099630114, 0.10327409644995515, 0.03409193581237911, 0.000853532708741031, 0.039960545560582594, 0.09234181113718348]
Training Loss (progress: 0.40), 0.11473995702775287, [0.001806658143540234, 0.10347330590903413, 0.034300742809780106, 0.0008604212850142887, 0.04014245148856275, 0.09240356802315565]
Training Loss (progress: 0.48), 0.12851307017253508, [0.0018008972561800475, 0.10393299222366811, 0.034626392203881334, 0.0008637824685589026, 0.040260983382439795, 0.0925831957502438]
Training Loss (progress: 0.56), 0.13284377593916155, [0.0018063609902380132, 0.10426565953496593, 0.034717689397035074, 0.0008274905281424323, 0.04052620123815207, 0.09271618424768176]
Training Loss (progress: 0.64), 0.13012711687522824, [0.001820970025622724, 0.10464063164357265, 0.034716337654290316, 0.0008506491435644454, 0.04075168971461992, 0.09291657101398919]
Training Loss (progress: 0.72), 0.11658606466617781, [0.0017999059954344374, 0.1049323598802337, 0.03491678967843564, 0.0008532114385379957, 0.04085543655146987, 0.09285925271100168]
Training Loss (progress: 0.80), 0.11399799277326814, [0.0018140365905587653, 0.10526371936701491, 0.03487711879162582, 0.0008383115781141872, 0.04090843605228226, 0.0930300231245585]
Training Loss (progress: 0.88), 0.12735114257055774, [0.0018154925592242259, 0.10565658804609865, 0.035076854187952984, 0.0008219553515660849, 0.04112471948768685, 0.09302061893337209]
Training Loss (progress: 0.96), 0.12457510301830436, [0.0018117527599769204, 0.10618604504531666, 0.03522470444743871, 0.0008652418770871402, 0.041180694697392724, 0.09310432980312813]
Evaluation on validation dataset:
Step 25, mean loss 0.04149859810130431
Step 50, mean loss 0.018392992611537082
Step 75, mean loss 0.027202364813811795
Step 100, mean loss 0.03326932500846973
Step 125, mean loss 0.03847968185716874
Step 150, mean loss 0.0389852609758607
Step 175, mean loss 0.0649551601986418
Step 200, mean loss 0.1991727571212278
Step 225, mean loss 0.14886627796116006
Unrolled forward losses 1.7994101927776474
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.31572e-01, 9.66005e-01, 9.01145e-01, 1.43955e-02, 9.14899e-01, 9.61374e-01
Node: 01 (pos: 0.010): 2.44517e-01, 9.76268e-01, 9.30267e-01, 5.26007e-02, 9.40103e-01, 9.73016e-01
Node: 02 (pos: 0.020): 4.05992e-01, 9.84746e-01, 9.54792e-01, 1.51857e-01, 9.61241e-01, 9.82645e-01
Node: 03 (pos: 0.030): 6.02270e-01, 9.91391e-01, 9.74313e-01, 3.46383e-01, 9.78010e-01, 9.90200e-01
Node: 04 (pos: 0.040): 7.98233e-01, 9.96164e-01, 9.88501e-01, 6.24250e-01, 9.90166e-01, 9.95633e-01
Node: 05 (pos: 0.051): 9.45219e-01, 9.99040e-01, 9.97113e-01, 8.88873e-01, 9.97532e-01, 9.98906e-01
-
Node: 07 (pos: 0.071): 9.45219e-01, 9.99040e-01, 9.97113e-01, 8.88873e-01, 9.97532e-01, 9.98906e-01
Node: 08 (pos: 0.081): 7.98233e-01, 9.96164e-01, 9.88501e-01, 6.24250e-01, 9.90166e-01, 9.95633e-01
Node: 09 (pos: 0.091): 6.02270e-01, 9.91391e-01, 9.74313e-01, 3.46383e-01, 9.78010e-01, 9.90200e-01
Node: 10 (pos: 0.101): 4.05992e-01, 9.84746e-01, 9.54792e-01, 1.51857e-01, 9.61241e-01, 9.82645e-01
Node: 11 (pos: 0.111): 2.44517e-01, 9.76268e-01, 9.30267e-01, 5.26007e-02, 9.40103e-01, 9.73016e-01
Node: 12 (pos: 0.121): 1.31572e-01, 9.66005e-01, 9.01145e-01, 1.43955e-02, 9.14899e-01, 9.61374e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45219e-01, 9.99040e-01, 9.97113e-01, 8.88873e-01, 9.97532e-01, 9.98906e-01
Node: 58 (pos: 0.586): 7.98233e-01, 9.96164e-01, 9.88501e-01, 6.24250e-01, 9.90166e-01, 9.95633e-01
Node: 59 (pos: 0.596): 6.02270e-01, 9.91391e-01, 9.74313e-01, 3.46383e-01, 9.78010e-01, 9.90200e-01
Node: 60 (pos: 0.606): 4.05992e-01, 9.84746e-01, 9.54792e-01, 1.51857e-01, 9.61241e-01, 9.82645e-01
Node: 61 (pos: 0.616): 2.44517e-01, 9.76268e-01, 9.30267e-01, 5.26007e-02, 9.40103e-01, 9.73016e-01
Node: 50 (pos: 0.505): 1.31572e-01, 9.66005e-01, 9.01145e-01, 1.43955e-02, 9.14899e-01, 9.61374e-01
-
Node: 51 (pos: 0.515): 2.44517e-01, 9.76268e-01, 9.30267e-01, 5.26007e-02, 9.40103e-01, 9.73016e-01
Node: 52 (pos: 0.525): 4.05992e-01, 9.84746e-01, 9.54792e-01, 1.51857e-01, 9.61241e-01, 9.82645e-01
Node: 53 (pos: 0.535): 6.02270e-01, 9.91391e-01, 9.74313e-01, 3.46383e-01, 9.78010e-01, 9.90200e-01
Node: 54 (pos: 0.545): 7.98233e-01, 9.96164e-01, 9.88501e-01, 6.24250e-01, 9.90166e-01, 9.95633e-01
Node: 55 (pos: 0.556): 9.45219e-01, 9.99040e-01, 9.97113e-01, 8.88873e-01, 9.97532e-01, 9.98906e-01
Node: 62 (pos: 0.626): 1.31572e-01, 9.66005e-01, 9.01145e-01, 1.43955e-02, 9.14899e-01, 9.61374e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03531122740849513
Step 50, mean loss 0.014490163356169473
Step 75, mean loss 0.019102696849872012
Step 100, mean loss 0.02532738014680277
Step 125, mean loss 0.0538449822948201
Step 150, mean loss 0.03920576607098802
Step 175, mean loss 0.06402216890591017
Step 200, mean loss 0.06946203934406225
Step 225, mean loss 0.08001922153296964
Unrolled forward losses 1.3800610614830697
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.09929085210559867, [0.0018228672204012358, 0.10614014710478177, 0.035320453017921895, 0.0008523740152202697, 0.041318537978023755, 0.0932501153061259]
Training Loss (progress: 0.08), 0.10248420196893347, [0.0018195810715256166, 0.10631093511212993, 0.035482749563797995, 0.0008585731746663833, 0.041411204620527275, 0.09330642979330547]
Training Loss (progress: 0.16), 0.11419408393970486, [0.0018089277451553449, 0.10658900377172573, 0.03562810101042386, 0.0008389166569771847, 0.04147512889540756, 0.09349839441666485]
Training Loss (progress: 0.24), 0.11134243535253685, [0.0018169369892823072, 0.10675435781977576, 0.035750788831614505, 0.000856075447996713, 0.041564044901870774, 0.09358933748382367]
Training Loss (progress: 0.32), 0.12048341962910086, [0.0018290958923177108, 0.10703301898442107, 0.03585487736344091, 0.0008425890902440988, 0.04164727694390431, 0.09365676751003071]
Training Loss (progress: 0.40), 0.10267855020956138, [0.0018075728564945885, 0.10717833504076106, 0.035911527033964906, 0.0008566405788352001, 0.04172099911536947, 0.09378773535319235]
Training Loss (progress: 0.48), 0.11676719364961445, [0.0018065730474885596, 0.1074562626657145, 0.03601919378856189, 0.000871084323089227, 0.04182752778681354, 0.09386382362783684]
Training Loss (progress: 0.56), 0.10251109720304465, [0.0018232828583714252, 0.10760306806257536, 0.03625498762748419, 0.0008382429574631501, 0.041896657785394614, 0.09395932610400665]
Training Loss (progress: 0.64), 0.11427027547480018, [0.0018165929261398845, 0.10770523732578284, 0.03625875157614827, 0.0008553102333305161, 0.041966833249617023, 0.09402928201146066]
Training Loss (progress: 0.72), 0.10879464798084815, [0.0017957105604125337, 0.10786216679136819, 0.0363915945112965, 0.0008494256730891141, 0.04206105280553134, 0.09408559053321974]
Training Loss (progress: 0.80), 0.12273171821519666, [0.001844485950306103, 0.10809351311659426, 0.036467426473236836, 0.0008550207225833441, 0.042146596399910904, 0.09428418800526371]
Training Loss (progress: 0.88), 0.10504189553850447, [0.0018210449428093215, 0.10829479303401247, 0.03655999228097699, 0.0008519014924463101, 0.042212576696401966, 0.09434048740693231]
Training Loss (progress: 0.96), 0.11294950536683175, [0.001823846207556425, 0.10851476787066844, 0.03660189940816253, 0.0008650937202244679, 0.04235781664517632, 0.09438885308595979]
Evaluation on validation dataset:
Step 25, mean loss 0.03061840974064048
Step 50, mean loss 0.014868493759441121
Step 75, mean loss 0.024651522500416835
Step 100, mean loss 0.03381285801057342
Step 125, mean loss 0.03536809112342162
Step 150, mean loss 0.03782615344382345
Step 175, mean loss 0.0688895184400475
Step 200, mean loss 0.18822617616638287
Step 225, mean loss 0.13719845109119738
Unrolled forward losses 1.62654211514482
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.34351e-01, 9.66731e-01, 9.04388e-01, 1.31741e-02, 9.16906e-01, 9.61844e-01
Node: 01 (pos: 0.010): 2.48092e-01, 9.76777e-01, 9.32590e-01, 4.94596e-02, 9.41536e-01, 9.73345e-01
Node: 02 (pos: 0.020): 4.09781e-01, 9.85075e-01, 9.56318e-01, 1.45989e-01, 9.62178e-01, 9.82858e-01
Node: 03 (pos: 0.030): 6.05425e-01, 9.91577e-01, 9.75189e-01, 3.38790e-01, 9.78546e-01, 9.90321e-01
Node: 04 (pos: 0.040): 8.00089e-01, 9.96248e-01, 9.88896e-01, 6.18130e-01, 9.90407e-01, 9.95687e-01
Node: 05 (pos: 0.051): 9.45768e-01, 9.99061e-01, 9.97212e-01, 8.86686e-01, 9.97593e-01, 9.98920e-01
-
Node: 07 (pos: 0.071): 9.45768e-01, 9.99061e-01, 9.97212e-01, 8.86686e-01, 9.97593e-01, 9.98920e-01
Node: 08 (pos: 0.081): 8.00089e-01, 9.96248e-01, 9.88896e-01, 6.18130e-01, 9.90407e-01, 9.95687e-01
Node: 09 (pos: 0.091): 6.05425e-01, 9.91577e-01, 9.75189e-01, 3.38790e-01, 9.78546e-01, 9.90321e-01
Node: 10 (pos: 0.101): 4.09781e-01, 9.85075e-01, 9.56318e-01, 1.45989e-01, 9.62178e-01, 9.82858e-01
Node: 11 (pos: 0.111): 2.48092e-01, 9.76777e-01, 9.32590e-01, 4.94596e-02, 9.41536e-01, 9.73345e-01
Node: 12 (pos: 0.121): 1.34351e-01, 9.66731e-01, 9.04388e-01, 1.31741e-02, 9.16906e-01, 9.61844e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45768e-01, 9.99061e-01, 9.97212e-01, 8.86686e-01, 9.97593e-01, 9.98920e-01
Node: 58 (pos: 0.586): 8.00089e-01, 9.96248e-01, 9.88896e-01, 6.18130e-01, 9.90407e-01, 9.95687e-01
Node: 59 (pos: 0.596): 6.05425e-01, 9.91577e-01, 9.75189e-01, 3.38790e-01, 9.78546e-01, 9.90321e-01
Node: 60 (pos: 0.606): 4.09781e-01, 9.85075e-01, 9.56318e-01, 1.45989e-01, 9.62178e-01, 9.82858e-01
Node: 61 (pos: 0.616): 2.48092e-01, 9.76777e-01, 9.32590e-01, 4.94596e-02, 9.41536e-01, 9.73345e-01
Node: 50 (pos: 0.505): 1.34351e-01, 9.66731e-01, 9.04388e-01, 1.31741e-02, 9.16906e-01, 9.61844e-01
-
Node: 51 (pos: 0.515): 2.48092e-01, 9.76777e-01, 9.32590e-01, 4.94596e-02, 9.41536e-01, 9.73345e-01
Node: 52 (pos: 0.525): 4.09781e-01, 9.85075e-01, 9.56318e-01, 1.45989e-01, 9.62178e-01, 9.82858e-01
Node: 53 (pos: 0.535): 6.05425e-01, 9.91577e-01, 9.75189e-01, 3.38790e-01, 9.78546e-01, 9.90321e-01
Node: 54 (pos: 0.545): 8.00089e-01, 9.96248e-01, 9.88896e-01, 6.18130e-01, 9.90407e-01, 9.95687e-01
Node: 55 (pos: 0.556): 9.45768e-01, 9.99061e-01, 9.97212e-01, 8.86686e-01, 9.97593e-01, 9.98920e-01
Node: 62 (pos: 0.626): 1.34351e-01, 9.66731e-01, 9.04388e-01, 1.31741e-02, 9.16906e-01, 9.61844e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.02613483069436478
Step 50, mean loss 0.010989080750516862
Step 75, mean loss 0.017585791288325683
Step 100, mean loss 0.024326815715769497
Step 125, mean loss 0.046563979380063555
Step 150, mean loss 0.03831842902485652
Step 175, mean loss 0.062603098540269
Step 200, mean loss 0.06699601021834584
Step 225, mean loss 0.0793321582400353
Unrolled forward losses 1.3789792856005318
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.11367308416823522, [0.0018379028158904934, 0.1085428249714095, 0.03655897656813484, 0.000859211885097311, 0.042348098331831806, 0.09440950591634671]
Training Loss (progress: 0.08), 0.1255877075696773, [0.0018044428222962698, 0.10877078338185832, 0.03660274878897442, 0.0008524024545281475, 0.04244305416597429, 0.0945243194854297]
Training Loss (progress: 0.16), 0.10569665692820708, [0.0018255151355277411, 0.10890879644323725, 0.036630036289627084, 0.0008725176515887879, 0.04256516605059179, 0.0945593524298161]
Training Loss (progress: 0.24), 0.11735063288831081, [0.0018199394614769684, 0.1091777810833423, 0.03670671679557343, 0.000844806006268615, 0.042629256583759254, 0.0946731540182375]
Training Loss (progress: 0.32), 0.11541706066598323, [0.0018412262586468813, 0.10935186439500348, 0.03676659906045292, 0.000844507626911718, 0.04264531855242468, 0.09474405519312722]
Training Loss (progress: 0.40), 0.10675664246665022, [0.0017992464960354241, 0.10947936556771214, 0.03682550709630565, 0.0008638441796292958, 0.04272220557980498, 0.09474621791308424]
Training Loss (progress: 0.48), 0.12125746177711048, [0.0018015083341255944, 0.10965084787149132, 0.03696160059377821, 0.0008660003843677954, 0.04282562231364354, 0.09481737385794378]
Training Loss (progress: 0.56), 0.1132855165205389, [0.001821451591376541, 0.10986022105197167, 0.03698241756085104, 0.000851361377468803, 0.04291255177722786, 0.09484599085208867]
Training Loss (progress: 0.64), 0.12031454159401445, [0.0017970426502793588, 0.1101120721921602, 0.037089745811126645, 0.0008651983897455117, 0.04294016626555915, 0.09492708675267947]
Training Loss (progress: 0.72), 0.11045441371386291, [0.0018161687739055349, 0.11020740562122423, 0.03726231963294841, 0.0008636232078550111, 0.04305870831319324, 0.0950667596422131]
Training Loss (progress: 0.80), 0.10310057679823834, [0.001813339369398952, 0.11033635959370514, 0.03729534227644489, 0.0008601621711693248, 0.04311273902289935, 0.09514841931322725]
Training Loss (progress: 0.88), 0.11201532112152171, [0.0018203110304758634, 0.11066461757072861, 0.03738288431903677, 0.0008692510033835463, 0.04322610100827669, 0.09518959644161078]
Training Loss (progress: 0.96), 0.12388491209850966, [0.001809634217300063, 0.1107310891356296, 0.037358830270027975, 0.0008858162937812596, 0.04330055143388704, 0.09521149709443359]
Evaluation on validation dataset:
Step 25, mean loss 0.02881015740759061
Step 50, mean loss 0.014441396156920448
Step 75, mean loss 0.02399753741155754
Step 100, mean loss 0.031436198444315305
Step 125, mean loss 0.03296309041797039
Step 150, mean loss 0.034877770141238856
Step 175, mean loss 0.06470405292531742
Step 200, mean loss 0.1733732633690608
Step 225, mean loss 0.13402072638694143
Unrolled forward losses 1.5554237664318276
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.31109e-01, 9.67408e-01, 9.06502e-01, 1.34133e-02, 9.18684e-01, 9.62168e-01
Node: 01 (pos: 0.010): 2.43919e-01, 9.77253e-01, 9.34103e-01, 5.00815e-02, 9.42803e-01, 9.73573e-01
Node: 02 (pos: 0.020): 4.05356e-01, 9.85381e-01, 9.57310e-01, 1.47161e-01, 9.63007e-01, 9.83005e-01
Node: 03 (pos: 0.030): 6.01739e-01, 9.91751e-01, 9.75758e-01, 3.40317e-01, 9.79020e-01, 9.90405e-01
Node: 04 (pos: 0.040): 7.97920e-01, 9.96325e-01, 9.89152e-01, 6.19367e-01, 9.90621e-01, 9.95724e-01
Node: 05 (pos: 0.051): 9.45126e-01, 9.99080e-01, 9.97277e-01, 8.87130e-01, 9.97647e-01, 9.98929e-01
-
Node: 07 (pos: 0.071): 9.45126e-01, 9.99080e-01, 9.97277e-01, 8.87130e-01, 9.97647e-01, 9.98929e-01
Node: 08 (pos: 0.081): 7.97920e-01, 9.96325e-01, 9.89152e-01, 6.19367e-01, 9.90621e-01, 9.95724e-01
Node: 09 (pos: 0.091): 6.01739e-01, 9.91751e-01, 9.75758e-01, 3.40317e-01, 9.79020e-01, 9.90405e-01
Node: 10 (pos: 0.101): 4.05356e-01, 9.85381e-01, 9.57310e-01, 1.47161e-01, 9.63007e-01, 9.83005e-01
Node: 11 (pos: 0.111): 2.43919e-01, 9.77253e-01, 9.34103e-01, 5.00815e-02, 9.42803e-01, 9.73573e-01
Node: 12 (pos: 0.121): 1.31109e-01, 9.67408e-01, 9.06502e-01, 1.34133e-02, 9.18684e-01, 9.62168e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45126e-01, 9.99080e-01, 9.97277e-01, 8.87130e-01, 9.97647e-01, 9.98929e-01
Node: 58 (pos: 0.586): 7.97920e-01, 9.96325e-01, 9.89152e-01, 6.19367e-01, 9.90621e-01, 9.95724e-01
Node: 59 (pos: 0.596): 6.01739e-01, 9.91751e-01, 9.75758e-01, 3.40317e-01, 9.79020e-01, 9.90405e-01
Node: 60 (pos: 0.606): 4.05356e-01, 9.85381e-01, 9.57310e-01, 1.47161e-01, 9.63007e-01, 9.83005e-01
Node: 61 (pos: 0.616): 2.43919e-01, 9.77253e-01, 9.34103e-01, 5.00815e-02, 9.42803e-01, 9.73573e-01
Node: 50 (pos: 0.505): 1.31109e-01, 9.67408e-01, 9.06502e-01, 1.34133e-02, 9.18684e-01, 9.62168e-01
-
Node: 51 (pos: 0.515): 2.43919e-01, 9.77253e-01, 9.34103e-01, 5.00815e-02, 9.42803e-01, 9.73573e-01
Node: 52 (pos: 0.525): 4.05356e-01, 9.85381e-01, 9.57310e-01, 1.47161e-01, 9.63007e-01, 9.83005e-01
Node: 53 (pos: 0.535): 6.01739e-01, 9.91751e-01, 9.75758e-01, 3.40317e-01, 9.79020e-01, 9.90405e-01
Node: 54 (pos: 0.545): 7.97920e-01, 9.96325e-01, 9.89152e-01, 6.19367e-01, 9.90621e-01, 9.95724e-01
Node: 55 (pos: 0.556): 9.45126e-01, 9.99080e-01, 9.97277e-01, 8.87130e-01, 9.97647e-01, 9.98929e-01
Node: 62 (pos: 0.626): 1.31109e-01, 9.67408e-01, 9.06502e-01, 1.34133e-02, 9.18684e-01, 9.62168e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.024502529097981198
Step 50, mean loss 0.010800538114182752
Step 75, mean loss 0.01713818772380954
Step 100, mean loss 0.024307946464745142
Step 125, mean loss 0.05040851731903219
Step 150, mean loss 0.03685374895647639
Step 175, mean loss 0.05970174744363119
Step 200, mean loss 0.06412978883976007
Step 225, mean loss 0.07555137002779527
Unrolled forward losses 1.280280164877825
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.11607818174789909, [0.0018001674881749287, 0.11088851076824616, 0.037495844235138014, 0.0008601125339365987, 0.043347565702583336, 0.09528251897839085]
Training Loss (progress: 0.08), 0.11108557828030285, [0.0018105976821792946, 0.11098812701777618, 0.03748397790129608, 0.0008548790647601146, 0.043437636632324236, 0.09529397655001401]
Training Loss (progress: 0.16), 0.11168933959890964, [0.0018049558545825896, 0.1111592278047141, 0.03750970018503663, 0.0008475119116851974, 0.0434766703581823, 0.09530314474742667]
Training Loss (progress: 0.24), 0.11312540493602785, [0.0017923421762926653, 0.11136516956037398, 0.03764321646662274, 0.0008618112614101257, 0.04356118612431424, 0.09540009351506497]
Training Loss (progress: 0.32), 0.11298385622673454, [0.0018229264208117011, 0.1116134945103277, 0.03770585529645587, 0.0008598387136611539, 0.043704967844228386, 0.09545772076751968]
Training Loss (progress: 0.40), 0.1093094490642149, [0.00181859277823413, 0.11172751419360788, 0.03782742471602816, 0.000853041605529241, 0.04374522606214415, 0.09552964530604972]
Training Loss (progress: 0.48), 0.11801856244871034, [0.001808779960246983, 0.11188038045989009, 0.03791854077681001, 0.0008521577846592429, 0.043817796451674655, 0.09560336605866458]
Training Loss (progress: 0.56), 0.09628292876265314, [0.001806309295973702, 0.1120749511812337, 0.037954036571356396, 0.0008724584877782254, 0.04384202519991014, 0.09566966075702807]
Training Loss (progress: 0.64), 0.10611355464313724, [0.0017808354870829475, 0.11229479714054408, 0.037994603309189856, 0.000864578615796012, 0.04394989198680657, 0.09576145655182494]
Training Loss (progress: 0.72), 0.11176147896253853, [0.001823251891050376, 0.1123785843774159, 0.03797534972568691, 0.0008602234543980933, 0.04397365861926779, 0.0958548554198112]
Training Loss (progress: 0.80), 0.10856098385936078, [0.001796828437424006, 0.1124802698986611, 0.038188353474771126, 0.0008677292991243827, 0.043996160775609854, 0.09593105061944832]
Training Loss (progress: 0.88), 0.11731571409353893, [0.0018132912982298953, 0.11270416232382674, 0.038337655981341826, 0.0008628767048770704, 0.044035398426022374, 0.09598762490194504]
Training Loss (progress: 0.96), 0.10835533130520277, [0.0018315981721246583, 0.11290605081315039, 0.03837902216801055, 0.0008701416497761535, 0.044138979778921145, 0.09608410293282812]
Evaluation on validation dataset:
Step 25, mean loss 0.028739386368999528
Step 50, mean loss 0.014532622066559746
Step 75, mean loss 0.023636306570677296
Step 100, mean loss 0.0314877540492848
Step 125, mean loss 0.03398137280413327
Step 150, mean loss 0.03632548095750107
Step 175, mean loss 0.06275577446190536
Step 200, mean loss 0.1815088815291478
Step 225, mean loss 0.13724650930276944
Unrolled forward losses 1.6215783940904849
Unrolled forward base losses 2.565701273852575
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.11346096544822533, [0.001807866139182824, 0.11294755036625276, 0.038384943613997594, 0.0008574467176719569, 0.04417493579650331, 0.09605945697679644]
Training Loss (progress: 0.08), 0.12408513882550805, [0.0018197838784862529, 0.1130285410079178, 0.03850191139465688, 0.0008649239360711074, 0.04420766244864762, 0.09620115738099258]
Training Loss (progress: 0.16), 0.11001232528005073, [0.0018237628633383288, 0.11311765532555464, 0.038623405611699205, 0.000844573063155064, 0.04432395462760724, 0.09627350011944211]
Training Loss (progress: 0.24), 0.12805438637795072, [0.0018130200385587706, 0.1133482163789103, 0.03872381016553221, 0.0008708461910614378, 0.04440421261070705, 0.09633320464062979]
Training Loss (progress: 0.32), 0.11721460178086042, [0.001807472109556286, 0.11355432780290109, 0.038723836149337144, 0.0008743594698523115, 0.04446776177247827, 0.09639524511043449]
Training Loss (progress: 0.40), 0.10640503362548569, [0.0018024268513043612, 0.11370524871119732, 0.038856303864130415, 0.0008835182953067651, 0.04456094733215964, 0.09644128507111352]
Training Loss (progress: 0.48), 0.11650964420385662, [0.0017971939774411012, 0.11387247819635837, 0.03893146954205709, 0.0008780240153328052, 0.044590809552675435, 0.09653495997228739]
Training Loss (progress: 0.56), 0.1066485271985465, [0.0018071953781008264, 0.11406408815033496, 0.039044831393577874, 0.0008681549643633103, 0.04470259981904044, 0.09661796339408323]
Training Loss (progress: 0.64), 0.10010014637514338, [0.0018193066610611237, 0.11414478933720382, 0.039157495935003554, 0.0008682991368832394, 0.044795870438607076, 0.09664325889182551]
Training Loss (progress: 0.72), 0.11690509546928121, [0.0018222384518847383, 0.11434789190539728, 0.03913652232891644, 0.0008536018596103376, 0.044922201457701434, 0.0967102708921966]
Training Loss (progress: 0.80), 0.1152878832102945, [0.0018285443927472983, 0.11461827649323326, 0.03926795099307775, 0.0008596713149470537, 0.044992465210113686, 0.09673681911735359]
Training Loss (progress: 0.88), 0.11908757891128378, [0.0018180700136008858, 0.11476369218842959, 0.03935213965553578, 0.0008698747990263203, 0.04506914633254456, 0.09684891703085442]
Training Loss (progress: 0.96), 0.11472304824537133, [0.0018264090170268166, 0.11484064226445863, 0.039354235956052114, 0.00087480241050453, 0.04509994538900527, 0.09696307230975348]
Evaluation on validation dataset:
Step 25, mean loss 0.027976822234103887
Step 50, mean loss 0.01331389800345167
Step 75, mean loss 0.023757055992317595
Step 100, mean loss 0.033363363735235824
Step 125, mean loss 0.03292317760211394
Step 150, mean loss 0.0364081245037523
Step 175, mean loss 0.06573321803966113
Step 200, mean loss 0.1816541085269139
Step 225, mean loss 0.13237981459588863
Unrolled forward losses 1.5732629196446195
Unrolled forward base losses 2.565701273852575
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00), 0.10153634287094879, [0.0018288949217757716, 0.11497206349867232, 0.039442881572374765, 0.0008651381045490521, 0.04510217669747594, 0.09696276825482274]
Training Loss (progress: 0.08), 0.10714815859102506, [0.0018067316874995588, 0.11517035294868033, 0.039470884848477605, 0.0008570998512435603, 0.045184046296553274, 0.09706099749881171]
Training Loss (progress: 0.16), 0.10923477778649161, [0.0017898219824253165, 0.11540754844516343, 0.03962807237136509, 0.0008611014097390745, 0.045269615290582155, 0.09712086513414259]
Training Loss (progress: 0.24), 0.11263593686583405, [0.0017970059460956976, 0.11546830726654318, 0.03977022698771188, 0.0008740005325190118, 0.045355341196566465, 0.09715210072798147]
Training Loss (progress: 0.32), 0.11641297368198329, [0.0018173546444757273, 0.11567095041641813, 0.03981565307926862, 0.0008654016927345915, 0.04543372569932252, 0.09718033785005176]
Training Loss (progress: 0.40), 0.11076205812440958, [0.0018230234187976103, 0.11580816895558899, 0.03999673048323922, 0.0008596559092574102, 0.045585753132556035, 0.09728023176788993]
Training Loss (progress: 0.48), 0.1101193478738459, [0.0018127454293887862, 0.11581886526856507, 0.040036279183587056, 0.0008606734544356129, 0.045596708503220215, 0.09737261193480465]
Training Loss (progress: 0.56), 0.1214367386182874, [0.001797516106007732, 0.11602074726324997, 0.040131700038751925, 0.0008683031483351088, 0.045727594577471116, 0.09739316611803633]
Training Loss (progress: 0.64), 0.10960406652531134, [0.0017853964392837134, 0.1162597191962115, 0.040334151261943584, 0.0008739096610836733, 0.04572387043682756, 0.0975090878561954]
Training Loss (progress: 0.72), 0.10315952803024678, [0.0018061775483695164, 0.11638306201178363, 0.04030713437546375, 0.0008627612916689472, 0.04581961461401035, 0.09754711459280271]
Training Loss (progress: 0.80), 0.11019224075099668, [0.0018028271804841419, 0.11655063062628146, 0.04037981298358895, 0.0008583109362452853, 0.045863531426914635, 0.09761573567903531]
Training Loss (progress: 0.88), 0.10775420326768669, [0.0018070086744580323, 0.1166948904413867, 0.04044108936850042, 0.0008590531542451824, 0.045976143702857034, 0.09766261100282471]
Training Loss (progress: 0.96), 0.10676113025882544, [0.0018025008655260483, 0.11686874821937175, 0.040583346698856326, 0.000873108662388829, 0.046036870403517574, 0.09771110994824025]
Evaluation on validation dataset:
Step 25, mean loss 0.027169560581781882
Step 50, mean loss 0.014220684851803984
Step 75, mean loss 0.023435773755188974
Step 100, mean loss 0.030516804830281803
Step 125, mean loss 0.03244085098820062
Step 150, mean loss 0.035009152691953586
Step 175, mean loss 0.06573148251087141
Step 200, mean loss 0.18186179840826225
Step 225, mean loss 0.12818552285551454
Unrolled forward losses 1.5831435072146578
Unrolled forward base losses 2.565701273852575
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00), 0.11381766657265825, [0.0018062229316046174, 0.11698932578938538, 0.04075665152565987, 0.0008515147588486736, 0.04606051122468073, 0.097731193843399]
Training Loss (progress: 0.08), 0.10038351710435282, [0.0018052454745299906, 0.1170152838870258, 0.040788628681439766, 0.0008634916832348711, 0.046118831935947395, 0.09779998821732741]
Training Loss (progress: 0.16), 0.10931872474850376, [0.0018092369984869, 0.11713505786100464, 0.040825889539874795, 0.0008705659294430028, 0.04616998666148001, 0.09780834955819302]
Training Loss (progress: 0.24), 0.1087599841525401, [0.001803105810892152, 0.11727221009280539, 0.0408675789399616, 0.0008697224939690455, 0.04620326853211176, 0.09786166315211409]
Training Loss (progress: 0.32), 0.10279746473913716, [0.001809961838306336, 0.11731722518805994, 0.04087332602391753, 0.0008788465205148602, 0.046234448369421755, 0.0979004081221983]
Training Loss (progress: 0.40), 0.1000255723884398, [0.00180524365551722, 0.11741469799081887, 0.040944709586536655, 0.0008625794872960799, 0.04625942144995007, 0.09792673337161162]
Training Loss (progress: 0.48), 0.10753756641029412, [0.0018091495044596675, 0.11750361932699883, 0.04097153551691288, 0.000865947824602876, 0.04628579001458116, 0.09798248593703808]
Training Loss (progress: 0.56), 0.10788672199850376, [0.0018064525608759972, 0.11751809931683252, 0.04098396308235312, 0.0008651897603026371, 0.046322534142148486, 0.09800442810563183]
Training Loss (progress: 0.64), 0.10169874383022083, [0.0018141598184492412, 0.11755572087328, 0.04095038805564683, 0.0008651221403156482, 0.04638152769379365, 0.09805075078299394]
Training Loss (progress: 0.72), 0.09557604325164719, [0.0017955349920360962, 0.11769235837698573, 0.041026290473306234, 0.0008652830630068483, 0.046403190364592485, 0.09807701916618469]
Training Loss (progress: 0.80), 0.10585361753420616, [0.0018199901256473255, 0.11784721529116474, 0.04110269477911148, 0.0008569818468212303, 0.046408148720800214, 0.09811877134171872]
Training Loss (progress: 0.88), 0.10468940583288376, [0.00180823865025053, 0.11792558078213751, 0.04116082114502768, 0.0008718515264178688, 0.04646528598420711, 0.09815193697411927]
Training Loss (progress: 0.96), 0.1000034926451475, [0.0018059247198389603, 0.11801825474253688, 0.04121999281583273, 0.0008692413231914209, 0.04651608834373062, 0.0982149905712676]
Evaluation on validation dataset:
Step 25, mean loss 0.02467380325905871
Step 50, mean loss 0.01331574312141533
Step 75, mean loss 0.02270261583687255
Step 100, mean loss 0.030206731920987585
Step 125, mean loss 0.03220358079273594
Step 150, mean loss 0.03386886479861386
Step 175, mean loss 0.06021048925421542
Step 200, mean loss 0.1755069451667603
Step 225, mean loss 0.1273947406181895
Unrolled forward losses 1.5912141186108992
Unrolled forward base losses 2.565701273852575
Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00), 0.10586954531842825, [0.0018057701233292831, 0.1180612729767343, 0.04121904450813161, 0.0008606511540767683, 0.04650845394852295, 0.09822891094062136]
Training Loss (progress: 0.08), 0.10128522888223615, [0.0018333532293149219, 0.11814944657948404, 0.041211754405510295, 0.0008609857349581977, 0.04656763471954594, 0.09826906768316786]
Training Loss (progress: 0.16), 0.09879685307798097, [0.001811197175723265, 0.11818017793633515, 0.04122930061274331, 0.0008758220920473998, 0.0466269457762248, 0.09831071762198149]
Training Loss (progress: 0.24), 0.10199124921383688, [0.0018111993945477852, 0.11830252258138864, 0.04130072908185923, 0.0008685949867017734, 0.04667840361059215, 0.0983579151694957]
Training Loss (progress: 0.32), 0.09599775287819526, [0.0018304148610931643, 0.11835315772506354, 0.041407299463900656, 0.0008730458103532565, 0.04671242593831003, 0.09839888045216985]
Training Loss (progress: 0.40), 0.09586501194687676, [0.0018148802001975261, 0.11845059321908799, 0.04141666705966259, 0.0008764341064658617, 0.046745014134206876, 0.09843326564514605]
Training Loss (progress: 0.48), 0.10416472597837291, [0.0017962146584584028, 0.11851515650551502, 0.04145587789878583, 0.0008720666675260696, 0.046792580820633166, 0.09846737798805949]
Training Loss (progress: 0.56), 0.11069835504612971, [0.001834228227087538, 0.11865809115398099, 0.04154792353356187, 0.0008728767753213826, 0.04682005116928371, 0.09849915957658792]
Training Loss (progress: 0.64), 0.10812468152885076, [0.0018108106117095408, 0.11870963053455658, 0.04155006480759338, 0.0008775297181227257, 0.04683563182459621, 0.0985576435358675]
Training Loss (progress: 0.72), 0.10670113797433461, [0.0018187302345695259, 0.118812890790274, 0.04159517958458692, 0.0008717723759431879, 0.04686043551424356, 0.09862839515685862]
Training Loss (progress: 0.80), 0.10247616875494897, [0.001823962213626937, 0.11887516109147833, 0.04164253103610729, 0.0008704522459745228, 0.04691825114117625, 0.09862837085081977]
Training Loss (progress: 0.88), 0.101549421744599, [0.00182043831776181, 0.11897548647463073, 0.041708414689421346, 0.0008782522493917111, 0.04694416101914964, 0.0986753924449297]
Training Loss (progress: 0.96), 0.10729364783349499, [0.001810049409501847, 0.1190429246293042, 0.04173537131897804, 0.0008763071322740088, 0.046968895708097834, 0.09867705985145865]
Evaluation on validation dataset:
Step 25, mean loss 0.024344328456782492
Step 50, mean loss 0.013818495672354724
Step 75, mean loss 0.0222060685349791
Step 100, mean loss 0.029492083726407124
Step 125, mean loss 0.03145569900417052
Step 150, mean loss 0.0336781626782153
Step 175, mean loss 0.06018262397592926
Step 200, mean loss 0.17719766446737678
Step 225, mean loss 0.1281100798605559
Unrolled forward losses 1.5538050473774825
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.33049e-01, 9.69625e-01, 9.15819e-01, 1.45028e-02, 9.24777e-01, 9.63468e-01
Node: 01 (pos: 0.010): 2.46419e-01, 9.78807e-01, 9.40760e-01, 5.28726e-02, 9.47141e-01, 9.74487e-01
Node: 02 (pos: 0.020): 4.08010e-01, 9.86384e-01, 9.61671e-01, 1.52359e-01, 9.65840e-01, 9.83596e-01
Node: 03 (pos: 0.030): 6.03952e-01, 9.92318e-01, 9.78256e-01, 3.47027e-01, 9.80639e-01, 9.90739e-01
Node: 04 (pos: 0.040): 7.99223e-01, 9.96579e-01, 9.90277e-01, 6.24765e-01, 9.91348e-01, 9.95873e-01
Node: 05 (pos: 0.051): 9.45512e-01, 9.99144e-01, 9.97560e-01, 8.89056e-01, 9.97830e-01, 9.98967e-01
-
Node: 07 (pos: 0.071): 9.45512e-01, 9.99144e-01, 9.97560e-01, 8.89056e-01, 9.97830e-01, 9.98967e-01
Node: 08 (pos: 0.081): 7.99223e-01, 9.96579e-01, 9.90277e-01, 6.24765e-01, 9.91348e-01, 9.95873e-01
Node: 09 (pos: 0.091): 6.03952e-01, 9.92318e-01, 9.78256e-01, 3.47027e-01, 9.80639e-01, 9.90739e-01
Node: 10 (pos: 0.101): 4.08010e-01, 9.86384e-01, 9.61671e-01, 1.52359e-01, 9.65840e-01, 9.83596e-01
Node: 11 (pos: 0.111): 2.46419e-01, 9.78807e-01, 9.40760e-01, 5.28726e-02, 9.47141e-01, 9.74487e-01
Node: 12 (pos: 0.121): 1.33049e-01, 9.69625e-01, 9.15819e-01, 1.45028e-02, 9.24777e-01, 9.63468e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45512e-01, 9.99144e-01, 9.97560e-01, 8.89056e-01, 9.97830e-01, 9.98967e-01
Node: 58 (pos: 0.586): 7.99223e-01, 9.96579e-01, 9.90277e-01, 6.24765e-01, 9.91348e-01, 9.95873e-01
Node: 59 (pos: 0.596): 6.03952e-01, 9.92318e-01, 9.78256e-01, 3.47027e-01, 9.80639e-01, 9.90739e-01
Node: 60 (pos: 0.606): 4.08010e-01, 9.86384e-01, 9.61671e-01, 1.52359e-01, 9.65840e-01, 9.83596e-01
Node: 61 (pos: 0.616): 2.46419e-01, 9.78807e-01, 9.40760e-01, 5.28726e-02, 9.47141e-01, 9.74487e-01
Node: 50 (pos: 0.505): 1.33049e-01, 9.69625e-01, 9.15819e-01, 1.45028e-02, 9.24777e-01, 9.63468e-01
-
Node: 51 (pos: 0.515): 2.46419e-01, 9.78807e-01, 9.40760e-01, 5.28726e-02, 9.47141e-01, 9.74487e-01
Node: 52 (pos: 0.525): 4.08010e-01, 9.86384e-01, 9.61671e-01, 1.52359e-01, 9.65840e-01, 9.83596e-01
Node: 53 (pos: 0.535): 6.03952e-01, 9.92318e-01, 9.78256e-01, 3.47027e-01, 9.80639e-01, 9.90739e-01
Node: 54 (pos: 0.545): 7.99223e-01, 9.96579e-01, 9.90277e-01, 6.24765e-01, 9.91348e-01, 9.95873e-01
Node: 55 (pos: 0.556): 9.45512e-01, 9.99144e-01, 9.97560e-01, 8.89056e-01, 9.97830e-01, 9.98967e-01
Node: 62 (pos: 0.626): 1.33049e-01, 9.69625e-01, 9.15819e-01, 1.45028e-02, 9.24777e-01, 9.63468e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.021345977723600126
Step 50, mean loss 0.009546231667028896
Step 75, mean loss 0.015785355242497755
Step 100, mean loss 0.02136031218490494
Step 125, mean loss 0.04604346917055317
Step 150, mean loss 0.034374594136463404
Step 175, mean loss 0.053530946570546675
Step 200, mean loss 0.06050422215680856
Step 225, mean loss 0.07489975254941142
Unrolled forward losses 1.2542785937555756
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00), 0.11105657781438928, [0.001822352036785179, 0.11909144403442928, 0.041779135598659194, 0.0008737166817006752, 0.04698058560005768, 0.0987046167293755]
Training Loss (progress: 0.08), 0.10181341508022118, [0.0018022545845065101, 0.11919152330118335, 0.04185636764478589, 0.0008763578134420769, 0.04702416052209054, 0.09876787418275519]
Training Loss (progress: 0.16), 0.10798472637510967, [0.001811890409631225, 0.11931758371036821, 0.04186227016246273, 0.0008748995593823613, 0.04705795375315084, 0.09879058203473774]
Training Loss (progress: 0.24), 0.10237594117578176, [0.001800971268246835, 0.11938248843813223, 0.04186327802444466, 0.0008732114503417762, 0.04710005796512661, 0.09884911609956175]
Training Loss (progress: 0.32), 0.10889754273873277, [0.0018091552289189333, 0.11944634013439964, 0.04194319239216394, 0.0008710567758307089, 0.04714966285559726, 0.09888577909624152]
Training Loss (progress: 0.40), 0.1026505293728678, [0.001829804502348731, 0.11951466732159012, 0.04198266715105765, 0.0008605329814355562, 0.047165243178928326, 0.09891713103148445]
Training Loss (progress: 0.48), 0.10993761170552772, [0.0018030174732600497, 0.11958614584119681, 0.04204233367117776, 0.0008696285104654316, 0.04720905361967308, 0.09895322834790628]
Training Loss (progress: 0.56), 0.09585036803635465, [0.0017904497158061036, 0.11974278358034345, 0.04207901912808299, 0.0008752935645919303, 0.04724710805503782, 0.09897621783552865]
Training Loss (progress: 0.64), 0.10983248736341761, [0.0018239079085509149, 0.11977667600644773, 0.04214968584081946, 0.0008795132068568952, 0.047268130973100446, 0.09900997730180881]
Training Loss (progress: 0.72), 0.09793869631587626, [0.0018169364482358073, 0.11989426000855334, 0.04221300918786795, 0.0008747000864667962, 0.04733303127573082, 0.09908765823953289]
Training Loss (progress: 0.80), 0.09455405958462401, [0.0018132682734984726, 0.11999940278912667, 0.04221522380253368, 0.000875673494624483, 0.04735247943291589, 0.09911608970901739]
Training Loss (progress: 0.88), 0.10546008450696838, [0.0018143224872420576, 0.12009227333678171, 0.04224404296891513, 0.0008755879282210724, 0.047400606572579475, 0.09911150256253347]
Training Loss (progress: 0.96), 0.1020383275666992, [0.0018115941589107597, 0.12019446288435448, 0.04229907114798427, 0.0008735102640246941, 0.0474480855466353, 0.0991366295873779]
Evaluation on validation dataset:
Step 25, mean loss 0.023869806244752594
Step 50, mean loss 0.013071888027370068
Step 75, mean loss 0.02201839616788616
Step 100, mean loss 0.02923692737300333
Step 125, mean loss 0.03128652127908838
Step 150, mean loss 0.033057062880834306
Step 175, mean loss 0.06070070435156632
Step 200, mean loss 0.17599813421855554
Step 225, mean loss 0.12666016511502567
Unrolled forward losses 1.5426867900405408
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.31308e-01, 9.69914e-01, 9.16864e-01, 1.50809e-02, 9.25509e-01, 9.63637e-01
Node: 01 (pos: 0.010): 2.44176e-01, 9.79010e-01, 9.41506e-01, 5.43275e-02, 9.47661e-01, 9.74605e-01
Node: 02 (pos: 0.020): 4.05630e-01, 9.86515e-01, 9.62159e-01, 1.55029e-01, 9.66180e-01, 9.83672e-01
Node: 03 (pos: 0.030): 6.01968e-01, 9.92392e-01, 9.78535e-01, 3.50435e-01, 9.80833e-01, 9.90783e-01
Node: 04 (pos: 0.040): 7.98054e-01, 9.96612e-01, 9.90402e-01, 6.27485e-01, 9.91436e-01, 9.95893e-01
Node: 05 (pos: 0.051): 9.45166e-01, 9.99152e-01, 9.97592e-01, 8.90022e-01, 9.97852e-01, 9.98972e-01
-
Node: 07 (pos: 0.071): 9.45166e-01, 9.99152e-01, 9.97592e-01, 8.90022e-01, 9.97852e-01, 9.98972e-01
Node: 08 (pos: 0.081): 7.98054e-01, 9.96612e-01, 9.90402e-01, 6.27485e-01, 9.91436e-01, 9.95893e-01
Node: 09 (pos: 0.091): 6.01968e-01, 9.92392e-01, 9.78535e-01, 3.50435e-01, 9.80833e-01, 9.90783e-01
Node: 10 (pos: 0.101): 4.05630e-01, 9.86515e-01, 9.62159e-01, 1.55029e-01, 9.66180e-01, 9.83672e-01
Node: 11 (pos: 0.111): 2.44176e-01, 9.79010e-01, 9.41506e-01, 5.43275e-02, 9.47661e-01, 9.74605e-01
Node: 12 (pos: 0.121): 1.31308e-01, 9.69914e-01, 9.16864e-01, 1.50809e-02, 9.25509e-01, 9.63637e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45166e-01, 9.99152e-01, 9.97592e-01, 8.90022e-01, 9.97852e-01, 9.98972e-01
Node: 58 (pos: 0.586): 7.98054e-01, 9.96612e-01, 9.90402e-01, 6.27485e-01, 9.91436e-01, 9.95893e-01
Node: 59 (pos: 0.596): 6.01968e-01, 9.92392e-01, 9.78535e-01, 3.50435e-01, 9.80833e-01, 9.90783e-01
Node: 60 (pos: 0.606): 4.05630e-01, 9.86515e-01, 9.62159e-01, 1.55029e-01, 9.66180e-01, 9.83672e-01
Node: 61 (pos: 0.616): 2.44176e-01, 9.79010e-01, 9.41506e-01, 5.43275e-02, 9.47661e-01, 9.74605e-01
Node: 50 (pos: 0.505): 1.31308e-01, 9.69914e-01, 9.16864e-01, 1.50809e-02, 9.25509e-01, 9.63637e-01
-
Node: 51 (pos: 0.515): 2.44176e-01, 9.79010e-01, 9.41506e-01, 5.43275e-02, 9.47661e-01, 9.74605e-01
Node: 52 (pos: 0.525): 4.05630e-01, 9.86515e-01, 9.62159e-01, 1.55029e-01, 9.66180e-01, 9.83672e-01
Node: 53 (pos: 0.535): 6.01968e-01, 9.92392e-01, 9.78535e-01, 3.50435e-01, 9.80833e-01, 9.90783e-01
Node: 54 (pos: 0.545): 7.98054e-01, 9.96612e-01, 9.90402e-01, 6.27485e-01, 9.91436e-01, 9.95893e-01
Node: 55 (pos: 0.556): 9.45166e-01, 9.99152e-01, 9.97592e-01, 8.90022e-01, 9.97852e-01, 9.98972e-01
Node: 62 (pos: 0.626): 1.31308e-01, 9.69914e-01, 9.16864e-01, 1.50809e-02, 9.25509e-01, 9.63637e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.021151773972026285
Step 50, mean loss 0.009679600529139914
Step 75, mean loss 0.015360490655420963
Step 100, mean loss 0.02098709091091084
Step 125, mean loss 0.041859717251978214
Step 150, mean loss 0.03293072354947585
Step 175, mean loss 0.053773201971115396
Step 200, mean loss 0.058232855996109216
Step 225, mean loss 0.07135074368387134
Unrolled forward losses 1.2253062454481713
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00), 0.09870726175557767, [0.0018329759915493796, 0.12022978396529223, 0.04233275055156164, 0.0008661422252913646, 0.047461898913136916, 0.09916888197939575]
Training Loss (progress: 0.08), 0.10974107626346942, [0.0018163616874507108, 0.12028533886447425, 0.04238219078976132, 0.0008729152619158572, 0.04746516383873613, 0.0991650270375496]
Training Loss (progress: 0.16), 0.10387651660559913, [0.0018208561305055203, 0.12036650506949856, 0.04245829394295823, 0.00086486555774097, 0.047505951205356334, 0.0991934715002135]
Training Loss (progress: 0.24), 0.10389219098114925, [0.001805211815939003, 0.12043367944401165, 0.04248392304160052, 0.0008709359960706707, 0.04754106101043184, 0.09921077076456633]
Training Loss (progress: 0.32), 0.10457399775157925, [0.0018130070616006958, 0.12045530199835609, 0.04243753666317865, 0.0008677184032215093, 0.047572536445428006, 0.09925089860283991]
Training Loss (progress: 0.40), 0.09760533899882234, [0.0018147373990402226, 0.12057387505069614, 0.042545215168415015, 0.0008706233146368804, 0.04761663728652277, 0.09931476415343554]
Training Loss (progress: 0.48), 0.10818250155204305, [0.0018056209975889981, 0.12065914103673771, 0.042613708177272405, 0.000869892445621894, 0.047661227158328955, 0.09934900893713279]
Training Loss (progress: 0.56), 0.1056551073556734, [0.0018086996086544556, 0.12074519029979525, 0.04256578224276298, 0.0008664757831888726, 0.047689064804128246, 0.0993987805239446]
Training Loss (progress: 0.64), 0.11301286067900349, [0.0017963023555180296, 0.12084514820875535, 0.04258079566487071, 0.0008803455787461306, 0.04772835431516868, 0.09944068895600487]
Training Loss (progress: 0.72), 0.10656840698014441, [0.0018168088911881181, 0.12089797154570052, 0.04263612053152583, 0.000871876779915258, 0.04774262098354903, 0.09946421109172397]
Training Loss (progress: 0.80), 0.10945144660577734, [0.0018126860794181751, 0.12102024026312423, 0.04268474975215302, 0.0008859461622002539, 0.04776665412922507, 0.099491718062585]
Training Loss (progress: 0.88), 0.10540743444730558, [0.0018089079921282214, 0.12104432896059301, 0.042729993901785654, 0.0008713133746451521, 0.047803929320069494, 0.09953019561232809]
Training Loss (progress: 0.96), 0.10690790800211489, [0.0018229411863346179, 0.12116305571898645, 0.04274432061905585, 0.0008866708352277269, 0.047820763108235645, 0.09954763583610456]
Evaluation on validation dataset:
Step 25, mean loss 0.02362006347216057
Step 50, mean loss 0.013236736243291873
Step 75, mean loss 0.02136078156875626
Step 100, mean loss 0.02906577136960837
Step 125, mean loss 0.030300839538049303
Step 150, mean loss 0.033263050973517505
Step 175, mean loss 0.060357025156576
Step 200, mean loss 0.17404783165682575
Step 225, mean loss 0.12717694066152424
Unrolled forward losses 1.5685651053062988
Unrolled forward base losses 2.565701273852575
Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00), 0.10484859342357077, [0.001820500940556059, 0.12116541866492198, 0.042795470333265806, 0.0008815693629021236, 0.04784713609856885, 0.09958004422397408]
Training Loss (progress: 0.08), 0.1008724694457336, [0.0018173655241413723, 0.12121444773122972, 0.042802162772384515, 0.0008719732977402195, 0.04788918170871469, 0.09958322934563295]
Training Loss (progress: 0.16), 0.10716870377962907, [0.0018000227169038799, 0.12129816141090431, 0.04278072115188252, 0.0008731638373296695, 0.04790537123848811, 0.09963152597696477]
Training Loss (progress: 0.24), 0.10032410738050294, [0.0017966959105024016, 0.1213335960890977, 0.04278633232491537, 0.0008782309541326747, 0.04794290835043116, 0.09966490554868639]
Training Loss (progress: 0.32), 0.10504204950828933, [0.0018080080729555245, 0.121419245455738, 0.04285357464432956, 0.0008875083315073277, 0.047961454877941435, 0.0997131098416935]
Training Loss (progress: 0.40), 0.09813177039722192, [0.0018084260565143957, 0.1215053866640671, 0.04290741521230613, 0.0008711806089922782, 0.04801356961883584, 0.09973265556651428]
Training Loss (progress: 0.48), 0.10986587045525227, [0.0018183591742123235, 0.12161866095851498, 0.042967344636862005, 0.0008829929941737857, 0.04804068945212921, 0.09975848141366461]
Training Loss (progress: 0.56), 0.1098252262150261, [0.0018037731957495423, 0.12171461303812281, 0.04298743220261759, 0.0008824691585555949, 0.04807371490934937, 0.09976613838789647]
Training Loss (progress: 0.64), 0.09486366033715424, [0.0018167894235483128, 0.12175933878565243, 0.0429441483781186, 0.000875528846673922, 0.04813026893544624, 0.09982521770777289]
Training Loss (progress: 0.72), 0.0950136901022929, [0.0018005269535922674, 0.12182701834774602, 0.04306007418231847, 0.0008809628502067228, 0.04814954156147484, 0.09984148178064613]
Training Loss (progress: 0.80), 0.10469448086322061, [0.0018048464010685936, 0.1219280274609576, 0.04313360955757908, 0.0008695273157949514, 0.048211453248524966, 0.09986954135334157]
Training Loss (progress: 0.88), 0.10179225543275951, [0.0018288666669756418, 0.12197423640463545, 0.043226350810726254, 0.0008680192339965591, 0.048249505812620226, 0.09990477050075601]
Training Loss (progress: 0.96), 0.09196314135175447, [0.0018064435155230939, 0.12206144059697559, 0.04317730774445252, 0.0008674452544397274, 0.04828161941339523, 0.09992918392029901]
Evaluation on validation dataset:
Step 25, mean loss 0.02450508497390865
Step 50, mean loss 0.013030263999319926
Step 75, mean loss 0.021870726314680462
Step 100, mean loss 0.028892005177947953
Step 125, mean loss 0.031360048123596385
Step 150, mean loss 0.033236965038091495
Step 175, mean loss 0.059796843543300796
Step 200, mean loss 0.1733508947748055
Step 225, mean loss 0.1246457103623727
Unrolled forward losses 1.5381045023487918
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 1.30862e-01, 9.70361e-01, 9.18549e-01, 1.57546e-02, 9.26744e-01, 9.63911e-01
Node: 01 (pos: 0.010): 2.43599e-01, 9.79323e-01, 9.42707e-01, 5.60015e-02, 9.48540e-01, 9.74798e-01
Node: 02 (pos: 0.020): 4.05017e-01, 9.86717e-01, 9.62944e-01, 1.58069e-01, 9.66753e-01, 9.83797e-01
Node: 03 (pos: 0.030): 6.01455e-01, 9.92506e-01, 9.78984e-01, 3.54284e-01, 9.81160e-01, 9.90853e-01
Node: 04 (pos: 0.040): 7.97753e-01, 9.96663e-01, 9.90604e-01, 6.30539e-01, 9.91583e-01, 9.95924e-01
Node: 05 (pos: 0.051): 9.45077e-01, 9.99165e-01, 9.97643e-01, 8.91103e-01, 9.97889e-01, 9.98980e-01
-
Node: 07 (pos: 0.071): 9.45077e-01, 9.99165e-01, 9.97643e-01, 8.91103e-01, 9.97889e-01, 9.98980e-01
Node: 08 (pos: 0.081): 7.97753e-01, 9.96663e-01, 9.90604e-01, 6.30539e-01, 9.91583e-01, 9.95924e-01
Node: 09 (pos: 0.091): 6.01455e-01, 9.92506e-01, 9.78984e-01, 3.54284e-01, 9.81160e-01, 9.90853e-01
Node: 10 (pos: 0.101): 4.05017e-01, 9.86717e-01, 9.62944e-01, 1.58069e-01, 9.66753e-01, 9.83797e-01
Node: 11 (pos: 0.111): 2.43599e-01, 9.79323e-01, 9.42707e-01, 5.60015e-02, 9.48540e-01, 9.74798e-01
Node: 12 (pos: 0.121): 1.30862e-01, 9.70361e-01, 9.18549e-01, 1.57546e-02, 9.26744e-01, 9.63911e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.45077e-01, 9.99165e-01, 9.97643e-01, 8.91103e-01, 9.97889e-01, 9.98980e-01
Node: 58 (pos: 0.586): 7.97753e-01, 9.96663e-01, 9.90604e-01, 6.30539e-01, 9.91583e-01, 9.95924e-01
Node: 59 (pos: 0.596): 6.01455e-01, 9.92506e-01, 9.78984e-01, 3.54284e-01, 9.81160e-01, 9.90853e-01
Node: 60 (pos: 0.606): 4.05017e-01, 9.86717e-01, 9.62944e-01, 1.58069e-01, 9.66753e-01, 9.83797e-01
Node: 61 (pos: 0.616): 2.43599e-01, 9.79323e-01, 9.42707e-01, 5.60015e-02, 9.48540e-01, 9.74798e-01
Node: 50 (pos: 0.505): 1.30862e-01, 9.70361e-01, 9.18549e-01, 1.57546e-02, 9.26744e-01, 9.63911e-01
-
Node: 51 (pos: 0.515): 2.43599e-01, 9.79323e-01, 9.42707e-01, 5.60015e-02, 9.48540e-01, 9.74798e-01
Node: 52 (pos: 0.525): 4.05017e-01, 9.86717e-01, 9.62944e-01, 1.58069e-01, 9.66753e-01, 9.83797e-01
Node: 53 (pos: 0.535): 6.01455e-01, 9.92506e-01, 9.78984e-01, 3.54284e-01, 9.81160e-01, 9.90853e-01
Node: 54 (pos: 0.545): 7.97753e-01, 9.96663e-01, 9.90604e-01, 6.30539e-01, 9.91583e-01, 9.95924e-01
Node: 55 (pos: 0.556): 9.45077e-01, 9.99165e-01, 9.97643e-01, 8.91103e-01, 9.97889e-01, 9.98980e-01
Node: 62 (pos: 0.626): 1.30862e-01, 9.70361e-01, 9.18549e-01, 1.57546e-02, 9.26744e-01, 9.63911e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.02134853361534945
Step 50, mean loss 0.009566796994417379
Step 75, mean loss 0.014962304381685356
Step 100, mean loss 0.020573652572630968
Step 125, mean loss 0.04366019853290063
Step 150, mean loss 0.03295123576529488
Step 175, mean loss 0.05515381440965649
Step 200, mean loss 0.059146502432817334
Step 225, mean loss 0.07293796841692637
Unrolled forward losses 1.2087633144969478
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time4301436.tar

Test loss: 1.2087633144969478
