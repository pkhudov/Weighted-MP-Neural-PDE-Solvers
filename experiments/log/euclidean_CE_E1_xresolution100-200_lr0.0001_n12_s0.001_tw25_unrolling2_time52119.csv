Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar
Number of parameters: 1031657.0
Saved initial model at models/init52119.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
Node: 14 (pos: 0.141): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 15 (pos: 0.152): 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07
Node: 00 (pos: 0.000): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 01 (pos: 0.010): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 02 (pos: 0.020): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
-
Node: 03 (pos: 0.030): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 05 (pos: 0.051): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 06 (pos: 0.061): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 07 (pos: 0.071): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 08 (pos: 0.081): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 09 (pos: 0.091): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 36 (pos: 0.364): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
Node: 37 (pos: 0.374): 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03, 6.74139e-03
Node: 38 (pos: 0.384): 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03
Node: 39 (pos: 0.394): 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04
Node: 40 (pos: 0.404): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
-
Node: 41 (pos: 0.414): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 42 (pos: 0.424): 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07, 4.16084e-07
Node: 19 (pos: 0.192): 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06, 4.34850e-06
Node: 20 (pos: 0.202): 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05, 3.70575e-05
Node: 21 (pos: 0.212): 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04, 2.57507e-04
Node: 22 (pos: 0.222): 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03, 1.45908e-03
=========================================================================================================
Training Loss (progress: 0.00), 1.341538503378043, [0.0018794271415789388, 0.009558053116737167, 0.009600929822833795, 0.010341937495607426, 0.00981462488158436, 0.010027479750138076], [1.0093705807528897, 1.0142385857846286, 1.0134208160114138, 1.0134621061817897, 1.0131935721677714, 1.0137110753148257]
Training Loss (progress: 0.08), 0.2391936588066549, [0.00039349615745697147, 0.0013593098239564488, 0.0015381021352671744, 0.010761289646774455, 0.013087673322479674, 0.008852259252744882], [1.0493901639165464, 1.0371815123163162, 1.0473747427514901, 1.0322530926417524, 1.0413527981861794, 1.040825907660604]
Training Loss (progress: 0.16), 0.19615145219935057, [0.00036383078466311865, 0.0013640745669605502, 0.0015732214171859384, 0.012184433307554643, 0.015315060299869234, 0.009044262713932431], [1.0494728406968785, 1.0461343021980452, 1.0685884282150542, 1.0374721523082535, 1.0476707766272444, 1.0494044930891757]
Training Loss (progress: 0.24), 0.17468400706921905, [0.0003648715388975045, 0.0015696941978814982, 0.0016025143357114133, 0.01306901441785612, 0.01820768787029793, 0.009182545530175069], [1.0486599167988693, 1.0540765273853347, 1.0856858491448889, 1.041072861855178, 1.0522894363589748, 1.0561802023516123]
Training Loss (progress: 0.32), 0.16290796754459416, [0.0004644766781591351, 0.0014941840059041636, 0.0015757701321670694, 0.014066895243385545, 0.020471787989634557, 0.008927454019178013], [1.046670345618888, 1.0597880267701814, 1.0976202252659488, 1.0447869114699349, 1.055351721767173, 1.0628212528962715]
Training Loss (progress: 0.40), 0.15108376486517874, [0.0004475108536693235, 0.0013886547973576927, 0.0016297468919201743, 0.014448652041466859, 0.023282865685093676, 0.008798193464641653], [1.0448848988360817, 1.0673992090634623, 1.1088487124145172, 1.0474932925169202, 1.0585844335996857, 1.0687605825010271]
Training Loss (progress: 0.48), 0.12634793262331237, [0.00036551457141164895, 0.0013440199438841248, 0.0014105394117538716, 0.014743589449706957, 0.026103554867134043, 0.008523530299655063], [1.0442383330392975, 1.0738260256553607, 1.1199986282291083, 1.0501038939191858, 1.0623265883649358, 1.0740388763088642]
Training Loss (progress: 0.56), 0.12241679325630042, [0.000425145217663819, 0.0013297389168991452, 0.0014524249545323131, 0.015568855893180187, 0.02788444679369672, 0.008771841102272658], [1.040977645958365, 1.0790487752290208, 1.12889133606315, 1.0525535020710033, 1.0647696089319207, 1.0785182922493526]
Training Loss (progress: 0.64), 0.11625139741284767, [0.0004634399698322403, 0.0012984471102553082, 0.0014190552454430452, 0.01516268944451559, 0.030305235531141856, 0.008370967527936327], [1.0389893641201953, 1.0842388093274893, 1.1375079463099933, 1.0552229056590556, 1.0677815009269855, 1.0826588705125597]
Training Loss (progress: 0.72), 0.11078191905712315, [0.0003985271144854029, 0.0013665159272120003, 0.0014419827373613294, 0.01563592520267431, 0.03219753212360817, 0.008514560050368465], [1.0359709731166165, 1.0895183189086506, 1.1445891099797139, 1.0579499207922656, 1.0707871533500368, 1.0879681137365]
Training Loss (progress: 0.80), 0.10787845768487142, [0.000419225554326345, 0.001286567578228126, 0.0015046811943476663, 0.015525203615871471, 0.03368810646796678, 0.008068675157119983], [1.0336353175026556, 1.093732170228749, 1.1526866783464975, 1.0603227246782085, 1.073636672538122, 1.0920031593126236]
Training Loss (progress: 0.88), 0.10343372085726535, [0.0003608274505749826, 0.0012728881659889602, 0.001390036304560441, 0.01590083910643367, 0.03472654138150942, 0.008104622722404375], [1.0318667622420603, 1.0976752114656825, 1.1577814237522732, 1.0629737862846964, 1.0758458287917643, 1.0964612585433953]
Training Loss (progress: 0.96), 0.10084289041019448, [0.0004567406045058903, 0.0011660362499942099, 0.0014586367273569903, 0.01583892857933932, 0.03635221716916791, 0.00789010174059659], [1.0293137873861242, 1.101984194981612, 1.1642891469811807, 1.0658067085506366, 1.078790006011943, 1.1008591624963586]
Evaluation on validation dataset:
Step 25, mean loss 0.07908922303249455
Step 50, mean loss 0.08774002189901972
Step 75, mean loss 0.10163165759400772
Step 100, mean loss 0.2561308206429217
Step 125, mean loss 0.21077088554462847
Step 150, mean loss 0.1782337258007396
Step 175, mean loss 0.32463723542907746
Step 200, mean loss 0.5890601857468287
Step 225, mean loss 0.39164507269809423
Unrolled forward losses 10.371827884622704
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 7.72496e-11, 3.75529e-04, 1.39237e-03, 5.54243e-01, 8.13165e-01, 2.97499e-01
Node: 14 (pos: 0.141): 5.77884e-13, 7.01850e-05, 3.38775e-04, 4.83046e-01, 7.66199e-01, 2.25940e-01
Node: 15 (pos: 0.152): 2.71209e-15, 1.11808e-05, 7.20452e-05, 4.15517e-01, 7.17866e-01, 1.67155e-01
Node: 00 (pos: 0.000): 2.46672e-02, 3.07779e-01, 3.97316e-01, 9.60616e-01, 1.03164e+00, 8.94247e-01
Node: 01 (pos: 0.010): 1.26126e-01, 5.38317e-01, 6.36430e-01, 1.00567e+00, 1.05230e+00, 9.80141e-01
Node: 02 (pos: 0.020): 4.04583e-01, 8.02540e-01, 8.91054e-01, 1.03913e+00, 1.06731e+00, 1.04650e+00
-
Node: 03 (pos: 0.030): 8.14199e-01, 1.01982e+00, 1.09043e+00, 1.05974e+00, 1.07642e+00, 1.08845e+00
Node: 05 (pos: 0.051): 8.14199e-01, 1.01982e+00, 1.09043e+00, 1.05974e+00, 1.07642e+00, 1.08845e+00
Node: 06 (pos: 0.061): 4.04583e-01, 8.02540e-01, 8.91054e-01, 1.03913e+00, 1.06731e+00, 1.04650e+00
Node: 07 (pos: 0.071): 1.26126e-01, 5.38317e-01, 6.36430e-01, 1.00567e+00, 1.05230e+00, 9.80141e-01
Node: 08 (pos: 0.081): 2.46672e-02, 3.07779e-01, 3.97316e-01, 9.60616e-01, 1.03164e+00, 8.94247e-01
Node: 09 (pos: 0.091): 3.02658e-03, 1.49992e-01, 2.16800e-01, 9.05646e-01, 1.00567e+00, 7.94779e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 3.02658e-03, 1.49992e-01, 2.16800e-01, 9.05646e-01, 1.00567e+00, 7.94779e-01
Node: 36 (pos: 0.364): 2.32972e-04, 6.23054e-02, 1.03400e-01, 8.42715e-01, 9.74812e-01, 6.88105e-01
Node: 37 (pos: 0.374): 1.12506e-05, 2.20603e-02, 4.31045e-02, 7.73956e-01, 9.39564e-01, 5.80341e-01
Node: 38 (pos: 0.384): 3.40850e-07, 6.65774e-03, 1.57059e-02, 7.01559e-01, 9.00474e-01, 4.76794e-01
Node: 39 (pos: 0.394): 6.47844e-09, 1.71266e-03, 5.00195e-03, 6.27662e-01, 8.58135e-01, 3.81591e-01
Node: 40 (pos: 0.404): 7.72496e-11, 3.75529e-04, 1.39237e-03, 5.54243e-01, 8.13165e-01, 2.97499e-01
-
Node: 41 (pos: 0.414): 5.77884e-13, 7.01850e-05, 3.38775e-04, 4.83046e-01, 7.66199e-01, 2.25940e-01
Node: 42 (pos: 0.424): 2.71209e-15, 1.11808e-05, 7.20452e-05, 4.15517e-01, 7.17866e-01, 1.67155e-01
Node: 19 (pos: 0.192): 5.77884e-13, 7.01850e-05, 3.38775e-04, 4.83046e-01, 7.66199e-01, 2.25940e-01
Node: 20 (pos: 0.202): 7.72496e-11, 3.75529e-04, 1.39237e-03, 5.54243e-01, 8.13165e-01, 2.97499e-01
Node: 21 (pos: 0.212): 6.47844e-09, 1.71266e-03, 5.00195e-03, 6.27662e-01, 8.58135e-01, 3.81591e-01
Node: 22 (pos: 0.222): 3.40850e-07, 6.65774e-03, 1.57059e-02, 7.01559e-01, 9.00474e-01, 4.76794e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.06762844359964691
Step 50, mean loss 0.0632606743309271
Step 75, mean loss 0.09067430267487
Step 100, mean loss 0.12051968704215328
Step 125, mean loss 0.32413549945319053
Step 150, mean loss 0.190174233130452
Step 175, mean loss 0.2808826680346914
Step 200, mean loss 0.3106025040651217
Step 225, mean loss 0.2866274967162752
Unrolled forward losses 9.526677770764987
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.23393307574147135, [0.0003373963383010215, 0.001128492722638756, 0.0013184813818509981, 0.016103650774743582, 0.03632489958635164, 0.007969242105062207], [1.0274959136915667, 1.1043059155236365, 1.1663318586104336, 1.0671799107967688, 1.079603476732392, 1.1032100264498996]
Training Loss (progress: 0.08), 0.22387986137459343, [0.0005077445848348073, 0.0015002146602497923, 0.001548546828477091, 0.018553820172466155, 0.03904544987062194, 0.008216202087677843], [1.0246058113117558, 1.1074326260372653, 1.171671889253912, 1.0708186880681931, 1.0822078299439049, 1.1074198556127093]
Training Loss (progress: 0.16), 0.19080679781297463, [0.000453482960127797, 0.0013114388991063784, 0.0015282274105660745, 0.019553595172436218, 0.04227618862058459, 0.007854183068079237], [1.022324211456963, 1.1097356780737682, 1.1747667315792893, 1.074801438741334, 1.0854193860931831, 1.1121082951325292]
Training Loss (progress: 0.24), 0.20832085128669434, [0.0005236258569078509, 0.001565836416356824, 0.001617755800185585, 0.02110236021957878, 0.045145573648967825, 0.007862091989845877], [1.019108659133822, 1.1120551028893204, 1.1794839698722321, 1.078212442320069, 1.0880108797871753, 1.116235060628965]
Training Loss (progress: 0.32), 0.20223157949911902, [0.0005778320490610953, 0.0013946215261048769, 0.0014907131088874302, 0.022098121256698442, 0.0471349005593351, 0.00802758710447984], [1.017869029724846, 1.1125225968070274, 1.1820817289539236, 1.0808428168787745, 1.0893379094285875, 1.1203899540492883]
Training Loss (progress: 0.40), 0.1881678850885361, [0.0005178890999809767, 0.0014564641295265566, 0.001653002440801413, 0.02275709805411026, 0.04864443310769455, 0.007760543570871505], [1.0151463619437822, 1.114118993393292, 1.1863519505716027, 1.083395274447977, 1.090359841317315, 1.1239170923454747]
Training Loss (progress: 0.48), 0.21249535228559235, [0.0005583710230240268, 0.0015699885810269581, 0.0016483137943051748, 0.02392557329597865, 0.05159909491361206, 0.007694802568498919], [1.0134612318249212, 1.1160177331934777, 1.188979365243036, 1.08789323959924, 1.09288796008469, 1.128101609491066]
Training Loss (progress: 0.56), 0.1905259952970536, [0.0005262566811739248, 0.001507844712583518, 0.001692958088931103, 0.02457608805264411, 0.053358809355561, 0.007601533245690234], [1.0117967538109511, 1.1181389162774447, 1.1920809522227436, 1.0896579560458335, 1.094235663374186, 1.1319795183932695]
Training Loss (progress: 0.64), 0.17279869240754328, [0.000562255568409326, 0.0015071236738146674, 0.001635631114433356, 0.02574758907771889, 0.0546453454724237, 0.007495117290329736], [1.0101121676018063, 1.1190035591689345, 1.1941923694724497, 1.0936644265856534, 1.095223550721095, 1.1355997786496634]
Training Loss (progress: 0.72), 0.15866009916477256, [0.0005346377295727716, 0.0015377819438156155, 0.0016158363406344489, 0.02592567986005528, 0.05635493589149858, 0.007224122152494372], [1.007955430328083, 1.120368085313723, 1.196980617842329, 1.0959533551273748, 1.0969539180601098, 1.1397988279527533]
Training Loss (progress: 0.80), 0.162336617234415, [0.0005315520911920556, 0.001740418495597034, 0.0016314797986131822, 0.028652899707831782, 0.0587058113634472, 0.007424721970339906], [1.0049274629521827, 1.122592959253925, 1.1998983856222987, 1.1002090490772016, 1.0990389728426773, 1.1432383380701296]
Training Loss (progress: 0.88), 0.15087544511370646, [0.0005493080491559213, 0.0017208750989185877, 0.0016577031289468868, 0.028605787609847756, 0.05982352564343055, 0.0074982022647230705], [1.0041245340955491, 1.1228928440449746, 1.202068918340835, 1.1026860118169397, 1.100064477903284, 1.1474451122638292]
Training Loss (progress: 0.96), 0.1653605805218955, [0.0004460911725956097, 0.0015534106153497246, 0.0017331315688715434, 0.02844654726260951, 0.06074586146929204, 0.007144149779171511], [1.0020586123142974, 1.1244538171642198, 1.2039918836767511, 1.1058617440459722, 1.1010988500593348, 1.1513489650257762]
Evaluation on validation dataset:
Step 25, mean loss 0.0968601035440444
Step 50, mean loss 0.07372650529575098
Step 75, mean loss 0.12588497435945495
Step 100, mean loss 0.15068759735706283
Step 125, mean loss 0.17701731008470709
Step 150, mean loss 0.14600136470165767
Step 175, mean loss 0.23229843777964038
Step 200, mean loss 0.47001652989952186
Step 225, mean loss 0.33676729342980266
Unrolled forward losses 5.2807702498180715
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 1.54709e-08, 2.93496e-03, 2.87113e-03, 7.76458e-01, 9.33300e-01, 2.66885e-01
Node: 14 (pos: 0.141): 3.54119e-10, 8.41428e-04, 8.07718e-04, 7.20721e-01, 9.01361e-01, 1.96281e-01
Node: 15 (pos: 0.152): 5.65661e-12, 2.14169e-04, 2.01377e-04, 6.64256e-01, 8.67632e-01, 1.40192e-01
Node: 00 (pos: 0.000): 5.63617e-02, 4.34454e-01, 4.58381e-01, 1.04598e+00, 1.07278e+00, 9.12237e-01
Node: 01 (pos: 0.010): 1.98506e-01, 6.58879e-01, 6.99560e-01, 1.07227e+00, 1.08531e+00, 1.01062e+00
Node: 02 (pos: 0.020): 4.87906e-01, 8.87142e-01, 9.46163e-01, 1.09146e+00, 1.09434e+00, 1.08733e+00
-
Node: 03 (pos: 0.030): 8.36902e-01, 1.06049e+00, 1.13410e+00, 1.10314e+00, 1.09980e+00, 1.13613e+00
Node: 05 (pos: 0.051): 8.36902e-01, 1.06049e+00, 1.13410e+00, 1.10314e+00, 1.09980e+00, 1.13613e+00
Node: 06 (pos: 0.061): 4.87906e-01, 8.87142e-01, 9.46163e-01, 1.09146e+00, 1.09434e+00, 1.08733e+00
Node: 07 (pos: 0.071): 1.98506e-01, 6.58879e-01, 6.99560e-01, 1.07227e+00, 1.08531e+00, 1.01062e+00
Node: 08 (pos: 0.081): 5.63617e-02, 4.34454e-01, 4.58381e-01, 1.04598e+00, 1.07278e+00, 9.12237e-01
Node: 09 (pos: 0.091): 1.11678e-02, 2.54336e-01, 2.66178e-01, 1.01311e+00, 1.05689e+00, 7.99680e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.11678e-02, 2.54336e-01, 2.66178e-01, 1.01311e+00, 1.05689e+00, 7.99680e-01
Node: 36 (pos: 0.364): 1.54429e-03, 1.32189e-01, 1.36981e-01, 9.74342e-01, 1.03779e+00, 6.80795e-01
Node: 37 (pos: 0.374): 1.49027e-04, 6.09972e-02, 6.24733e-02, 9.30432e-01, 1.01566e+00, 5.62868e-01
Node: 38 (pos: 0.384): 1.00363e-05, 2.49890e-02, 2.52505e-02, 8.82220e-01, 9.90709e-01, 4.51948e-01
Node: 39 (pos: 0.394): 4.71691e-07, 9.08893e-03, 9.04461e-03, 8.30592e-01, 9.63172e-01, 3.52420e-01
Node: 40 (pos: 0.404): 1.54709e-08, 2.93496e-03, 2.87113e-03, 7.76458e-01, 9.33300e-01, 2.66885e-01
-
Node: 41 (pos: 0.414): 3.54119e-10, 8.41428e-04, 8.07718e-04, 7.20721e-01, 9.01361e-01, 1.96281e-01
Node: 42 (pos: 0.424): 5.65661e-12, 2.14169e-04, 2.01377e-04, 6.64256e-01, 8.67632e-01, 1.40192e-01
Node: 19 (pos: 0.192): 3.54119e-10, 8.41428e-04, 8.07718e-04, 7.20721e-01, 9.01361e-01, 1.96281e-01
Node: 20 (pos: 0.202): 1.54709e-08, 2.93496e-03, 2.87113e-03, 7.76458e-01, 9.33300e-01, 2.66885e-01
Node: 21 (pos: 0.212): 4.71691e-07, 9.08893e-03, 9.04461e-03, 8.30592e-01, 9.63172e-01, 3.52420e-01
Node: 22 (pos: 0.222): 1.00363e-05, 2.49890e-02, 2.52505e-02, 8.82220e-01, 9.90709e-01, 4.51948e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.08023459860260766
Step 50, mean loss 0.06018924498398983
Step 75, mean loss 0.08963050106976428
Step 100, mean loss 0.1099519763554371
Step 125, mean loss 0.15425100234353642
Step 150, mean loss 0.14074842598858456
Step 175, mean loss 0.2512793460212789
Step 200, mean loss 0.1886671017373985
Step 225, mean loss 0.3034958593770197
Unrolled forward losses 3.8909249143707685
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.24469188340637868, [0.0005725362118047353, 0.0014860440932179663, 0.0016680810538857271, 0.02897659214339532, 0.06132703162125061, 0.007017866026352259], [1.0017647415494488, 1.12526169306677, 1.2049153783795952, 1.1072234634971776, 1.1015784151784478, 1.153054693144619]
Training Loss (progress: 0.08), 0.25021545283941304, [0.0005494435238101222, 0.0016179028533349858, 0.0016620631934033724, 0.02968698838381673, 0.062286235631962485, 0.0071976463121801275], [1.0009995800310774, 1.1251824870957745, 1.204599525039916, 1.1090887525241082, 1.1025961476076607, 1.1549551536137925]
Training Loss (progress: 0.16), 0.21067338293077506, [0.0005844009299476282, 0.0015733136450879648, 0.0016719491718114204, 0.030507496468920016, 0.06286827677119869, 0.0073977779874118774], [1.0008420841051497, 1.1257913000240896, 1.2049475898787336, 1.1110199149108266, 1.1035955331502894, 1.1575817393029602]
Training Loss (progress: 0.24), 0.2264081824413601, [0.000572317693853968, 0.0015299859882898054, 0.001711481250273493, 0.03138611392172639, 0.0638404593590327, 0.007349791349552302], [0.9999876117066399, 1.1253636280330033, 1.2047580667467666, 1.1124412613683845, 1.1046365889251324, 1.1591360119086267]
Training Loss (progress: 0.32), 0.22680663515408356, [0.0005238622753839985, 0.0016492896963845901, 0.0016630207836850085, 0.03171634558921266, 0.06551218196248093, 0.007408201432513346], [0.9991953725846193, 1.126061303550201, 1.205194026642505, 1.1137897662806902, 1.106073480079248, 1.160373433503357]
Training Loss (progress: 0.40), 0.22299442396946256, [0.0005477007326555711, 0.0016400283562932277, 0.001658322107070697, 0.031960712039650487, 0.06561141645660534, 0.007481827757557126], [0.999029974455822, 1.1260841354475937, 1.2051385273514386, 1.1151447775341645, 1.1065131168832405, 1.1623248454123414]
Training Loss (progress: 0.48), 0.21520334111131736, [0.0005542332586547024, 0.0015913481854131462, 0.0016837276359385712, 0.032622584784642694, 0.06660601389617186, 0.007414145508014058], [0.997986322322639, 1.1259468282582332, 1.2054957291127062, 1.1163835327722906, 1.1075974982684313, 1.1637855783963287]
Training Loss (progress: 0.56), 0.21596317594889383, [0.0005455935349870788, 0.001607910568145548, 0.0017091657067593608, 0.032908434539756296, 0.0671038405691999, 0.00743490098755956], [0.9972338383433723, 1.1261366569209887, 1.2053188525448943, 1.1174873898372693, 1.108314174122139, 1.1652809823042323]
Training Loss (progress: 0.64), 0.2107512775454802, [0.0005482417247139033, 0.0015983988522755006, 0.0016291399535185752, 0.03323144047175217, 0.06761937631976557, 0.007429619415814439], [0.9969695823212762, 1.1264415140270057, 1.2051454034600309, 1.1186159856608802, 1.1090599129860084, 1.166965036899886]
Training Loss (progress: 0.72), 0.18314365735109686, [0.0006031127651540623, 0.0015808446941066871, 0.0016086354358523728, 0.03373020145079643, 0.06866372566748222, 0.007763047546127323], [0.9962090860869024, 1.126814730860122, 1.2054935294118203, 1.1202231264370295, 1.110233763972595, 1.1687882092445352]
Training Loss (progress: 0.80), 0.21517215044025076, [0.0005435566718645527, 0.0015759208706533263, 0.0016693223339037389, 0.03451442966473337, 0.06912190912552092, 0.0074929133069611825], [0.9956591403571549, 1.1262344083977716, 1.2057198871109025, 1.1215970404196995, 1.1105406049808222, 1.1699617766090296]
Training Loss (progress: 0.88), 0.22108869106872187, [0.0005746844856691579, 0.0016095250760508367, 0.0016879846024532277, 0.03447170799669852, 0.07024420569049668, 0.007628755597368415], [0.9946981173506154, 1.1267724118101332, 1.20556833247726, 1.1224323570389316, 1.1113417634538356, 1.171396690763475]
Training Loss (progress: 0.96), 0.21038176179190388, [0.0005834826126569555, 0.001731830490101056, 0.0017193877410777007, 0.035006567733194965, 0.07068620090426417, 0.007679629462100322], [0.9941598996344612, 1.1268464022309181, 1.2059182354116136, 1.1235478263064562, 1.1122371754229163, 1.1734424107237924]
Evaluation on validation dataset:
Step 25, mean loss 0.09737769322688795
Step 50, mean loss 0.0452462781236275
Step 75, mean loss 0.06939182947359436
Step 100, mean loss 0.06410889967674434
Step 125, mean loss 0.07383703334294184
Step 150, mean loss 0.07465646552755312
Step 175, mean loss 0.15300024547679772
Step 200, mean loss 0.27998967283347376
Step 225, mean loss 0.23774169691853742
Unrolled forward losses 2.9411106289877194
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 5.00026e-09, 2.45017e-03, 1.82702e-03, 8.39922e-01, 9.63652e-01, 3.01052e-01
Node: 14 (pos: 0.141): 9.04388e-11, 6.76198e-04, 4.67332e-04, 7.90099e-01, 9.35010e-01, 2.26223e-01
Node: 15 (pos: 0.152): 1.11623e-12, 1.65083e-04, 1.04982e-04, 7.38915e-01, 9.04616e-01, 1.65430e-01
Node: 00 (pos: 0.000): 4.67243e-02, 4.22360e-01, 4.26795e-01, 1.07268e+00, 1.08727e+00, 9.44194e-01
Node: 01 (pos: 0.010): 1.78001e-01, 6.48719e-01, 6.72345e-01, 1.09477e+00, 1.09826e+00, 1.03855e+00
Node: 02 (pos: 0.020): 4.62738e-01, 8.81416e-01, 9.30193e-01, 1.11083e+00, 1.10617e+00, 1.11168e+00
-
Node: 03 (pos: 0.030): 8.20887e-01, 1.05939e+00, 1.13022e+00, 1.12057e+00, 1.11095e+00, 1.15800e+00
Node: 05 (pos: 0.051): 8.20887e-01, 1.05939e+00, 1.13022e+00, 1.12057e+00, 1.11095e+00, 1.15800e+00
Node: 06 (pos: 0.061): 4.62738e-01, 8.81416e-01, 9.30193e-01, 1.11083e+00, 1.10617e+00, 1.11168e+00
Node: 07 (pos: 0.071): 1.78001e-01, 6.48719e-01, 6.72345e-01, 1.09477e+00, 1.09826e+00, 1.03855e+00
Node: 08 (pos: 0.081): 4.67243e-02, 4.22360e-01, 4.26795e-01, 1.07268e+00, 1.08727e+00, 9.44194e-01
Node: 09 (pos: 0.091): 8.36947e-03, 2.43253e-01, 2.37933e-01, 1.04493e+00, 1.07330e+00, 8.35359e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 8.36947e-03, 2.43253e-01, 2.37933e-01, 1.04493e+00, 1.07330e+00, 8.35359e-01
Node: 36 (pos: 0.364): 1.02303e-03, 1.23933e-01, 1.16492e-01, 1.01199e+00, 1.05647e+00, 7.19227e-01
Node: 37 (pos: 0.374): 8.53320e-05, 5.58551e-02, 5.00895e-02, 9.74398e-01, 1.03692e+00, 6.02614e-01
Node: 38 (pos: 0.384): 4.85703e-06, 2.22685e-02, 1.89149e-02, 9.32753e-01, 1.01481e+00, 4.91352e-01
Node: 39 (pos: 0.394): 1.88653e-07, 7.85358e-03, 6.27293e-03, 8.87703e-01, 9.90321e-01, 3.89877e-01
Node: 40 (pos: 0.404): 5.00026e-09, 2.45017e-03, 1.82702e-03, 8.39922e-01, 9.63652e-01, 3.01052e-01
-
Node: 41 (pos: 0.414): 9.04388e-11, 6.76198e-04, 4.67332e-04, 7.90099e-01, 9.35010e-01, 2.26223e-01
Node: 42 (pos: 0.424): 1.11623e-12, 1.65083e-04, 1.04982e-04, 7.38915e-01, 9.04616e-01, 1.65430e-01
Node: 19 (pos: 0.192): 9.04388e-11, 6.76198e-04, 4.67332e-04, 7.90099e-01, 9.35010e-01, 2.26223e-01
Node: 20 (pos: 0.202): 5.00026e-09, 2.45017e-03, 1.82702e-03, 8.39922e-01, 9.63652e-01, 3.01052e-01
Node: 21 (pos: 0.212): 1.88653e-07, 7.85358e-03, 6.27293e-03, 8.87703e-01, 9.90321e-01, 3.89877e-01
Node: 22 (pos: 0.222): 4.85703e-06, 2.22685e-02, 1.89149e-02, 9.32753e-01, 1.01481e+00, 4.91352e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.07954449786384103
Step 50, mean loss 0.0351387238159917
Step 75, mean loss 0.04984440471684168
Step 100, mean loss 0.055383390501982634
Step 125, mean loss 0.06822866157039612
Step 150, mean loss 0.07895194192922333
Step 175, mean loss 0.1294533322413513
Step 200, mean loss 0.1583474670987088
Step 225, mean loss 0.13925664741372767
Unrolled forward losses 2.590908068358224
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.19820013943413475, [0.000559699889119455, 0.0016155236999642088, 0.0016533182197524619, 0.0349793839765878, 0.07125324030930304, 0.007462563229109259], [0.9938876034511346, 1.1263192017018688, 1.2060937371812332, 1.1238287074824815, 1.1126757385638093, 1.173788050395576]
Training Loss (progress: 0.08), 0.2208770783471282, [0.0005843688380682651, 0.0016124849701391207, 0.001646234170981795, 0.03578576668948805, 0.07196355299748262, 0.007561585034350023], [0.9933416711652002, 1.1263665366777886, 1.206436815484156, 1.1253215007632562, 1.1134812199150264, 1.1756655857399685]
Training Loss (progress: 0.16), 0.18441862536907372, [0.0005541844464868243, 0.0016260362399816899, 0.0017003262942101756, 0.03604664613929239, 0.07311322528833851, 0.00733002922740255], [0.9926890843667694, 1.1265919555759574, 1.206107599374546, 1.1262497148707364, 1.114478940704238, 1.1768427035311937]
Training Loss (progress: 0.24), 0.20700345177618953, [0.000578629534361731, 0.0016535283773600313, 0.0016252747462065452, 0.03633386968054772, 0.07396061879871331, 0.007590149254333605], [0.9913499833513449, 1.1262999682279538, 1.2061092282915313, 1.1269639159480012, 1.1154250048169037, 1.1786466913086142]
Training Loss (progress: 0.32), 0.20497496469178875, [0.0005476056223263727, 0.0016320502677730794, 0.0017178101976434686, 0.036105706385612765, 0.07492340070094626, 0.007477867635677798], [0.9910142321668982, 1.1267188539009767, 1.2060519112711177, 1.127296752434561, 1.116247516100309, 1.1799817360396256]
Training Loss (progress: 0.40), 0.19931793130042744, [0.0005719862167751889, 0.0016300867706474913, 0.0017578285501563557, 0.036836433267051175, 0.07577779288269489, 0.0075562699573150545], [0.9905096935694334, 1.1270090681058529, 1.2061696670359199, 1.129008355983241, 1.117219353572017, 1.1811257841428813]
Training Loss (progress: 0.48), 0.20582586962859006, [0.0006390259693594083, 0.001683655414316085, 0.0016509201276813535, 0.037526712192299964, 0.07612295885030324, 0.007584591343313324], [0.9899367432788289, 1.126783274853586, 1.2060798109800688, 1.1300028690633874, 1.117798287966646, 1.182228523620351]
Training Loss (progress: 0.56), 0.1818685591076758, [0.0005740076792171362, 0.0016717901971765108, 0.0016713498772818448, 0.03800044140242449, 0.0767777917108016, 0.007579786895703989], [0.9894950865657653, 1.1265391236455098, 1.2064103035511855, 1.1310759715615264, 1.1186522532203829, 1.1837429491329723]
Training Loss (progress: 0.64), 0.20106148904705898, [0.0005940603095453108, 0.0017400811786100817, 0.0017293235591639994, 0.03859001374736976, 0.07791435961699968, 0.007739194805626128], [0.9889255089393532, 1.1264129509888203, 1.206073985700103, 1.1321306739924046, 1.119226021020149, 1.1850509630799264]
Training Loss (progress: 0.72), 0.18802404679638218, [0.0005776477724779879, 0.0017005982471005312, 0.0016678323888581778, 0.03849529280487508, 0.07828646770940788, 0.007520955498016215], [0.9884543252878453, 1.1265778010625285, 1.2064261879755431, 1.1325732144500038, 1.1195832379970352, 1.186146260070438]
Training Loss (progress: 0.80), 0.17013542668008705, [0.0005345498238579983, 0.0016752095335420035, 0.0016548779300384038, 0.03857333177344658, 0.07961117791797435, 0.007538804400613196], [0.9881902155438304, 1.1259681939252006, 1.2064228348871906, 1.1333318161028652, 1.1210842403220298, 1.1878256500728879]
Training Loss (progress: 0.88), 0.1765863847363345, [0.0005893594158712077, 0.001692928224349998, 0.0017000193860387588, 0.03882706273002724, 0.08016698301240971, 0.007573430796453718], [0.9873409727070696, 1.125784681108571, 1.2062621051952516, 1.1339888895576073, 1.1214132802328154, 1.1889163960076934]
Training Loss (progress: 0.96), 0.17979523824592675, [0.0005728917758046271, 0.0016710378329196293, 0.0016595865114185007, 0.03961095334994633, 0.08140338767588334, 0.007633616447701075], [0.9868564489659889, 1.1259522097764563, 1.2064686825499948, 1.1354180342523668, 1.122629723867581, 1.1905354015453515]
Evaluation on validation dataset:
Step 25, mean loss 0.07419542311925614
Step 50, mean loss 0.03880570291839353
Step 75, mean loss 0.055072409425768944
Step 100, mean loss 0.05674943725013167
Step 125, mean loss 0.05863821759426967
Step 150, mean loss 0.06109888484673065
Step 175, mean loss 0.10038858862510706
Step 200, mean loss 0.2697195537860662
Step 225, mean loss 0.20904922370894385
Unrolled forward losses 3.1222403429750365
Unrolled forward base losses 2.565701273852575
Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.17163899338094074, [0.0006010594401160388, 0.0016481127981248871, 0.0017054631144955213, 0.0397910671418159, 0.08145845192679169, 0.007738407606615352], [0.986681603872784, 1.1260253537071847, 1.2065719197411313, 1.135690924799459, 1.1230307511252862, 1.19158497943429]
Training Loss (progress: 0.08), 0.17834678990078373, [0.0006062523198134367, 0.001580804508156603, 0.0017642329939463128, 0.04004485741070362, 0.08174603643795995, 0.007564764538922106], [0.9860695700197649, 1.1261545403510067, 1.2063877857205365, 1.1361916645408368, 1.1233895921481278, 1.1925655987672483]
Training Loss (progress: 0.16), 0.18374191732788997, [0.000598883824725279, 0.0017449286156383387, 0.00167947734610071, 0.04045706813923554, 0.0822672508184881, 0.007687040189344298], [0.985812214108399, 1.1263724466197325, 1.2060656187035277, 1.1371521820799149, 1.1241971672326603, 1.1938121478123696]
Training Loss (progress: 0.24), 0.1784169173047433, [0.0005336197521761145, 0.0016444757585251915, 0.0016688836358758094, 0.040526556448964114, 0.0832125978388204, 0.0076341129776661635], [0.9853717082663732, 1.1264025384265524, 1.206372130137645, 1.1374453458329552, 1.1250352803148749, 1.1949059258376098]
Training Loss (progress: 0.32), 0.18541030250155316, [0.0005870989352203841, 0.0017333191157999654, 0.0016958389537545036, 0.041408716299555885, 0.08354629050383855, 0.007699641997216902], [0.9843866351961119, 1.1259648811675642, 1.2060626785158588, 1.1387180117237925, 1.1255005051937754, 1.1963045408594613]
Training Loss (progress: 0.40), 0.21163548778306357, [0.000554118681714412, 0.0017322052631821684, 0.001801964421274002, 0.04162404181567194, 0.08443073732826595, 0.0076535465201562096], [0.983225575524366, 1.1259824657907276, 1.2063973176815912, 1.139693530904163, 1.1262538211465958, 1.1977922683506617]
Training Loss (progress: 0.48), 0.188999629128114, [0.0005612330576082496, 0.0016459596649713265, 0.0017824276239592433, 0.041937980261562535, 0.0849883258595894, 0.007534245331919239], [0.982760044558262, 1.12614597889066, 1.2063985452514705, 1.1402586653942939, 1.1270123276866393, 1.1991063103127189]
Training Loss (progress: 0.56), 0.19034089256358291, [0.0006118671037649133, 0.0017170645392122646, 0.001714907645948462, 0.04217592198815278, 0.08582623488688222, 0.007531073982681483], [0.9823579124460146, 1.1258165259909687, 1.2064414863823012, 1.1409885379780036, 1.1276603017026707, 1.2006629122369796]
Training Loss (progress: 0.64), 0.16653604878776276, [0.0005780667316980304, 0.0017635214890221746, 0.0017180595809588404, 0.041997405925009, 0.08604429385853116, 0.007517167187779696], [0.9813850494233416, 1.1255155602362354, 1.206134572205515, 1.1412685502815938, 1.1280800378421043, 1.2013773248811384]
Training Loss (progress: 0.72), 0.16872780295577894, [0.0005308554390567104, 0.001669458607806397, 0.001725633568559883, 0.042656256430599906, 0.08657469822077332, 0.007490445563552928], [0.9811166980062115, 1.1254627664713546, 1.2061019836198774, 1.14233537501274, 1.1287178282760695, 1.2026705663773372]
Training Loss (progress: 0.80), 0.1829043762429263, [0.0005751992074630415, 0.001704668937763655, 0.0017572277604229513, 0.04283889885692763, 0.08779371178523313, 0.0076176054013492416], [0.9799521052303147, 1.1253670677544065, 1.2062458254189228, 1.1432112578849762, 1.1300449614674328, 1.2044953276531383]
Training Loss (progress: 0.88), 0.1661228906463618, [0.0005651730240919334, 0.0016896463023319372, 0.0017019604694734623, 0.04320571963889103, 0.08809309059204853, 0.007603752679337048], [0.9797389425013691, 1.1254806937049944, 1.2059683369789793, 1.143749260956304, 1.130371141185828, 1.2057239700163693]
Training Loss (progress: 0.96), 0.18698183676507318, [0.0005835272158904237, 0.0017787540966689717, 0.0017487763618696936, 0.04370441247720576, 0.08859904426437243, 0.007457878280451722], [0.9787857653652464, 1.1250328947651398, 1.2056978863766616, 1.1443634762632644, 1.1307117028619467, 1.207227136882698]
Evaluation on validation dataset:
Step 25, mean loss 0.06653549738500934
Step 50, mean loss 0.03310434024000179
Step 75, mean loss 0.055563175186572666
Step 100, mean loss 0.05253908106982691
Step 125, mean loss 0.06328352580753069
Step 150, mean loss 0.06627726547562587
Step 175, mean loss 0.11309169746633073
Step 200, mean loss 0.33417891117941056
Step 225, mean loss 0.2770541745848995
Unrolled forward losses 2.559467667722828
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 1.26014e-08, 3.02554e-03, 3.22595e-03, 9.06701e-01, 1.00933e+00, 3.16273e-01
Node: 14 (pos: 0.141): 2.77646e-10, 8.73001e-04, 9.29874e-04, 8.63422e-01, 9.85425e-01, 2.38712e-01
Node: 15 (pos: 0.152): 4.25364e-12, 2.23779e-04, 2.38089e-04, 8.18386e-01, 9.59895e-01, 1.75408e-01
Node: 00 (pos: 0.000): 5.34725e-02, 4.36468e-01, 4.67294e-01, 1.10263e+00, 1.11087e+00, 9.74576e-01
Node: 01 (pos: 0.010): 1.90739e-01, 6.60514e-01, 7.07405e-01, 1.12075e+00, 1.11978e+00, 1.07040e+00
Node: 02 (pos: 0.020): 4.73093e-01, 8.87979e-01, 9.51250e-01, 1.13388e+00, 1.12619e+00, 1.14456e+00
-
Node: 03 (pos: 0.030): 8.15925e-01, 1.06051e+00, 1.13624e+00, 1.14183e+00, 1.13006e+00, 1.19150e+00
Node: 05 (pos: 0.051): 8.15925e-01, 1.06051e+00, 1.13624e+00, 1.14183e+00, 1.13006e+00, 1.19150e+00
Node: 06 (pos: 0.061): 4.73093e-01, 8.87979e-01, 9.51250e-01, 1.13388e+00, 1.12619e+00, 1.14456e+00
Node: 07 (pos: 0.071): 1.90739e-01, 6.60514e-01, 7.07405e-01, 1.12075e+00, 1.11978e+00, 1.07040e+00
Node: 08 (pos: 0.081): 5.34725e-02, 4.36468e-01, 4.67294e-01, 1.10263e+00, 1.11087e+00, 9.74576e-01
Node: 09 (pos: 0.091): 1.04236e-02, 2.56221e-01, 2.74196e-01, 1.07976e+00, 1.09952e+00, 8.63871e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.04236e-02, 2.56221e-01, 2.74196e-01, 1.07976e+00, 1.09952e+00, 8.63871e-01
Node: 36 (pos: 0.364): 1.41288e-03, 1.33619e-01, 1.42916e-01, 1.05245e+00, 1.08581e+00, 7.45497e-01
Node: 37 (pos: 0.374): 1.33164e-04, 6.19035e-02, 6.61686e-02, 1.02106e+00, 1.06982e+00, 6.26333e-01
Node: 38 (pos: 0.384): 8.72703e-06, 2.54772e-02, 2.72127e-02, 9.86003e-01, 1.05166e+00, 5.12304e-01
Node: 39 (pos: 0.394): 3.97690e-07, 9.31497e-03, 9.94122e-03, 9.47726e-01, 1.03145e+00, 4.07956e-01
Node: 40 (pos: 0.404): 1.26014e-08, 3.02554e-03, 3.22595e-03, 9.06701e-01, 1.00933e+00, 3.16273e-01
-
Node: 41 (pos: 0.414): 2.77646e-10, 8.73001e-04, 9.29874e-04, 8.63422e-01, 9.85425e-01, 2.38712e-01
Node: 42 (pos: 0.424): 4.25364e-12, 2.23779e-04, 2.38089e-04, 8.18386e-01, 9.59895e-01, 1.75408e-01
Node: 19 (pos: 0.192): 2.77646e-10, 8.73001e-04, 9.29874e-04, 8.63422e-01, 9.85425e-01, 2.38712e-01
Node: 20 (pos: 0.202): 1.26014e-08, 3.02554e-03, 3.22595e-03, 9.06701e-01, 1.00933e+00, 3.16273e-01
Node: 21 (pos: 0.212): 3.97690e-07, 9.31497e-03, 9.94122e-03, 9.47726e-01, 1.03145e+00, 4.07956e-01
Node: 22 (pos: 0.222): 8.72703e-06, 2.54772e-02, 2.72127e-02, 9.86003e-01, 1.05166e+00, 5.12304e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.05498726217949128
Step 50, mean loss 0.026627765740585247
Step 75, mean loss 0.03518935647798488
Step 100, mean loss 0.04401160430313319
Step 125, mean loss 0.08102102879105239
Step 150, mean loss 0.06711526294524414
Step 175, mean loss 0.09970529886045668
Step 200, mean loss 0.11867968622961494
Step 225, mean loss 0.13288514224390602
Unrolled forward losses 2.1865567490508395
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.17053557106757036, [0.0006106923876179264, 0.0016840709504554686, 0.0017454706014129687, 0.0439456124201044, 0.0895090717207027, 0.007558944250302214], [0.9785532869078185, 1.1252174151972885, 1.2055777075099097, 1.144667969883147, 1.131506098647194, 1.2076787265242932]
Training Loss (progress: 0.08), 0.15251480502346157, [0.0006071273419439161, 0.0016881626016638231, 0.0017192076956611486, 0.044106300731327396, 0.08997582974917731, 0.007705778141966566], [0.9783971587712211, 1.1250887974981594, 1.205576920516359, 1.1453040958521699, 1.1321630118993447, 1.2084689735182927]
Training Loss (progress: 0.16), 0.14398837486231275, [0.0005980413101440597, 0.001658574030181961, 0.0017273932806940965, 0.044043275207949414, 0.09037111341492933, 0.007741072099101428], [0.9782725624145319, 1.1252760392444148, 1.205492796679084, 1.1457043806327833, 1.1328372757604004, 1.2093432028129467]
Training Loss (progress: 0.24), 0.14227220994253717, [0.0005874015279707213, 0.0017409765576276682, 0.0017151771593200645, 0.04398789272807446, 0.09069320702184745, 0.007646162395509473], [0.978224947827803, 1.1251164886695864, 1.205730285197372, 1.1461019092764728, 1.13324740215075, 1.209995039650759]
Training Loss (progress: 0.32), 0.15306307265215857, [0.0005888414355266552, 0.0017331326565326827, 0.0017507441883344548, 0.044261722119405086, 0.0907979057052376, 0.007720308658868291], [0.9781668361876158, 1.1250922958147749, 1.2055517927794919, 1.1466491458727024, 1.1335795909688655, 1.2106908058165666]
Training Loss (progress: 0.40), 0.13934325088826643, [0.0006163600126904053, 0.0016943818110767039, 0.0017405404448691188, 0.04432326804262673, 0.09127629919272902, 0.007652999506246831], [0.9778894651442913, 1.1250937413995419, 1.2055932729407333, 1.1470179182103464, 1.1343674244857924, 1.21163445429646]
Training Loss (progress: 0.48), 0.1459549343238855, [0.0006041914288186639, 0.0016312162084582886, 0.0017289237917292855, 0.044396495148466594, 0.09137154762852283, 0.007675707038925915], [0.977789642760729, 1.1253392279570327, 1.2054520903804544, 1.1476045781959752, 1.1348284435501994, 1.2121883246739975]
Training Loss (progress: 0.56), 0.15540689179720357, [0.0006137807423113132, 0.0016802988092757436, 0.001730871152658801, 0.04429697387280845, 0.09154690933311732, 0.007672066765462744], [0.977540308982736, 1.1253206341923758, 1.2052853799275163, 1.1478091707300655, 1.1351760325545974, 1.2131142992748039]
Training Loss (progress: 0.64), 0.14265917159622166, [0.0005912071655784622, 0.0017315727321503885, 0.00174946087371068, 0.04431735653044539, 0.09214726145509865, 0.007641584037224604], [0.9771444376072975, 1.1254301040392523, 1.2051832508590854, 1.1481741269196175, 1.135982671291606, 1.2137952764692772]
Training Loss (progress: 0.72), 0.13907126272220416, [0.0006149625118176262, 0.0016974328080492176, 0.0017401736758247196, 0.04434539573564512, 0.09239127047711859, 0.007634568192977452], [0.9770466105168095, 1.1253109311241132, 1.2052418205593465, 1.1484279020780748, 1.1364789573105785, 1.2144376150200593]
Training Loss (progress: 0.80), 0.1605553120718121, [0.0005972653750580211, 0.0017421536095665207, 0.0017173715751174355, 0.04434516498547294, 0.09244828609181878, 0.007649575805195673], [0.9768448123681374, 1.125279112417492, 1.204907176286002, 1.1487789262242292, 1.1368188257027818, 1.2151284866249756]
Training Loss (progress: 0.88), 0.1531384601603226, [0.0005772565682813471, 0.0017298029554821714, 0.0017399744467869354, 0.044339528730316685, 0.0928440397032596, 0.007691382477270676], [0.9767615879898829, 1.125173746626079, 1.2049356192832525, 1.149080027212889, 1.1372784980320354, 1.215913435013339]
Training Loss (progress: 0.96), 0.1466923965292118, [0.0005878631700164744, 0.0017402172468483762, 0.0017423522289678772, 0.04451304327529837, 0.09327585988769274, 0.007706916032832819], [0.9765845167611779, 1.1252977419490062, 1.2048827748292739, 1.149531711112405, 1.1379686874807902, 1.2167047440329881]
Evaluation on validation dataset:
Step 25, mean loss 0.06387967777629369
Step 50, mean loss 0.021987281203224338
Step 75, mean loss 0.04636828334865652
Step 100, mean loss 0.04219532517645681
Step 125, mean loss 0.04649679876406019
Step 150, mean loss 0.04611151522765907
Step 175, mean loss 0.08018627011539547
Step 200, mean loss 0.21297148755507764
Step 225, mean loss 0.20610313821096535
Unrolled forward losses 1.9456095960360904
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 4.53891e-08, 2.62965e-03, 3.36702e-03, 9.14444e-01, 1.02047e+00, 3.23247e-01
Node: 14 (pos: 0.141): 1.30945e-09, 7.36742e-04, 9.79419e-04, 8.71511e-01, 9.97333e-01, 2.44697e-01
Node: 15 (pos: 0.152): 2.69511e-11, 1.82854e-04, 2.53290e-04, 8.26798e-01, 9.72594e-01, 1.80389e-01
Node: 00 (pos: 0.000): 6.55231e-02, 4.26806e-01, 4.70276e-01, 1.10840e+00, 1.11851e+00, 9.84362e-01
Node: 01 (pos: 0.010): 2.13639e-01, 6.52266e-01, 7.09761e-01, 1.12631e+00, 1.12709e+00, 1.08008e+00
Node: 02 (pos: 0.020): 4.96952e-01, 8.83064e-01, 9.52351e-01, 1.13928e+00, 1.13326e+00, 1.15410e+00
-
Node: 03 (pos: 0.030): 8.24698e-01, 1.05909e+00, 1.13608e+00, 1.14713e+00, 1.13698e+00, 1.20092e+00
Node: 05 (pos: 0.051): 8.24698e-01, 1.05909e+00, 1.13608e+00, 1.14713e+00, 1.13698e+00, 1.20092e+00
Node: 06 (pos: 0.061): 4.96952e-01, 8.83064e-01, 9.52351e-01, 1.13928e+00, 1.13326e+00, 1.15410e+00
Node: 07 (pos: 0.071): 2.13639e-01, 6.52266e-01, 7.09761e-01, 1.12631e+00, 1.12709e+00, 1.08008e+00
Node: 08 (pos: 0.081): 6.55231e-02, 4.26806e-01, 4.70276e-01, 1.10840e+00, 1.11851e+00, 9.84362e-01
Node: 09 (pos: 0.091): 1.43369e-02, 2.47406e-01, 2.77026e-01, 1.08579e+00, 1.10757e+00, 8.73652e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.43369e-02, 2.47406e-01, 2.77026e-01, 1.08579e+00, 1.10757e+00, 8.73652e-01
Node: 36 (pos: 0.364): 2.23800e-03, 1.27046e-01, 1.45082e-01, 1.05878e+00, 1.09434e+00, 7.55104e-01
Node: 37 (pos: 0.374): 2.49237e-04, 5.77944e-02, 6.75511e-02, 1.02772e+00, 1.07892e+00, 6.35566e-01
Node: 38 (pos: 0.384): 1.98022e-05, 2.32908e-02, 2.79626e-02, 9.93022e-01, 1.06139e+00, 5.20955e-01
Node: 39 (pos: 0.394): 1.12243e-06, 8.31485e-03, 1.02908e-02, 9.55108e-01, 1.04186e+00, 4.15838e-01
Node: 40 (pos: 0.404): 4.53891e-08, 2.62965e-03, 3.36702e-03, 9.14444e-01, 1.02047e+00, 3.23247e-01
-
Node: 41 (pos: 0.414): 1.30945e-09, 7.36742e-04, 9.79419e-04, 8.71511e-01, 9.97333e-01, 2.44697e-01
Node: 42 (pos: 0.424): 2.69511e-11, 1.82854e-04, 2.53290e-04, 8.26798e-01, 9.72594e-01, 1.80389e-01
Node: 19 (pos: 0.192): 1.30945e-09, 7.36742e-04, 9.79419e-04, 8.71511e-01, 9.97333e-01, 2.44697e-01
Node: 20 (pos: 0.202): 4.53891e-08, 2.62965e-03, 3.36702e-03, 9.14444e-01, 1.02047e+00, 3.23247e-01
Node: 21 (pos: 0.212): 1.12243e-06, 8.31485e-03, 1.02908e-02, 9.55108e-01, 1.04186e+00, 4.15838e-01
Node: 22 (pos: 0.222): 1.98022e-05, 2.32908e-02, 2.79626e-02, 9.93022e-01, 1.06139e+00, 5.20955e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.05386337298547436
Step 50, mean loss 0.01809678086570636
Step 75, mean loss 0.027469881286044016
Step 100, mean loss 0.03256043638623837
Step 125, mean loss 0.05198203002103204
Step 150, mean loss 0.04936831590953526
Step 175, mean loss 0.07654390407686279
Step 200, mean loss 0.08707337437806761
Step 225, mean loss 0.09654745730165656
Unrolled forward losses 1.7356450470829468
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.13363775545042997, [0.0005936150128690791, 0.0017200057890650738, 0.001746900420144651, 0.04454692245295305, 0.09333964105864272, 0.007690035826792478], [0.976289466853132, 1.125396791752892, 1.2048233443198717, 1.149756027526479, 1.1381653218778915, 1.2169671436533869]
Training Loss (progress: 0.08), 0.15410248999387705, [0.0006062880434225791, 0.0017041636096836578, 0.0017376246998686321, 0.0446455385025972, 0.09359980692970882, 0.0077257113858462964], [0.9763404818244431, 1.125397904829235, 1.2047855778956562, 1.150227101754408, 1.1385400837407718, 1.2176399834148872]
Training Loss (progress: 0.16), 0.14099187507446231, [0.0006151053552811855, 0.0017090766231265962, 0.0017089889023373227, 0.04482338136471171, 0.09367757692905383, 0.007709693191096615], [0.9759793631283863, 1.125267410008647, 1.2047489007013439, 1.1505413236650859, 1.139009499077642, 1.2183443902862687]
Training Loss (progress: 0.24), 0.12525053604796665, [0.0006138354002541384, 0.0017214308860761776, 0.001747943795309856, 0.04485592249978453, 0.09410260902933558, 0.007716464137557484], [0.975842207616477, 1.1252737153745977, 1.204740095561672, 1.1509175352145657, 1.1395143683577087, 1.2189094243645526]
Training Loss (progress: 0.32), 0.14338368499264076, [0.0005896552898291364, 0.0017641681861843993, 0.001742961190500669, 0.044841127723185575, 0.0944111812031706, 0.007681314385998722], [0.9755637625106693, 1.1253193207647254, 1.2046828613591574, 1.1511753768147859, 1.1401072158083851, 1.2195932232784081]
Training Loss (progress: 0.40), 0.13982693369670302, [0.0006076445743548594, 0.0017127717125343354, 0.0017467627643820182, 0.0448803115629733, 0.09485024978549945, 0.0077726282035169595], [0.9755144104327937, 1.1252779414597383, 1.2044694536354876, 1.1515249056025425, 1.1406642428460971, 1.2202385706603605]
Training Loss (progress: 0.48), 0.14300060496625402, [0.0005850965361679912, 0.001721689734435835, 0.0017500636678032954, 0.04514650368355706, 0.09502875877043432, 0.007701790333844091], [0.9753183354198306, 1.1252754060756682, 1.2043811256185506, 1.151964821507779, 1.1411349217759104, 1.22075755696704]
Training Loss (progress: 0.56), 0.14401233944914962, [0.0005791389540603868, 0.0017150517097076583, 0.0017374740096285314, 0.04507302618244113, 0.09535924710851362, 0.00771737821251702], [0.9750313679985972, 1.1249893413318488, 1.2043407422941232, 1.152172682620404, 1.1416085763925705, 1.2214727730673052]
Training Loss (progress: 0.64), 0.12816601430847901, [0.0006249192804028957, 0.001687070991276313, 0.001742778376794193, 0.045037241767179104, 0.09577408370761131, 0.007717140836829069], [0.9747788633273896, 1.1251077741146935, 1.2042607626492832, 1.1524253367191155, 1.1421806826342369, 1.222073321831802]
Training Loss (progress: 0.72), 0.14479815477821137, [0.000587575761294389, 0.001730422510889231, 0.0017466233833053773, 0.0453191192765105, 0.0959597900935579, 0.0077647067563120795], [0.9747012951127441, 1.1253031104456068, 1.2042120013027775, 1.1529509191320113, 1.1425973275357262, 1.2228059522681691]
Training Loss (progress: 0.80), 0.14519325890294676, [0.0006006517151587713, 0.0017620868925809386, 0.0017530506746855602, 0.04529468861422765, 0.09627873482074416, 0.007746015571683571], [0.9744058655201486, 1.1250993426790885, 1.2040850392850615, 1.1531264430155188, 1.1431426632594637, 1.2233152395375586]
Training Loss (progress: 0.88), 0.12059794115269652, [0.0006027836506356367, 0.001726917478029029, 0.0017186898198272716, 0.045245167237112104, 0.09662409393785476, 0.007654219469013971], [0.9740278925727681, 1.1250054747227025, 1.2040813870321918, 1.1534494350630435, 1.1435773620988845, 1.2238032909955725]
Training Loss (progress: 0.96), 0.13631466253683397, [0.0005896910716436533, 0.001735805378306773, 0.0017668424998461625, 0.045395554049058554, 0.09680418795409275, 0.007699478358610101], [0.974057100864419, 1.1250072874949333, 1.203941156369008, 1.1537858984357987, 1.1439921129581996, 1.2244950070329184]
Evaluation on validation dataset:
Step 25, mean loss 0.06588398888233163
Step 50, mean loss 0.022863304425003166
Step 75, mean loss 0.040512555239299
Step 100, mean loss 0.04072411575391824
Step 125, mean loss 0.04829309529583664
Step 150, mean loss 0.046448737642628725
Step 175, mean loss 0.08230336442241859
Step 200, mean loss 0.22051658044829836
Step 225, mean loss 0.19482729016496028
Unrolled forward losses 2.207676036142632
Unrolled forward base losses 2.565701273852575
Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.12651743933931464, [0.0005934742238302916, 0.0017576269105090338, 0.001754764673238443, 0.045516835065816656, 0.09678442753888403, 0.007698182189345557], [0.974007110754362, 1.1250185926623706, 1.2038700027622369, 1.1540570993065615, 1.1441178717744456, 1.2248589504678178]
Training Loss (progress: 0.08), 0.13929233490112633, [0.000639616749808103, 0.0016798340560116043, 0.00175347018615203, 0.0456333285439954, 0.09705854063845795, 0.00770695550153848], [0.973627894283743, 1.1250299403668724, 1.2039213316656312, 1.1544828158303793, 1.1445873463208103, 1.2254171715006215]
Training Loss (progress: 0.16), 0.13574532988297255, [0.0005972847594059965, 0.0017592387767376796, 0.0017494481611799594, 0.045554657226520044, 0.09759452702817945, 0.007698049533235891], [0.9734614966115551, 1.1249215219196205, 1.2037757716861925, 1.154664571156365, 1.1453144649207356, 1.22615178811943]
Training Loss (progress: 0.24), 0.13405511302805212, [0.0005873516415702809, 0.0016851003711435848, 0.0017585856664201242, 0.04565991965429941, 0.09808042303972181, 0.007688876324182788], [0.9731956421801126, 1.1249415908556157, 1.2036788311658209, 1.1549758512581558, 1.1458754240756177, 1.2267931099194906]
Training Loss (progress: 0.32), 0.12876064564721526, [0.000600695809009049, 0.0017196480267259056, 0.0017341732900366374, 0.04555263795518568, 0.09824909054377054, 0.007673810224464145], [0.9730544899349225, 1.1251957523310425, 1.2036768389955441, 1.1551319600887517, 1.1462549116558205, 1.2273939801391893]
Training Loss (progress: 0.40), 0.13340504559479333, [0.0005619963471917998, 0.0017559502804573514, 0.001718743219304866, 0.04569323500922778, 0.09860621774170807, 0.007632035422782614], [0.9730214058552306, 1.1250695768972403, 1.2036098533766468, 1.1555403010262983, 1.146727923383236, 1.2280525146003272]
Training Loss (progress: 0.48), 0.13783281836692227, [0.0006152305440034905, 0.0017636316674409466, 0.0017543412411441673, 0.04579614556625346, 0.09909214380633986, 0.0076519088347906826], [0.9726641255211549, 1.1248290039054698, 1.2035041607689765, 1.155876370147227, 1.1473059288824121, 1.2287105018706]
Training Loss (progress: 0.56), 0.13138859359413504, [0.0006168759525607185, 0.001776053958390859, 0.0017564046903518857, 0.045767271130241545, 0.09934473076582238, 0.0077157616205524825], [0.97249112319315, 1.1248579188806997, 1.2033341962035535, 1.1559465464384227, 1.1476661033882836, 1.2291583410735134]
Training Loss (progress: 0.64), 0.14247649503094284, [0.0006184608600584421, 0.0017427296100968185, 0.0017511873240303897, 0.04591971652962202, 0.09958814763655917, 0.007695208221639954], [0.9724069345176053, 1.1249473789118885, 1.203205120914623, 1.1562759866083336, 1.1481066479564785, 1.2295045211592333]
Training Loss (progress: 0.72), 0.135555700723086, [0.0006239746823069222, 0.001767342388959383, 0.0017828079743569963, 0.04626080505822317, 0.09997522048586491, 0.007687035030021374], [0.9722998979746666, 1.1250026636503907, 1.2031946572965266, 1.1567681900897593, 1.1486780629725264, 1.2302066825297697]
Training Loss (progress: 0.80), 0.1379802087552826, [0.0006093277104678626, 0.0017512151450769886, 0.0017683106130105316, 0.04626258164190356, 0.10018158895962727, 0.007739722915465043], [0.9720043086176108, 1.1248732300330406, 1.2030304768792615, 1.1569770134842512, 1.1489904189175346, 1.2309072589968684]
Training Loss (progress: 0.88), 0.12414280507851609, [0.0006063088730512026, 0.0017620534450927736, 0.0017372251449803986, 0.04631883191872756, 0.10052739833118554, 0.0077198022370090985], [0.9717727000730808, 1.1246753282546929, 1.2029436139213017, 1.1573778481805428, 1.1494774894323598, 1.2314871139411014]
Training Loss (progress: 0.96), 0.14515507170198833, [0.0006058472479286436, 0.0017570896151833581, 0.0017463743805039018, 0.046437129543102064, 0.10120543445352359, 0.007677876063517771], [0.9716875938324953, 1.124885913319899, 1.2027762488205638, 1.1575542373975352, 1.150138846629472, 1.232058617374553]
Evaluation on validation dataset:
Step 25, mean loss 0.04977655864142419
Step 50, mean loss 0.0212077134782713
Step 75, mean loss 0.0373317347174701
Step 100, mean loss 0.03920839432684796
Step 125, mean loss 0.04720394181753404
Step 150, mean loss 0.045346760980489106
Step 175, mean loss 0.078405582163431
Step 200, mean loss 0.21198583448661218
Step 225, mean loss 0.18607127429827802
Unrolled forward losses 1.8468584114160476
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 4.43781e-08, 3.40939e-03, 3.42433e-03, 9.29487e-01, 1.04034e+00, 3.27014e-01
Node: 14 (pos: 0.141): 1.27558e-09, 1.00883e-03, 9.99978e-04, 8.87594e-01, 1.01858e+00, 2.47490e-01
Node: 15 (pos: 0.152): 2.61483e-11, 2.65823e-04, 2.59712e-04, 8.43874e-01, 9.95276e-01, 1.82400e-01
Node: 00 (pos: 0.000): 6.50141e-02, 4.44744e-01, 4.70890e-01, 1.11779e+00, 1.13212e+00, 9.96784e-01
Node: 01 (pos: 0.010): 2.12240e-01, 6.67416e-01, 7.09765e-01, 1.13510e+00, 1.14012e+00, 1.09380e+00
Node: 02 (pos: 0.020): 4.94131e-01, 8.91898e-01, 9.51473e-01, 1.14764e+00, 1.14588e+00, 1.16882e+00
-
Node: 03 (pos: 0.030): 8.20447e-01, 1.06137e+00, 1.13440e+00, 1.15522e+00, 1.14934e+00, 1.21629e+00
Node: 05 (pos: 0.051): 8.20447e-01, 1.06137e+00, 1.13440e+00, 1.15522e+00, 1.14934e+00, 1.21629e+00
Node: 06 (pos: 0.061): 4.94131e-01, 8.91898e-01, 9.51473e-01, 1.14764e+00, 1.14588e+00, 1.16882e+00
Node: 07 (pos: 0.071): 2.12240e-01, 6.67416e-01, 7.09765e-01, 1.13510e+00, 1.14012e+00, 1.09380e+00
Node: 08 (pos: 0.081): 6.50141e-02, 4.44744e-01, 4.70890e-01, 1.11779e+00, 1.13212e+00, 9.96784e-01
Node: 09 (pos: 0.091): 1.42031e-02, 2.63910e-01, 2.77850e-01, 1.09591e+00, 1.12191e+00, 8.84586e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.42031e-02, 2.63910e-01, 2.77850e-01, 1.09591e+00, 1.12191e+00, 8.84586e-01
Node: 36 (pos: 0.364): 2.21284e-03, 1.39455e-01, 1.45811e-01, 1.06975e+00, 1.10956e+00, 7.64460e-01
Node: 37 (pos: 0.374): 2.45875e-04, 6.56216e-02, 6.80543e-02, 1.03965e+00, 1.09514e+00, 6.43346e-01
Node: 38 (pos: 0.384): 1.94837e-05, 2.74974e-02, 2.82494e-02, 1.00596e+00, 1.07873e+00, 5.27242e-01
Node: 39 (pos: 0.394): 1.10109e-06, 1.02605e-02, 1.04292e-02, 9.69092e-01, 1.06043e+00, 4.20775e-01
Node: 40 (pos: 0.404): 4.43781e-08, 3.40939e-03, 3.42433e-03, 9.29487e-01, 1.04034e+00, 3.27014e-01
-
Node: 41 (pos: 0.414): 1.27558e-09, 1.00883e-03, 9.99978e-04, 8.87594e-01, 1.01858e+00, 2.47490e-01
Node: 42 (pos: 0.424): 2.61483e-11, 2.65823e-04, 2.59712e-04, 8.43874e-01, 9.95276e-01, 1.82400e-01
Node: 19 (pos: 0.192): 1.27558e-09, 1.00883e-03, 9.99978e-04, 8.87594e-01, 1.01858e+00, 2.47490e-01
Node: 20 (pos: 0.202): 4.43781e-08, 3.40939e-03, 3.42433e-03, 9.29487e-01, 1.04034e+00, 3.27014e-01
Node: 21 (pos: 0.212): 1.10109e-06, 1.02605e-02, 1.04292e-02, 9.69092e-01, 1.06043e+00, 4.20775e-01
Node: 22 (pos: 0.222): 1.94837e-05, 2.74974e-02, 2.82494e-02, 1.00596e+00, 1.07873e+00, 5.27242e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.04308986225513611
Step 50, mean loss 0.016151479543120663
Step 75, mean loss 0.024016579088271386
Step 100, mean loss 0.028735735869502437
Step 125, mean loss 0.047004065561634965
Step 150, mean loss 0.04487154427503369
Step 175, mean loss 0.07678835995566995
Step 200, mean loss 0.08297931821107368
Step 225, mean loss 0.09735015881681507
Unrolled forward losses 1.6156082429912033
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.12788274143757383, [0.0006224949444641, 0.0017470659961736658, 0.0017684500504696328, 0.04652289983565278, 0.10137613661091939, 0.007656678875938564], [0.9714789810750959, 1.1247988314609898, 1.2029148274463624, 1.157821938309276, 1.1505117151070396, 1.2325053343316654]
Training Loss (progress: 0.08), 0.12647388640930252, [0.0005769672823989291, 0.0017200435052563217, 0.0017458574555374196, 0.046530365161140376, 0.10150503935256479, 0.007591187383342527], [0.9712581586749788, 1.124621779770447, 1.2027567737174243, 1.1579979274368997, 1.150692146149143, 1.2330491568064328]
Training Loss (progress: 0.16), 0.14662137442414752, [0.0005777196549190202, 0.00172671165258268, 0.0017709938466411695, 0.04664380016536521, 0.10164205770288075, 0.007681002279973315], [0.9711253908892655, 1.1246006523533967, 1.2026584735327792, 1.1582386995932819, 1.151156830544459, 1.233596190022843]
Training Loss (progress: 0.24), 0.1316988708078661, [0.0005794403484192455, 0.001773145804444505, 0.001765395236289985, 0.04686531255626257, 0.10209389877982798, 0.007656075083095069], [0.9708463142265525, 1.1246737816573784, 1.2026127776265854, 1.158609642956861, 1.1515594171186279, 1.2340697032330556]
Training Loss (progress: 0.32), 0.1404371963928677, [0.0006207845340958823, 0.0017946841676218005, 0.001764148640358042, 0.04696647180525563, 0.10240733389737033, 0.007642200174275509], [0.9704163338308641, 1.1248517587472384, 1.2025547086513668, 1.1587551473565643, 1.151993731031873, 1.2349108230038222]
Training Loss (progress: 0.40), 0.13755375855194404, [0.0005791368517971535, 0.0017604623991965707, 0.0017684197975974688, 0.04709416117634871, 0.10264059197293021, 0.007671329576919521], [0.9700976950699262, 1.1246951348245522, 1.2024261520558592, 1.1590677637505682, 1.1523670057755677, 1.2352914619603474]
Training Loss (progress: 0.48), 0.14026919964781143, [0.000600445396862398, 0.0017866141413429458, 0.0017570723817775982, 0.04724129571403857, 0.10294593247054502, 0.007681922576393252], [0.9697733427394413, 1.1247583147719609, 1.20225610593154, 1.1593835961256571, 1.1529197761858132, 1.236006506024666]
Training Loss (progress: 0.56), 0.13432448568962457, [0.0005795897653197631, 0.0017367857836130133, 0.00175324908182631, 0.04739058090120014, 0.10326752773844951, 0.007694099560491739], [0.9697772534931275, 1.1245179253106306, 1.2019966934896367, 1.159732586638958, 1.1533267303032704, 1.236389840618695]
Training Loss (progress: 0.64), 0.12799716517981782, [0.0005980999620555264, 0.0017047821118837884, 0.001781091503341158, 0.04753666854857929, 0.10326616558791359, 0.007618416590885983], [0.9696007386386266, 1.124568289499068, 1.20190937492588, 1.1600164277253733, 1.1536141521785448, 1.2369832652213653]
Training Loss (progress: 0.72), 0.12565193805479627, [0.000588784989123048, 0.0017730356473791276, 0.001781672206757005, 0.04764384988213109, 0.10376310291526793, 0.007668270715872351], [0.9693530179640157, 1.12456507543498, 1.201915430035089, 1.1602483714721725, 1.1541744816564412, 1.237699302710512]
Training Loss (progress: 0.80), 0.13028207606522488, [0.000621168144556896, 0.0017831863603745746, 0.0017819053981325702, 0.047701404988560646, 0.10409416109852983, 0.007738662626356154], [0.9691170186020486, 1.124443784076312, 1.2017224138355784, 1.1606026393458857, 1.1547538499158219, 1.238297113587947]
Training Loss (progress: 0.88), 0.1374747182148517, [0.0006106241768795719, 0.0017616933338002975, 0.0017807468961854765, 0.047744258742898976, 0.10430092013841871, 0.007675054528121612], [0.9688566216707473, 1.1244726586497216, 1.2015851173985947, 1.1608290931441596, 1.1550522881255119, 1.2385806675688311]
Training Loss (progress: 0.96), 0.13675486067807827, [0.000602966733205894, 0.001786022627951527, 0.0017897738599702976, 0.047819846653231336, 0.10477264463913694, 0.007670580787504318], [0.9685392744806343, 1.1243316495964273, 1.201643190888078, 1.1610044756342655, 1.1555346739603878, 1.2389252351389488]
Evaluation on validation dataset:
Step 25, mean loss 0.052920852068102875
Step 50, mean loss 0.023279664722972666
Step 75, mean loss 0.03909996300794061
Step 100, mean loss 0.043196400152642506
Step 125, mean loss 0.04297178798311598
Step 150, mean loss 0.04644314130836344
Step 175, mean loss 0.07469447777998275
Step 200, mean loss 0.2201188446144405
Step 225, mean loss 0.1843662972669945
Unrolled forward losses 2.2324021531743923
Unrolled forward base losses 2.565701273852575
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.12081831270575183, [0.0005991745755743178, 0.0017598697283092532, 0.001776417878156282, 0.04790260026162373, 0.10493149605987763, 0.007714455385954282], [0.9686296383908946, 1.1242406327440508, 1.2017222191082815, 1.1611387134821367, 1.155645171852648, 1.2391757823956462]
Training Loss (progress: 0.08), 0.1319306672044469, [0.0005863349560361676, 0.0017489619819377538, 0.0017944653726667747, 0.048103959193033695, 0.10522371959491562, 0.007720556847691793], [0.9684817911003615, 1.1243226659503616, 1.2015207246851267, 1.1614519944850805, 1.1561526379150113, 1.2398414782994174]
Training Loss (progress: 0.16), 0.13730146672915064, [0.0005950027532384986, 0.0017340626592966285, 0.0017750874427277443, 0.048195182377620195, 0.10564571544643084, 0.00779710181297596], [0.9683665699745663, 1.1242838121443912, 1.2013159906028263, 1.1616370317670786, 1.156534008507296, 1.2405306239059934]
Training Loss (progress: 0.24), 0.1352269214176703, [0.0005623433799809063, 0.0017862379818852784, 0.0017969979781952707, 0.048303567377295034, 0.10600315666908777, 0.007682034387851451], [0.9681171903642124, 1.1243466074965227, 1.201223334061524, 1.1617657831759203, 1.1569741570498173, 1.2409969844267281]
Training Loss (progress: 0.32), 0.13650653361159434, [0.0006261406864999284, 0.0017679449364179812, 0.0017812837718850338, 0.048474885316361176, 0.10629696845270978, 0.007725778894213375], [0.9678439010348686, 1.124173621062616, 1.2009519743342338, 1.1621240315650916, 1.157327925011334, 1.2415418391671216]
Training Loss (progress: 0.40), 0.1289201384170681, [0.0006166288701219003, 0.0017933134430189374, 0.0017626355288144467, 0.04863779995388803, 0.10664151016410273, 0.007785541174624996], [0.9676935201993113, 1.1240053791333293, 1.2010120841697183, 1.1624480079831434, 1.1578126503676185, 1.2420491083854743]
Training Loss (progress: 0.48), 0.13070629547903315, [0.0006084443357076833, 0.0018010156828450591, 0.001805372109455246, 0.048716761432266976, 0.1072577291761867, 0.007689534554608868], [0.9672797761772322, 1.1240857889706408, 1.2008324052759893, 1.1626610509131843, 1.1584664819604695, 1.2423866404758948]
Training Loss (progress: 0.56), 0.11909490402739809, [0.0005838028537296883, 0.0017912249516998636, 0.001789764536450104, 0.048874470710669776, 0.10752744061291963, 0.007660153368490519], [0.9670277416942711, 1.1239426046358612, 1.200659755779801, 1.1628796326491935, 1.1588887764078377, 1.2429650826755996]
Training Loss (progress: 0.64), 0.14354083695661568, [0.0005911672570523218, 0.00174591022834632, 0.0017808161786235651, 0.04893608862367569, 0.10785001367023286, 0.007728097990725742], [0.966970387489335, 1.1237986499215529, 1.2005926438534387, 1.1630718984874036, 1.1592755133490231, 1.2435428856040724]
Training Loss (progress: 0.72), 0.12887478881258757, [0.0006003034315775004, 0.0017867569025247366, 0.0017806068071094716, 0.0491184030739136, 0.10813620942663912, 0.007678601281965664], [0.9667475041863147, 1.1237168094983296, 1.2004681323713557, 1.1634804001394126, 1.1596927817363367, 1.244143428658765]
Training Loss (progress: 0.80), 0.11990720439457735, [0.0006028765115503397, 0.001766805754590466, 0.0017826352775544853, 0.04941282372084206, 0.10864040806180938, 0.0077915086273784], [0.9665026734107885, 1.1239794376115793, 1.200524620475021, 1.1636961068271778, 1.1601614347198193, 1.2447393958239445]
Training Loss (progress: 0.88), 0.13659857714958398, [0.0006003385173854593, 0.0017788682207433714, 0.0017695566617258818, 0.049491767814986896, 0.10872398011737858, 0.0077225077711235495], [0.9662846243609801, 1.1237208888038508, 1.2004763365902735, 1.163909997576854, 1.1604898880568952, 1.2451908612508025]
Training Loss (progress: 0.96), 0.11444791736242528, [0.0006075335820700717, 0.001770661058622412, 0.0018063282993320633, 0.049553538151641166, 0.10894796464140254, 0.007723957057193692], [0.9660761754788113, 1.1237374906661866, 1.200294927459674, 1.1641116789304164, 1.1608224777668295, 1.2455865082282047]
Evaluation on validation dataset:
Step 25, mean loss 0.04483564042736504
Step 50, mean loss 0.023238194645464883
Step 75, mean loss 0.0336831739956147
Step 100, mean loss 0.037410156214767815
Step 125, mean loss 0.04372697859058084
Step 150, mean loss 0.041215722567450694
Step 175, mean loss 0.0735251509363528
Step 200, mean loss 0.2400863450829625
Step 225, mean loss 0.17165001521574003
Unrolled forward losses 2.0073276173470638
Unrolled forward base losses 2.565701273852575
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.12762176567574363, [0.0006057407183881064, 0.0017824938852275738, 0.0017958642450290518, 0.04961118664445281, 0.10917315413235466, 0.007767945176828403], [0.9660889205062668, 1.1236831435161352, 1.2001875165428162, 1.1642609709590082, 1.161091712183825, 1.2459359827109193]
Training Loss (progress: 0.08), 0.12380570283837738, [0.0005999109486908749, 0.0017464539481292982, 0.0017797096820151839, 0.04973618655182386, 0.10935729652287204, 0.007803585316123955], [0.9661063504448605, 1.1237458494109227, 1.200209909616213, 1.1645217030257888, 1.1614565449774694, 1.246326634051175]
Training Loss (progress: 0.16), 0.12302568904987352, [0.0006116161336442383, 0.0017814960304261896, 0.0017757179674630999, 0.04969386388225597, 0.1095395740381658, 0.007808106923011987], [0.966221971574802, 1.1237695935585759, 1.2001283192123406, 1.1645725129284024, 1.161741759556961, 1.2465992792801723]
Training Loss (progress: 0.24), 0.13059799165046998, [0.0006029054433497327, 0.001786476550418909, 0.0017961578887902925, 0.0497587758663726, 0.10974348595561287, 0.007821853898957483], [0.9660575795540456, 1.1236667964567784, 1.2001016042486292, 1.164796323300617, 1.162070079026601, 1.2469304285286744]
Training Loss (progress: 0.32), 0.11299097850535936, [0.0005898464407707486, 0.0017620410631988205, 0.0017874939644321522, 0.049780098247776156, 0.10987767910896311, 0.00779178598879171], [0.9659581562320517, 1.1236910148393147, 1.2000672076378391, 1.1649905309117805, 1.1623378315879114, 1.2471870816010793]
Training Loss (progress: 0.40), 0.1245254887802924, [0.0006045936405009624, 0.0018045441422060871, 0.0017773929155334568, 0.04978930801443709, 0.11001225742580009, 0.007805697430116818], [0.9659162595668883, 1.1237724986008053, 1.2000361816818597, 1.165111904523552, 1.1626316074787328, 1.2473901802464258]
Training Loss (progress: 0.48), 0.1122422920645682, [0.0005998166713252024, 0.001788740710207369, 0.0017923604440344748, 0.049868987733103616, 0.11014637565009275, 0.007797191620503236], [0.9657286962526151, 1.1238277832521562, 1.199903018782256, 1.1652985892800172, 1.162928024673748, 1.2477439773191605]
Training Loss (progress: 0.56), 0.10957946688540567, [0.0006034204028946159, 0.0017755211966239428, 0.0017836444767998212, 0.04991497662759263, 0.1104023013044173, 0.007779822675600469], [0.9655284010754162, 1.1238352219177352, 1.199919419667164, 1.1654822333429073, 1.163216773742535, 1.2480276264001466]
Training Loss (progress: 0.64), 0.12505830237760318, [0.0005875032571955341, 0.0017902860421430624, 0.0017784510847511103, 0.049892282054492275, 0.11051540584277612, 0.007776568513855784], [0.965534528660284, 1.123773206530594, 1.199890855970889, 1.1655389794829412, 1.1635018836899877, 1.2482901553364227]
Training Loss (progress: 0.72), 0.11082223936494784, [0.000602079778335225, 0.0018176003943419586, 0.0017898317989589122, 0.04993011152889568, 0.11072491504817707, 0.0077935298855755794], [0.9654089488963927, 1.1236931594165325, 1.1998325632769071, 1.165752267573108, 1.1637566508225177, 1.2486349498066314]
Training Loss (progress: 0.80), 0.12162424331749622, [0.0006027692937529693, 0.0017983775322560308, 0.0018097039424256947, 0.04990119112373419, 0.1108982130926942, 0.007789859556809099], [0.9654225830110789, 1.1237215051877465, 1.199863625855288, 1.165912697998278, 1.1640523151010373, 1.2489171199790958]
Training Loss (progress: 0.88), 0.1152932886611404, [0.0005961594147239255, 0.001788680634056476, 0.0017862843710891307, 0.04988774610802038, 0.11099982359151092, 0.00781765751767133], [0.9653209982060988, 1.123824376920491, 1.199810322763685, 1.1660101689676103, 1.1643308005003703, 1.249144788460261]
Training Loss (progress: 0.96), 0.12434006058367007, [0.0005910771749247859, 0.0017916792388916866, 0.0018023694349687052, 0.05001343247487317, 0.11111853197774044, 0.007826218070969298], [0.9653019692136625, 1.123830259149042, 1.199799863705357, 1.1662524463976083, 1.1646096410100848, 1.2495322072649107]
Evaluation on validation dataset:
Step 25, mean loss 0.04033068351212485
Step 50, mean loss 0.019083053132785445
Step 75, mean loss 0.03081062353168781
Step 100, mean loss 0.035159720019862534
Step 125, mean loss 0.038555714214470536
Step 150, mean loss 0.03959946024630884
Step 175, mean loss 0.06384921834086216
Step 200, mean loss 0.19656250536930614
Step 225, mean loss 0.17731822200740938
Unrolled forward losses 1.9034833823991297
Unrolled forward base losses 2.565701273852575
Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.11378542895910823, [0.0005961292859185791, 0.0017872446926006226, 0.0017923190604512476, 0.0499682677402118, 0.11125844678064256, 0.0078078554062431665], [0.9652809987424872, 1.1238574241441601, 1.1997490169285, 1.1662602033571687, 1.1647949701709879, 1.249641144763005]
Training Loss (progress: 0.08), 0.11569025951598509, [0.0006188053210770043, 0.0017725766637002051, 0.001767099325077427, 0.05002696192019384, 0.11137690438113504, 0.00776590926731684], [0.9651848089109645, 1.12387470897555, 1.1997437829488715, 1.1664211981553707, 1.164957564970087, 1.249823855868891]
Training Loss (progress: 0.16), 0.12251189875695179, [0.0005980550474223945, 0.001814245446250585, 0.0018092409898345426, 0.050115692963115854, 0.11150164414110367, 0.007782004356146355], [0.965004976756231, 1.1238651076511708, 1.1996327076012245, 1.1665264215628122, 1.165237996803477, 1.2501916866612406]
Training Loss (progress: 0.24), 0.10712907070863617, [0.0005795046470165365, 0.0017882967302614429, 0.00179325113918637, 0.05004896521179252, 0.11167490506438006, 0.00778942133584524], [0.9649662144621486, 1.123883276493566, 1.1995988802820305, 1.1665989797766405, 1.1655059690930616, 1.2504208573493396]
Training Loss (progress: 0.32), 0.12493289644333462, [0.0006215913128246694, 0.0017867352989283218, 0.0017932764410232137, 0.05011864871237884, 0.11187124966963935, 0.007791256106648815], [0.964938828916458, 1.1239773062164378, 1.1996005863613555, 1.1668224422824631, 1.1658437038311962, 1.250685176773125]
Training Loss (progress: 0.40), 0.11749812526407635, [0.000598113963708946, 0.0018212929051552992, 0.0017865275060669147, 0.05013412728545037, 0.11191072832616246, 0.007807622949031513], [0.964879585405695, 1.1238603907251503, 1.1995351304416602, 1.1669068974431673, 1.1660199031711047, 1.25087452505747]
Training Loss (progress: 0.48), 0.11839466699713522, [0.0006007608032931494, 0.0017892467638409243, 0.0017857530613518179, 0.05014959391295187, 0.11216901973237248, 0.00784163844668153], [0.9646678452794825, 1.1238119632108894, 1.1995102780707483, 1.1670559024982143, 1.166324912229049, 1.2513008552683431]
Training Loss (progress: 0.56), 0.1189335616850941, [0.0006024713987977719, 0.001825298635781115, 0.0017973506741765669, 0.050205111927054784, 0.11232826714835795, 0.007840650677594423], [0.964556978186644, 1.123823344949988, 1.1994543191984393, 1.1672373686409265, 1.1666153965126398, 1.2515195809811663]
Training Loss (progress: 0.64), 0.09694866401544858, [0.0005969509724322943, 0.001800747660234468, 0.00179380904652735, 0.05021403401537946, 0.11249872226389313, 0.007762937366691119], [0.9646998090758414, 1.1237338628413596, 1.1994407153976117, 1.1673638695747288, 1.1668589498631439, 1.2517559885841327]
Training Loss (progress: 0.72), 0.1074817814386938, [0.0005989354992714213, 0.001787696705009039, 0.0017912025062427378, 0.05021796824441204, 0.11276679121911146, 0.007818544828465916], [0.9645751773270108, 1.1238050197326228, 1.199340824454391, 1.1675462164992534, 1.1671938558158406, 1.252090713993381]
Training Loss (progress: 0.80), 0.11877284883375822, [0.0005985980588830032, 0.0018222165774318413, 0.0017938460612501276, 0.05027004693659338, 0.11295368191124527, 0.007828280102655126], [0.9643972186692554, 1.123762115717207, 1.1993443567271538, 1.167660265212423, 1.1675244518446317, 1.2524012921128231]
Training Loss (progress: 0.88), 0.1155547947850897, [0.000620070997408643, 0.0018076825787630253, 0.0017759563147370548, 0.05039666855159375, 0.11307299841392834, 0.007809903524263368], [0.9645550405572021, 1.1237563097661476, 1.1992399256104629, 1.1678422691793133, 1.1677681444151176, 1.2526186641757286]
Training Loss (progress: 0.96), 0.09941758418501555, [0.0006038662835488695, 0.001793309529468801, 0.0017906529016232862, 0.050375623942986406, 0.1132208903659962, 0.007819476674278656], [0.9644784788308376, 1.1238563825002723, 1.1991735347876493, 1.1679440728150665, 1.1680441590429966, 1.2528845361033667]
Evaluation on validation dataset:
Step 25, mean loss 0.037815573667589204
Step 50, mean loss 0.01749774838394161
Step 75, mean loss 0.02981152414067018
Step 100, mean loss 0.03335888243009505
Step 125, mean loss 0.03728488217492351
Step 150, mean loss 0.03804468829098182
Step 175, mean loss 0.06494640621202949
Step 200, mean loss 0.20490673331327927
Step 225, mean loss 0.1668179707075477
Unrolled forward losses 1.65671060463545
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.49705e-08, 3.91776e-03, 3.98066e-03, 9.54102e-01, 1.06747e+00, 3.40082e-01
Node: 14 (pos: 0.141): 9.57598e-10, 1.19380e-03, 1.20054e-03, 9.14417e-01, 1.04746e+00, 2.58611e-01
Node: 15 (pos: 0.152): 1.86146e-11, 3.24842e-04, 3.23013e-04, 8.72844e-01, 1.02598e+00, 1.91594e-01
Node: 00 (pos: 0.000): 6.21983e-02, 4.54427e-01, 4.81134e-01, 1.13083e+00, 1.15138e+00, 1.01703e+00
Node: 01 (pos: 0.010): 2.06356e-01, 6.75304e-01, 7.17450e-01, 1.14696e+00, 1.15867e+00, 1.11424e+00
Node: 02 (pos: 0.020): 4.86011e-01, 8.96153e-01, 9.54419e-01, 1.15862e+00, 1.16390e+00, 1.18932e+00
-
Node: 03 (pos: 0.030): 8.12575e-01, 1.06197e+00, 1.13268e+00, 1.16567e+00, 1.16705e+00, 1.23677e+00
Node: 05 (pos: 0.051): 8.12575e-01, 1.06197e+00, 1.13268e+00, 1.16567e+00, 1.16705e+00, 1.23677e+00
Node: 06 (pos: 0.061): 4.86011e-01, 8.96153e-01, 9.54419e-01, 1.15862e+00, 1.16390e+00, 1.18932e+00
Node: 07 (pos: 0.071): 2.06356e-01, 6.75304e-01, 7.17450e-01, 1.14696e+00, 1.15867e+00, 1.11424e+00
Node: 08 (pos: 0.081): 6.21983e-02, 4.54427e-01, 4.81134e-01, 1.13083e+00, 1.15138e+00, 1.01703e+00
Node: 09 (pos: 0.091): 1.33084e-02, 2.73071e-01, 2.87847e-01, 1.11043e+00, 1.14209e+00, 9.04400e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.33084e-02, 2.73071e-01, 2.87847e-01, 1.11043e+00, 1.14209e+00, 9.04400e-01
Node: 36 (pos: 0.364): 2.02146e-03, 1.46533e-01, 1.53631e-01, 1.08599e+00, 1.13082e+00, 7.83538e-01
Node: 37 (pos: 0.374): 2.17967e-04, 7.02166e-02, 7.31501e-02, 1.05780e+00, 1.11766e+00, 6.61351e-01
Node: 38 (pos: 0.384): 1.66842e-05, 3.00464e-02, 3.10723e-02, 1.02618e+00, 1.10265e+00, 5.43847e-01
Node: 39 (pos: 0.394): 9.06588e-07, 1.14813e-02, 1.17748e-02, 9.91490e-01, 1.08590e+00, 4.35707e-01
Node: 40 (pos: 0.404): 3.49705e-08, 3.91776e-03, 3.98066e-03, 9.54102e-01, 1.06747e+00, 3.40082e-01
-
Node: 41 (pos: 0.414): 9.57598e-10, 1.19380e-03, 1.20054e-03, 9.14417e-01, 1.04746e+00, 2.58611e-01
Node: 42 (pos: 0.424): 1.86146e-11, 3.24842e-04, 3.23013e-04, 8.72844e-01, 1.02598e+00, 1.91594e-01
Node: 19 (pos: 0.192): 9.57598e-10, 1.19380e-03, 1.20054e-03, 9.14417e-01, 1.04746e+00, 2.58611e-01
Node: 20 (pos: 0.202): 3.49705e-08, 3.91776e-03, 3.98066e-03, 9.54102e-01, 1.06747e+00, 3.40082e-01
Node: 21 (pos: 0.212): 9.06588e-07, 1.14813e-02, 1.17748e-02, 9.91490e-01, 1.08590e+00, 4.35707e-01
Node: 22 (pos: 0.222): 1.66842e-05, 3.00464e-02, 3.10723e-02, 1.02618e+00, 1.10265e+00, 5.43847e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03306611102030705
Step 50, mean loss 0.01358226345305285
Step 75, mean loss 0.020871055767477373
Step 100, mean loss 0.024392433749190947
Step 125, mean loss 0.040336650526856005
Step 150, mean loss 0.036505425271893266
Step 175, mean loss 0.060395694929896875
Step 200, mean loss 0.07338739755341646
Step 225, mean loss 0.08285430687696982
Unrolled forward losses 1.5052269824491193
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.109547738347387, [0.0006017099806921678, 0.0017950774936577173, 0.001792052742215981, 0.050436076211851075, 0.11324746543236504, 0.007810481643597528], [0.9644392556265915, 1.1238146745922908, 1.199220685469126, 1.1680426799659467, 1.1681243894630657, 1.2530311832679437]
Training Loss (progress: 0.08), 0.12641180150766262, [0.0006015874648345423, 0.0018198989363850029, 0.001787995694405235, 0.050358398033700034, 0.11336627922910947, 0.007849881989909465], [0.9643215224230377, 1.1239356331711285, 1.1991027284511655, 1.168166546239101, 1.1683564680690484, 1.253250682993909]
Training Loss (progress: 0.16), 0.12665078063985039, [0.0005992068391125159, 0.0018086168521180305, 0.001795072044893356, 0.05036049491603569, 0.11352272494446715, 0.007836335432883609], [0.964142307792491, 1.1238940559032378, 1.1990681072150273, 1.1682858001824885, 1.1686389355075097, 1.253516216676019]
Training Loss (progress: 0.24), 0.10934688555501919, [0.0005811824350857984, 0.0018012570990258757, 0.001791299186435964, 0.0503998322225933, 0.1136366798802362, 0.007809697432789432], [0.964041227550017, 1.1238822094835166, 1.1990784302994166, 1.16840768828377, 1.168892624704401, 1.253756456675215]
Training Loss (progress: 0.32), 0.1185916187446677, [0.000608725075188501, 0.0018107600895899225, 0.0017916811364639923, 0.05040189175257119, 0.11379401515942338, 0.007853340251466678], [0.9640266375958861, 1.1238319520688467, 1.1990514500988467, 1.16849719451055, 1.1691034773600366, 1.254052158563395]
Training Loss (progress: 0.40), 0.12061919048418858, [0.000598811554901693, 0.0017913012570650551, 0.0018091393858758948, 0.05053947381038033, 0.11397258961745753, 0.007849110168048266], [0.9639495957241555, 1.1238135358627286, 1.199058466081064, 1.168720765813644, 1.1694330362923784, 1.2543463536403396]
Training Loss (progress: 0.48), 0.11858995437784435, [0.0006053884396754561, 0.0018226240258460116, 0.0017916433740339862, 0.050594516546827714, 0.11398867371282827, 0.007859578119842992], [0.9639031399101642, 1.123823086889546, 1.198980245651761, 1.1688795905669722, 1.169486291165785, 1.2544931672452477]
Training Loss (progress: 0.56), 0.11464985442989414, [0.0006102541369929781, 0.0018227869119519212, 0.0018127886202914817, 0.050670225007456555, 0.11416088722336944, 0.00786026534327671], [0.9638166178382014, 1.1237416876422126, 1.198960434544553, 1.1690250560775899, 1.1697846418020956, 1.2547360234293743]
Training Loss (progress: 0.64), 0.1109638487758326, [0.0006077874218285531, 0.0018240406009379434, 0.001792755610107782, 0.05063969365154535, 0.11435880065148421, 0.007838148005483986], [0.963793433251194, 1.123720260102895, 1.198923219834503, 1.1690801150803969, 1.170061301873073, 1.255049953352501]
Training Loss (progress: 0.72), 0.12374318056525607, [0.000591118851780585, 0.0018030047657262838, 0.0018057408601070669, 0.05070282507620678, 0.11442433622914323, 0.007844842369222615], [0.9636239030425716, 1.1236769137441063, 1.198808559964651, 1.1692399415720578, 1.170247897919799, 1.2552467515506829]
Training Loss (progress: 0.80), 0.12155034769288932, [0.0005872858393789136, 0.001806445154993532, 0.00181615939250467, 0.050785851757875394, 0.11458715236805316, 0.007860739448677033], [0.9634765091254154, 1.123664205046115, 1.1987924439659607, 1.1694183166457106, 1.17056398675527, 1.2556028181333059]
Training Loss (progress: 0.88), 0.12229446253581341, [0.0006017437863424629, 0.0018012398765467153, 0.0018016863034364318, 0.050844159058361885, 0.1147428293263679, 0.007864930031803585], [0.9634843554806153, 1.1237071445428095, 1.1987535541300187, 1.169526571257283, 1.170798103911127, 1.2557780535124272]
Training Loss (progress: 0.96), 0.10649600874802502, [0.0006037539213715299, 0.0017977996522893203, 0.001818335359559711, 0.050823171542121744, 0.11493025225076907, 0.007861475403344089], [0.9634760925479441, 1.1238040512914074, 1.1986996361589166, 1.169607445625651, 1.1710356630431584, 1.2559676480327842]
Evaluation on validation dataset:
Step 25, mean loss 0.03892645927039701
Step 50, mean loss 0.017472686504290492
Step 75, mean loss 0.028390297283564084
Step 100, mean loss 0.03294936417781816
Step 125, mean loss 0.036275118106498645
Step 150, mean loss 0.0376215021888476
Step 175, mean loss 0.06222756700791566
Step 200, mean loss 0.20691450442784026
Step 225, mean loss 0.16601715369715075
Unrolled forward losses 1.6226494825290225
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 3.13345e-08, 3.89272e-03, 4.23058e-03, 9.56804e-01, 1.07159e+00, 3.41780e-01
Node: 14 (pos: 0.141): 8.38672e-10, 1.18458e-03, 1.29245e-03, 9.17290e-01, 1.05179e+00, 2.60038e-01
Node: 15 (pos: 0.152): 1.59003e-11, 3.21857e-04, 3.52681e-04, 8.75883e-01, 1.03053e+00, 1.92762e-01
Node: 00 (pos: 0.000): 6.10585e-02, 4.53960e-01, 4.85674e-01, 1.13263e+00, 1.15455e+00, 1.01996e+00
Node: 01 (pos: 0.010): 2.04122e-01, 6.74913e-01, 7.21118e-01, 1.14866e+00, 1.16175e+00, 1.11726e+00
Node: 02 (pos: 0.020): 4.83368e-01, 8.95920e-01, 9.56362e-01, 1.16026e+00, 1.16692e+00, 1.19239e+00
-
Node: 03 (pos: 0.030): 8.10795e-01, 1.06190e+00, 1.13290e+00, 1.16727e+00, 1.17003e+00, 1.23987e+00
Node: 05 (pos: 0.051): 8.10795e-01, 1.06190e+00, 1.13290e+00, 1.16727e+00, 1.17003e+00, 1.23987e+00
Node: 06 (pos: 0.061): 4.83368e-01, 8.95920e-01, 9.56362e-01, 1.16026e+00, 1.16692e+00, 1.19239e+00
Node: 07 (pos: 0.071): 2.04122e-01, 6.74913e-01, 7.21118e-01, 1.14866e+00, 1.16175e+00, 1.11726e+00
Node: 08 (pos: 0.081): 6.10585e-02, 4.53960e-01, 4.85674e-01, 1.13263e+00, 1.15455e+00, 1.01996e+00
Node: 09 (pos: 0.091): 1.29374e-02, 2.72633e-01, 2.92170e-01, 1.11234e+00, 1.14536e+00, 9.07212e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.29374e-02, 2.72633e-01, 2.92170e-01, 1.11234e+00, 1.14536e+00, 9.07212e-01
Node: 36 (pos: 0.364): 1.94175e-03, 1.46194e-01, 1.56994e-01, 1.08804e+00, 1.13423e+00, 7.86190e-01
Node: 37 (pos: 0.374): 2.06435e-04, 6.99962e-02, 7.53496e-02, 1.06000e+00, 1.12122e+00, 6.63805e-01
Node: 38 (pos: 0.384): 1.55460e-05, 2.99233e-02, 3.23023e-02, 1.02854e+00, 1.10639e+00, 5.46070e-01
Node: 39 (pos: 0.394): 8.29275e-07, 1.14218e-02, 1.23692e-02, 9.94019e-01, 1.08981e+00, 4.37674e-01
Node: 40 (pos: 0.404): 3.13345e-08, 3.89272e-03, 4.23058e-03, 9.56804e-01, 1.07159e+00, 3.41780e-01
-
Node: 41 (pos: 0.414): 8.38672e-10, 1.18458e-03, 1.29245e-03, 9.17290e-01, 1.05179e+00, 2.60038e-01
Node: 42 (pos: 0.424): 1.59003e-11, 3.21857e-04, 3.52681e-04, 8.75883e-01, 1.03053e+00, 1.92762e-01
Node: 19 (pos: 0.192): 8.38672e-10, 1.18458e-03, 1.29245e-03, 9.17290e-01, 1.05179e+00, 2.60038e-01
Node: 20 (pos: 0.202): 3.13345e-08, 3.89272e-03, 4.23058e-03, 9.56804e-01, 1.07159e+00, 3.41780e-01
Node: 21 (pos: 0.212): 8.29275e-07, 1.14218e-02, 1.23692e-02, 9.94019e-01, 1.08981e+00, 4.37674e-01
Node: 22 (pos: 0.222): 1.55460e-05, 2.99233e-02, 3.23023e-02, 1.02854e+00, 1.10639e+00, 5.46070e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03447195378809092
Step 50, mean loss 0.013798763609765768
Step 75, mean loss 0.02010009794518062
Step 100, mean loss 0.024103213026658944
Step 125, mean loss 0.041620915969568066
Step 150, mean loss 0.0383700305193114
Step 175, mean loss 0.06136121262078756
Step 200, mean loss 0.07486745421712349
Step 225, mean loss 0.08470434231316763
Unrolled forward losses 1.4682520219571078
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.11643781056049532, [0.0005862641318306954, 0.001803806478717579, 0.0018012344990159539, 0.05081966408536462, 0.1149435172475443, 0.007841490757795017], [0.9633543225183681, 1.1237893416186933, 1.1986925017597452, 1.1696219157942955, 1.1710959581048133, 1.2561605972735828]
Training Loss (progress: 0.08), 0.11334531661511338, [0.0006093311831611289, 0.0018075942116059355, 0.00180821779342392, 0.050842053057783484, 0.11510023816771846, 0.007866142792958514], [0.9631567188684849, 1.1237009268819997, 1.1986535011660042, 1.169730226082868, 1.1713186104690798, 1.2563754531364388]
Training Loss (progress: 0.16), 0.11437763708011071, [0.0006042074581042161, 0.0018429468539405368, 0.0018012207267183958, 0.050884712852514745, 0.11529472713556395, 0.00786368561571954], [0.9631911399713154, 1.1237872824818573, 1.1986056624398005, 1.1699177483564873, 1.1715673287481752, 1.2565695551436695]
Training Loss (progress: 0.24), 0.1112666515697453, [0.0006036001529395295, 0.0017952528189235678, 0.0017962337164623002, 0.05090289304524704, 0.11538899822863316, 0.007834005114363099], [0.96316866083908, 1.1236892505383342, 1.1985717649503913, 1.1700562845468403, 1.1718334296264372, 1.2568023876565226]
Training Loss (progress: 0.32), 0.12224549017553037, [0.000605968995775673, 0.001787656389230111, 0.0018267204565962064, 0.05102799434224236, 0.11553771239296118, 0.007837581417170265], [0.9630237260960564, 1.1236214097212067, 1.1985037646185805, 1.1702360398153169, 1.1720605525253711, 1.2571381892663729]
Training Loss (progress: 0.40), 0.1231888589968087, [0.0005985714536600742, 0.0018306800634316687, 0.0017962884912371697, 0.05104268631888046, 0.1157409554086344, 0.007884244160226867], [0.9629461516810777, 1.1235593301184479, 1.1984694121469273, 1.1703401125404533, 1.1724226084866027, 1.257489137799974]
Training Loss (progress: 0.48), 0.11031817404038359, [0.0006039644147807938, 0.0018477433251489063, 0.0018053727611148014, 0.05101601141342349, 0.11588066814100666, 0.00784656401554978], [0.9628282952187969, 1.123544934398253, 1.1984265843745898, 1.1703858209371363, 1.1726553120210368, 1.2574930965669566]
Training Loss (progress: 0.56), 0.11838563783922168, [0.0006067294005483706, 0.0018344496985675204, 0.0017949839636738853, 0.05106824508872209, 0.11608770113289392, 0.007863531168488638], [0.9627753793279181, 1.123681853078267, 1.1984155846886513, 1.1705584787917551, 1.1729337275031841, 1.257782100798552]
Training Loss (progress: 0.64), 0.12153063864869038, [0.000610091308677835, 0.0018288397763586432, 0.001797990566953429, 0.0511518147117105, 0.11618925292087195, 0.007864215139438267], [0.9626340309359602, 1.1236499550459043, 1.1983481972844496, 1.170736496246296, 1.1731156760054513, 1.2581299163287119]
Training Loss (progress: 0.72), 0.11521495901179096, [0.0006156860426509671, 0.001820336750231348, 0.00179641962409492, 0.05118420321408669, 0.11634567561517935, 0.007848503793903444], [0.962635266861858, 1.1236562091408027, 1.198385339476585, 1.1708544733127133, 1.1733594584992968, 1.2583745506709016]
Training Loss (progress: 0.80), 0.0983329763354992, [0.000592845791830726, 0.0018194200244642659, 0.0018003113568630588, 0.051180683493993774, 0.11644130110745883, 0.00782426621903788], [0.9625490160405795, 1.1236902762033991, 1.1982663402859604, 1.170885763843771, 1.1735290694303477, 1.2585154317671514]
Training Loss (progress: 0.88), 0.10531035435057166, [0.000586513088473305, 0.0018054168187281979, 0.0018058019580171425, 0.051189450769037505, 0.1164875431708737, 0.007874277523194155], [0.9624695594862362, 1.1236633630091712, 1.198198676411401, 1.1709499400034042, 1.1737238907771173, 1.258907011081199]
Training Loss (progress: 0.96), 0.11507471205511521, [0.0005993923870919429, 0.0018037273293580497, 0.0018283017793835077, 0.05125075410200252, 0.11666647278029416, 0.007829277936093173], [0.9623878082608843, 1.1235678052048268, 1.1982246896148705, 1.1711220921603416, 1.174042789420327, 1.2591582827581125]
Evaluation on validation dataset:
Step 25, mean loss 0.03578094079868993
Step 50, mean loss 0.019168332090294577
Step 75, mean loss 0.027196642379955265
Step 100, mean loss 0.03459650333692082
Step 125, mean loss 0.03748745512294947
Step 150, mean loss 0.03697217227028611
Step 175, mean loss 0.059784797617201735
Step 200, mean loss 0.19516687616362258
Step 225, mean loss 0.1682706569370645
Unrolled forward losses 1.6912317509311623
Unrolled forward base losses 2.565701273852575
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00), 0.10922207996831208, [0.0006128236882837028, 0.0018372422654042526, 0.0018077543878937965, 0.05130778244714147, 0.11673365893357199, 0.007838083883360217], [0.9624080735973779, 1.1235829761399243, 1.1981851383120083, 1.1712102035618157, 1.1740984497967732, 1.2592834894039673]
Training Loss (progress: 0.08), 0.11112288087094176, [0.0006084147675832223, 0.0018326015298199374, 0.0018155541234045399, 0.051396393975486204, 0.11684116239694355, 0.00785241417718024], [0.9622790815288893, 1.1235258008899696, 1.1981635444124343, 1.1713319905483999, 1.1743053831564902, 1.2595093043647203]
Training Loss (progress: 0.16), 0.11240704318657516, [0.00062146209432248, 0.0018300454878228244, 0.0018012020229040482, 0.051410949454810166, 0.11696203284119007, 0.007848701509311462], [0.962245006450274, 1.12349189072833, 1.1981047731907246, 1.1714317894806259, 1.174576312279295, 1.259785097706504]
Training Loss (progress: 0.24), 0.11131798307504727, [0.0006014825459812665, 0.0018624913503245703, 0.0018167547282146126, 0.05145824019204155, 0.11715011834240927, 0.0078743656869273], [0.9621279544249428, 1.123530750340652, 1.1981001376056484, 1.1715997849181783, 1.1748084634464893, 1.2600137067098838]
Training Loss (progress: 0.32), 0.1122137671085195, [0.0006042329938326838, 0.0018576864259898474, 0.0018263908219691405, 0.05155646470938696, 0.11735890178096178, 0.007858922599571492], [0.9621236086389181, 1.1235667020266757, 1.1980236786451335, 1.1718055085510348, 1.1750778510977031, 1.2602233070901279]
Training Loss (progress: 0.40), 0.11145607733652393, [0.0006141713157491048, 0.001825726639690074, 0.0018216803925058258, 0.05152563336324526, 0.11738834982633062, 0.007860830898488401], [0.962128378277484, 1.1236150370140463, 1.1979670263256958, 1.1718693346592723, 1.1752978891195554, 1.2604045279499962]
Training Loss (progress: 0.48), 0.11644598858634664, [0.0005915850264251339, 0.0018309058894645637, 0.0018059732989818545, 0.05160495473869676, 0.11757641098249673, 0.007910795924068502], [0.9619321546872263, 1.1235859790580736, 1.19785039918882, 1.1719710596239727, 1.1754856367028441, 1.2607294749310503]
Training Loss (progress: 0.56), 0.10962357091066838, [0.0006068434286754754, 0.0018328318442621596, 0.00182144151524117, 0.05164792764443794, 0.11779816963089704, 0.007853996017312832], [0.9619559783830824, 1.12346287356798, 1.1978662654254875, 1.1720580237716627, 1.1757711809371452, 1.2608520128364418]
Training Loss (progress: 0.64), 0.11101636451434933, [0.0005964416736557201, 0.00183395576941225, 0.0018113703575652613, 0.05169263800939801, 0.11801087691589286, 0.007868797660219853], [0.9618538531930115, 1.1234953867483424, 1.1978132259790206, 1.172160688613262, 1.176061467829578, 1.261066449570933]
Training Loss (progress: 0.72), 0.1126846943807629, [0.0005920939507008901, 0.0018556020936954588, 0.0018304581151017698, 0.0517251950609263, 0.11820123162728244, 0.007876314984780128], [0.961743210351511, 1.1235262335334555, 1.1978264975719493, 1.1722923790384834, 1.1763178323231407, 1.261358761892506]
Training Loss (progress: 0.80), 0.11344226960305609, [0.000600646064315505, 0.0018256841440636555, 0.0018122483026671522, 0.05177820800397565, 0.1183806063032945, 0.007898478074594617], [0.961590789167494, 1.1234980073094079, 1.197788865290575, 1.1724089200846701, 1.1766251365445335, 1.2615717129072788]
Training Loss (progress: 0.88), 0.10551980752796404, [0.0005882762532118646, 0.001844604598149014, 0.0018068609703810583, 0.05185153059974496, 0.11852943065644506, 0.007868661177854654], [0.9615726656563273, 1.123481499510134, 1.1977536801200142, 1.1725449171363485, 1.1768701935498476, 1.2618369425220237]
Training Loss (progress: 0.96), 0.11304262760316021, [0.0006054764419553875, 0.001854037495778464, 0.0018022081967196092, 0.051946216956184676, 0.11861547708831394, 0.00787938221113058], [0.9615859465297567, 1.1234739148897481, 1.1977169506531409, 1.1727427103188732, 1.1770843557376596, 1.2620130546581088]
Evaluation on validation dataset:
Step 25, mean loss 0.038218859345738626
Step 50, mean loss 0.01657817755098822
Step 75, mean loss 0.02886625937137129
Step 100, mean loss 0.031768841081838126
Step 125, mean loss 0.03664615991340399
Step 150, mean loss 0.036515849158768474
Step 175, mean loss 0.060203712833017446
Step 200, mean loss 0.19122789108049612
Step 225, mean loss 0.1727906483605542
Unrolled forward losses 1.668777086403924
Unrolled forward base losses 2.565701273852575
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00), 0.1112327263307057, [0.0006037402922917506, 0.0018354480052233458, 0.0018129536159413313, 0.05201382108077261, 0.11873677652193973, 0.007900776424215293], [0.9615634483716883, 1.1234370088796346, 1.1976387968695554, 1.1728192576309522, 1.1772429690090056, 1.262122233458874]
Training Loss (progress: 0.08), 0.1106807394074977, [0.0005986771883126618, 0.0018401499550378894, 0.0018050383512445906, 0.0520220393918361, 0.11881655870750886, 0.007897427138447077], [0.9614792121590304, 1.1234524345804495, 1.1976577192084124, 1.1728665011331485, 1.1773797510245643, 1.2622769106847653]
Training Loss (progress: 0.16), 0.10242687094636435, [0.0006036468345346487, 0.0018355897467392968, 0.0018107822988126473, 0.05203540269867176, 0.11889919707211412, 0.007906458513666989], [0.961409305017991, 1.123482901992956, 1.1976205184847022, 1.1729170191860416, 1.1775073316036846, 1.2623762868461894]
Training Loss (progress: 0.24), 0.10548037382977885, [0.0006040468163041453, 0.0018511585179559558, 0.0018165331467687588, 0.05201682234270466, 0.11899764966151573, 0.007895414867949622], [0.9614207416761075, 1.1234740534041792, 1.1975896572144111, 1.1729489021158255, 1.1776523129196848, 1.2625054855071316]
Training Loss (progress: 0.32), 0.11333079883439164, [0.0006092191671005507, 0.001837733227591929, 0.0018129579169554977, 0.052010452158861624, 0.11905374104494001, 0.0078889405395723], [0.9614558240023786, 1.123512471491108, 1.1975894150718005, 1.1730038791857733, 1.177736600894936, 1.2625902727871479]
Training Loss (progress: 0.40), 0.10617755362569299, [0.0006001236946329155, 0.0018350343975007336, 0.0018115343367371651, 0.05205333397806597, 0.11910613991432176, 0.007925053798865558], [0.9614020366782974, 1.1235249030345804, 1.1975925489195536, 1.1730846488125055, 1.1778401323033656, 1.2627858088179933]
Training Loss (progress: 0.48), 0.11172835937100693, [0.0005976622325117343, 0.001837510868159241, 0.0018264258540111073, 0.05207046779760533, 0.119097045470337, 0.00790945560212342], [0.9614039278092814, 1.1235188512755485, 1.1975610240100794, 1.1731609497901765, 1.177885291028503, 1.2628862272674017]
Training Loss (progress: 0.56), 0.1093775447523937, [0.0006020434365274765, 0.001844906603144831, 0.0018073652425841054, 0.052112623953593326, 0.11917050781668329, 0.007900638192401378], [0.961352966794055, 1.1235079145052431, 1.1975327850019188, 1.173243238608744, 1.1780168000355449, 1.2630465293806055]
Training Loss (progress: 0.64), 0.10043951447999275, [0.0006035949368735772, 0.0018431702115357523, 0.0018195324098866856, 0.05214659041110404, 0.11922898737410388, 0.007893042370700607], [0.9613479987845525, 1.1235280903856708, 1.197521888245268, 1.1733314989828207, 1.1781378439040087, 1.263130134044423]
Training Loss (progress: 0.72), 0.1044733642929528, [0.0005966866913073578, 0.0018484447753437233, 0.0018271480202242817, 0.05212339455210703, 0.11927136880092638, 0.007928155180338669], [0.961298032806193, 1.1235470529751748, 1.1975255546425305, 1.1733946166409963, 1.1782544828512709, 1.2633039201659917]
Training Loss (progress: 0.80), 0.10097010652008588, [0.0006001179566036581, 0.0018574323881850611, 0.0018170764307469216, 0.052157582572002696, 0.11938721774962506, 0.007900994984457324], [0.9612445623944974, 1.1235701546285293, 1.1975018503876602, 1.1734419451513032, 1.178430118904668, 1.2634575134114514]
Training Loss (progress: 0.88), 0.11596611629061522, [0.0005995666198692719, 0.0018450815705842057, 0.001817745208399052, 0.052200033041847234, 0.11941517014936269, 0.007896760819176674], [0.9612008648918462, 1.123525784780724, 1.1974687160661537, 1.1735561530704863, 1.1785411585562258, 1.2635586262174552]
Training Loss (progress: 0.96), 0.10972535848120486, [0.0005959452630106839, 0.0018482694347490153, 0.0018200896152569246, 0.05218269980323577, 0.11947135601834391, 0.007896039109967728], [0.9612044201693246, 1.123533908207181, 1.1974644505494083, 1.1735980485372635, 1.1786314928186663, 1.2636416011434461]
Evaluation on validation dataset:
Step 25, mean loss 0.0356460173165017
Step 50, mean loss 0.015821312967396698
Step 75, mean loss 0.027443264712031602
Step 100, mean loss 0.03216115082995159
Step 125, mean loss 0.036151051698114695
Step 150, mean loss 0.036335976474967974
Step 175, mean loss 0.05802251879380484
Step 200, mean loss 0.1900339060023341
Step 225, mean loss 0.16555072840450719
Unrolled forward losses 1.6404169503480568
Unrolled forward base losses 2.565701273852575
Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00), 0.10554643622883675, [0.0005997803008294295, 0.0018501800842812524, 0.0018190823850002223, 0.05215551960596482, 0.11953518325605053, 0.007894383853394712], [0.9612047137602006, 1.12352413981111, 1.197447835714811, 1.1735899013686932, 1.1787088154167475, 1.2637136618614124]
Training Loss (progress: 0.08), 0.11394100564359684, [0.0005960855657786884, 0.0018550303444323742, 0.0018173501274431378, 0.05221878160088717, 0.11962771457287587, 0.007915234677219081], [0.9611370581147138, 1.1235261969191355, 1.1974477391284688, 1.173717928624884, 1.1788131366941856, 1.2638470404629074]
Training Loss (progress: 0.16), 0.10593486472146195, [0.0005953811392089997, 0.0018272810860360995, 0.0018166633352840163, 0.05225046598307223, 0.1196848087879188, 0.007905464802707153], [0.961129440956171, 1.1235193908160812, 1.197443234135539, 1.173795879654412, 1.1789338935641431, 1.26397569251989]
Training Loss (progress: 0.24), 0.11412970027943327, [0.0006061335261069304, 0.0018301516451358134, 0.0018208499346947, 0.05222459923667892, 0.11976026413395086, 0.007920347345909831], [0.9611503305980295, 1.1235349259529772, 1.1974032284837481, 1.1738377750808178, 1.179080604490861, 1.2640967435436707]
Training Loss (progress: 0.32), 0.1051739253755553, [0.0006085944880893652, 0.0018311343651623627, 0.0018062652804866671, 0.05226539623667485, 0.11983440395058169, 0.007918761268215566], [0.9610984594307199, 1.1235350919123466, 1.197383437414356, 1.1739255540333988, 1.1792429316973334, 1.2641985391831547]
Training Loss (progress: 0.40), 0.11661600336913623, [0.0005914045406768887, 0.0018466809961602475, 0.0018218573845353507, 0.05226132247000103, 0.11997191524750912, 0.007928492972867268], [0.9610238723738045, 1.1235212014482454, 1.1973927639423354, 1.173970194205723, 1.1793862300506095, 1.264305421391554]
Training Loss (progress: 0.48), 0.10442062128400241, [0.0005968657030272998, 0.0018407998747073757, 0.0018269826830291077, 0.052297704903020416, 0.11998432473044315, 0.007926476218575961], [0.9610210084024178, 1.1235668868754916, 1.1973834510831511, 1.1740637181614557, 1.1794318582920353, 1.2644207047922165]
Training Loss (progress: 0.56), 0.1047184864552791, [0.0005986030175138855, 0.0018516727279663397, 0.001825838416591584, 0.052305865011172056, 0.12003753541516998, 0.007926032706677895], [0.9610009435619953, 1.1235633360355537, 1.1973421644025177, 1.174132681142868, 1.1795479826995205, 1.264588881064052]
Training Loss (progress: 0.64), 0.10752437323951272, [0.0005998943678414644, 0.001858977379802823, 0.0018128246681540356, 0.05231825588356909, 0.12012765277516406, 0.007929425258569205], [0.9610081779002798, 1.123577249307526, 1.1973083957619621, 1.174185039006359, 1.1797097803081296, 1.2646984239747256]
Training Loss (progress: 0.72), 0.10804659884132257, [0.0006080179863402772, 0.0018488829284819329, 0.001818473142474164, 0.052335799230288436, 0.1202229921097932, 0.00793218085732388], [0.9609943783774668, 1.123594206573721, 1.1973309384648403, 1.1742885442014674, 1.1798634163286257, 1.2648331375696564]
Training Loss (progress: 0.80), 0.10451879434960035, [0.000597957021761685, 0.0018300143032578705, 0.0018230344741428216, 0.05229152626970981, 0.12028011835760437, 0.007904477827703977], [0.9610189122015776, 1.1235635032295554, 1.197304241430035, 1.174287657776656, 1.179956121100044, 1.2649057635145744]
Training Loss (progress: 0.88), 0.10334589255022707, [0.0006062529285966578, 0.0018561989997286498, 0.0018229303068570686, 0.052304726755569325, 0.12031228604918122, 0.00790294413766801], [0.9609687345311825, 1.1235643025957356, 1.1973040717299266, 1.1743563678084823, 1.1800300865687543, 1.2650105729852992]
Training Loss (progress: 0.96), 0.10877161657098287, [0.0006014792908519778, 0.0018405982265380868, 0.0018267824236922424, 0.052351119575417264, 0.12039682140594586, 0.007932838559095233], [0.9609877178758955, 1.1235650147237786, 1.197290614474934, 1.1744296950342599, 1.1801414798929097, 1.2651927351827494]
Evaluation on validation dataset:
Step 25, mean loss 0.03246155645791972
Step 50, mean loss 0.01576195734615308
Step 75, mean loss 0.026714846482393303
Step 100, mean loss 0.03243058359985699
Step 125, mean loss 0.03530713079309203
Step 150, mean loss 0.036516694801279406
Step 175, mean loss 0.06068182217467365
Step 200, mean loss 0.1932936099163193
Step 225, mean loss 0.16920063398487856
Unrolled forward losses 1.587539441006144
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 13 (pos: 0.131): 4.78503e-08, 4.64014e-03, 4.54045e-03, 9.66566e-01, 1.08434e+00, 3.49705e-01
Node: 14 (pos: 0.141): 1.40055e-09, 1.46513e-03, 1.40822e-03, 9.27820e-01, 1.06522e+00, 2.66945e-01
Node: 15 (pos: 0.152): 2.92857e-11, 4.14512e-04, 3.90678e-04, 8.87164e-01, 1.04466e+00, 1.98597e-01
Node: 00 (pos: 0.000): 6.51975e-02, 4.66828e-01, 4.90698e-01, 1.13842e+00, 1.16432e+00, 1.02996e+00
Node: 01 (pos: 0.010): 2.11557e-01, 6.85554e-01, 7.24921e-01, 1.15405e+00, 1.17124e+00, 1.12697e+00
Node: 02 (pos: 0.020): 4.90418e-01, 9.02081e-01, 9.57956e-01, 1.16535e+00, 1.17622e+00, 1.20181e+00
-
Node: 03 (pos: 0.030): 8.12176e-01, 1.06357e+00, 1.13234e+00, 1.17218e+00, 1.17921e+00, 1.24908e+00
Node: 05 (pos: 0.051): 8.12176e-01, 1.06357e+00, 1.13234e+00, 1.17218e+00, 1.17921e+00, 1.24908e+00
Node: 06 (pos: 0.061): 4.90418e-01, 9.02081e-01, 9.57956e-01, 1.16535e+00, 1.17622e+00, 1.20181e+00
Node: 07 (pos: 0.071): 2.11557e-01, 6.85554e-01, 7.24921e-01, 1.15405e+00, 1.17124e+00, 1.12697e+00
Node: 08 (pos: 0.081): 6.51975e-02, 4.66828e-01, 4.90698e-01, 1.13842e+00, 1.16432e+00, 1.02996e+00
Node: 09 (pos: 0.091): 1.43542e-02, 2.84832e-01, 2.97109e-01, 1.11863e+00, 1.15547e+00, 9.17397e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 35 (pos: 0.354): 1.43542e-02, 2.84832e-01, 2.97109e-01, 1.11863e+00, 1.15547e+00, 9.17397e-01
Node: 36 (pos: 0.364): 2.25773e-03, 1.55719e-01, 1.60915e-01, 1.09492e+00, 1.14476e+00, 7.96389e-01
Node: 37 (pos: 0.374): 2.53693e-04, 7.62800e-02, 7.79569e-02, 1.06753e+00, 1.13222e+00, 6.73788e-01
Node: 38 (pos: 0.384): 2.03653e-05, 3.34811e-02, 3.37825e-02, 1.03679e+00, 1.11792e+00, 5.55587e-01
Node: 39 (pos: 0.394): 1.16793e-06, 1.31676e-02, 1.30950e-02, 1.00301e+00, 1.10193e+00, 4.46490e-01
Node: 40 (pos: 0.404): 4.78503e-08, 4.64014e-03, 4.54045e-03, 9.66566e-01, 1.08434e+00, 3.49705e-01
-
Node: 41 (pos: 0.414): 1.40055e-09, 1.46513e-03, 1.40822e-03, 9.27820e-01, 1.06522e+00, 2.66945e-01
Node: 42 (pos: 0.424): 2.92857e-11, 4.14512e-04, 3.90678e-04, 8.87164e-01, 1.04466e+00, 1.98597e-01
Node: 19 (pos: 0.192): 1.40055e-09, 1.46513e-03, 1.40822e-03, 9.27820e-01, 1.06522e+00, 2.66945e-01
Node: 20 (pos: 0.202): 4.78503e-08, 4.64014e-03, 4.54045e-03, 9.66566e-01, 1.08434e+00, 3.49705e-01
Node: 21 (pos: 0.212): 1.16793e-06, 1.31676e-02, 1.30950e-02, 1.00301e+00, 1.10193e+00, 4.46490e-01
Node: 22 (pos: 0.222): 2.03653e-05, 3.34811e-02, 3.37825e-02, 1.03679e+00, 1.11792e+00, 5.55587e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.029584922705082523
Step 50, mean loss 0.01242753960507448
Step 75, mean loss 0.018750447324997168
Step 100, mean loss 0.023049044359716907
Step 125, mean loss 0.04045229734776546
Step 150, mean loss 0.036042801952808366
Step 175, mean loss 0.05990300359055829
Step 200, mean loss 0.07151765573272793
Step 225, mean loss 0.08446057703628432
Unrolled forward losses 1.4107491003154171
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n12_s0.001_tw25_unrolling2_time52119.tar

Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00), 0.10344069799663107, [0.0006073382317409413, 0.001846703380766471, 0.001822955154818285, 0.052370064730085965, 0.12044615283307102, 0.007930662611059704], [0.9609131267402253, 1.1235826369322288, 1.197266011922719, 1.174468352221276, 1.1802364695775454, 1.265258506349755]
Training Loss (progress: 0.08), 0.09149681405739424, [0.0005985236187141929, 0.0018291951063838564, 0.0018227916036076361, 0.052363311472990226, 0.12052948668421626, 0.007923006009371172], [0.9608734093226653, 1.1235855647726813, 1.197215015023883, 1.1745039103433152, 1.1803505845166569, 1.2653517197423247]
Training Loss (progress: 0.16), 0.10906345085087682, [0.0005964689579295634, 0.0018363128339723461, 0.0018301611337032378, 0.052424208109257124, 0.12061172266873534, 0.007917723364656433], [0.9608307054971674, 1.1236075628363567, 1.1972288185708693, 1.174606362622664, 1.1804932158632937, 1.265452535108603]
Training Loss (progress: 0.24), 0.10332272254227957, [0.0005987512762625155, 0.0018489573648866877, 0.001822127368509804, 0.05243206374642082, 0.1207176608670083, 0.007914957486205543], [0.960833740816083, 1.1235671041131003, 1.1972370129112995, 1.1746708872518468, 1.1806469374328667, 1.2655650336823354]
Training Loss (progress: 0.32), 0.10357528375955523, [0.0006005203937828017, 0.0018512927300850298, 0.0018321784962558778, 0.052449761488270345, 0.12071529359446863, 0.007930032338722132], [0.9608094702392184, 1.1235821480479355, 1.19720490003564, 1.174754561039225, 1.1807321977844745, 1.2656964777912803]
Training Loss (progress: 0.40), 0.11139600627687046, [0.0006055025435833046, 0.0018498082259965443, 0.0018271722153950984, 0.05246509622798604, 0.1208058356540538, 0.007934902825158359], [0.9607976472275502, 1.1235446319724873, 1.19718255159311, 1.174796286219644, 1.180843556666928, 1.2657949700164508]
Training Loss (progress: 0.48), 0.10990227080735725, [0.0006047882648994738, 0.0018487058439168923, 0.0018278684363082058, 0.05243020678886032, 0.12088074224574957, 0.007916632865413062], [0.9607449131987579, 1.1235425774374663, 1.1971270155093214, 1.1748395839587364, 1.1809655269354773, 1.26590686609352]
Training Loss (progress: 0.56), 0.09257849171330759, [0.0005997236868037667, 0.001856355886952344, 0.0018326311014882862, 0.05245409375696746, 0.12092394700489897, 0.00790040436825077], [0.9606901135146396, 1.123523762511492, 1.1971184785212132, 1.174906631368206, 1.1810686532159596, 1.2659646264172315]
Training Loss (progress: 0.64), 0.10593320736026156, [0.000599779819276697, 0.0018595712832321879, 0.00182473594345867, 0.05248758522375427, 0.12098249396325127, 0.007934703689156108], [0.9606514477677209, 1.123567036060153, 1.1970934840514031, 1.1749745681658472, 1.1811715003864403, 1.26611356204252]
Training Loss (progress: 0.72), 0.09880500181828933, [0.0006058095314500272, 0.0018350361887890245, 0.001827095598112926, 0.05251844512197155, 0.12109575590590217, 0.007934381216655568], [0.9606636411733744, 1.123539488479655, 1.1970835931578643, 1.1750531628001972, 1.1813082094559153, 1.2662409166219317]
Training Loss (progress: 0.80), 0.1039064531105929, [0.0006158356661289716, 0.0018557630725482064, 0.0018137179707804123, 0.052538995480837306, 0.12115823023026144, 0.007933476942947772], [0.9606234939497266, 1.123546294460408, 1.197090772026226, 1.1751258206674053, 1.1814450926899274, 1.2663773137207595]
Training Loss (progress: 0.88), 0.10792110671652504, [0.0005950899538947582, 0.0018396229376505396, 0.0018298926220770737, 0.052521348170322585, 0.12115092827356777, 0.007941134179535061], [0.9606350777567426, 1.1235172739080312, 1.197061539997545, 1.1751276323708002, 1.1815013096952824, 1.2664209287153605]
Training Loss (progress: 0.96), 0.10511060528181157, [0.0005996451899690071, 0.001842799688184997, 0.0018392967349069764, 0.05254148877064413, 0.1212203957701471, 0.007932958218369337], [0.9605402554619891, 1.123531938857733, 1.197100905474436, 1.1752009499939133, 1.1815929677726216, 1.2664892369807421]
Evaluation on validation dataset:
Step 25, mean loss 0.03328943536958705
Step 50, mean loss 0.015780764841023927
Step 75, mean loss 0.026883026179139173
Step 100, mean loss 0.031842474597837546
Step 125, mean loss 0.03524028153518423
Step 150, mean loss 0.036005888055442754
Step 175, mean loss 0.05958239257141242
Step 200, mean loss 0.18961390400340694
Step 225, mean loss 0.17098874577969217
Unrolled forward losses 1.6113844396010943
Unrolled forward base losses 2.565701273852575
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00), 0.11620122233115232, [0.0005984401166878862, 0.0018601975673670362, 0.0018213979448858224, 0.05258028462692201, 0.12130315721637289, 0.007934632992623428], [0.9605325257336652, 1.12356193368188, 1.1971210191898531, 1.1752533853290905, 1.1817020479105558, 1.2665441442925438]
Training Loss (progress: 0.08), 0.11250068062843545, [0.0006031932288603398, 0.0018575652238604706, 0.0018292494743236774, 0.05258626988365255, 0.1213636792452275, 0.007943286054430966], [0.9605067289520235, 1.1235605416830092, 1.1971175239921867, 1.17528816529948, 1.1818179728920604, 1.2666712985757533]
Training Loss (progress: 0.16), 0.1121023976150758, [0.0005984030148463553, 0.0018527523019637303, 0.0018231353272765002, 0.052625006114484306, 0.12140199026162983, 0.007967624391553378], [0.9604816486683398, 1.123590799753743, 1.1970913501566032, 1.1753779516612293, 1.1819263190594786, 1.2667931325397164]
Training Loss (progress: 0.24), 0.10787442673640253, [0.0006055163180745837, 0.0018639317996502146, 0.001838434915631319, 0.05267986346943156, 0.12146144290664883, 0.00794018612177572], [0.9604549135592615, 1.123593542461559, 1.197096902752831, 1.175489758418692, 1.1820663140491514, 1.2669070846271455]
Training Loss (progress: 0.32), 0.10112217955238156, [0.000611131668230496, 0.0018666563517690984, 0.0018235485249642367, 0.0526903699246648, 0.12149055615735586, 0.007915101319937506], [0.9604032251760866, 1.1236148980650693, 1.1970689145155455, 1.175538378455039, 1.1821599426491118, 1.2669978077840485]
Training Loss (progress: 0.40), 0.09878286110110727, [0.0006013828745309454, 0.0018431237828086522, 0.001830874718701888, 0.05265244842286687, 0.12154502277600854, 0.007901967580464724], [0.9603786170476538, 1.1235950514012114, 1.197069887356657, 1.1755163834924343, 1.182253556956566, 1.2670950618227357]
Training Loss (progress: 0.48), 0.11271892327341801, [0.0006106477262343535, 0.0018566440625011062, 0.001818016700206571, 0.052658939787899074, 0.12162410874718059, 0.007925419173186373], [0.9603208955344887, 1.1235591207129598, 1.197057780051003, 1.1755496611172647, 1.182359868210142, 1.267231081082666]
Training Loss (progress: 0.56), 0.11055585282488904, [0.0005949729898738528, 0.00185743138302666, 0.001822913616529414, 0.05270384500847762, 0.12172647454942327, 0.00793454961102409], [0.9602805653971828, 1.1236027380903417, 1.197046927730601, 1.1756272987700043, 1.1825221706384417, 1.2673408467739777]
Training Loss (progress: 0.64), 0.09836643559876868, [0.0005976446075852421, 0.0018448563380825183, 0.0018360658707216627, 0.052737924206687596, 0.12175868672578054, 0.007929010783293396], [0.9602243842653863, 1.1236129663583292, 1.1970516882069875, 1.1756832106191863, 1.1825896860633929, 1.2674166364163015]
Training Loss (progress: 0.72), 0.1025920472186827, [0.0006075268663307592, 0.0018431774259581762, 0.0018187964451117221, 0.05275772905758942, 0.12183346278381361, 0.007925044307831529], [0.9602223761194142, 1.1235967269421825, 1.1969977845570843, 1.1757536175357284, 1.1827091450428937, 1.2675243903143847]
Training Loss (progress: 0.80), 0.0958420595561256, [0.0005909605504376952, 0.0018586909914368027, 0.0018311650981396954, 0.052767009367488835, 0.12190148443073194, 0.007934222775330449], [0.9602173022649886, 1.123606713656525, 1.1969820129779345, 1.175834218316592, 1.1828331777039585, 1.2676483429305314]
Training Loss (progress: 0.88), 0.09874802106195975, [0.0005997917949571068, 0.0018520439053104289, 0.0018286614147159654, 0.05275559256567763, 0.12194539601071826, 0.007940414526998493], [0.9601834716666507, 1.1235697987743285, 1.197001650905565, 1.175879752008423, 1.1829212486249474, 1.2677476847542501]
Training Loss (progress: 0.96), 0.10858198354416755, [0.0005929127316294604, 0.0018447030345381994, 0.0018282450277001454, 0.05275090026119037, 0.12198548619870522, 0.007958560827885264], [0.9602293355233718, 1.123568149842382, 1.1969816515603624, 1.1759388829317143, 1.1830163118229464, 1.267924700101575]
Evaluation on validation dataset:
Step 25, mean loss 0.03236735415571476
Step 50, mean loss 0.016087042902789197
Step 75, mean loss 0.026172843657731266
Step 100, mean loss 0.031782831596866935
Step 125, mean loss 0.03405054754776591
Step 150, mean loss 0.03509261314269027
Step 175, mean loss 0.060183312702892386
Step 200, mean loss 0.1860651016877053
Step 225, mean loss 0.17346043829887697
Unrolled forward losses 1.6393411639279327
Unrolled forward base losses 2.565701273852575
Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00), 0.1038139989290224, [0.0006063795066364552, 0.0018753277448908246, 0.001822068121700598, 0.05275365987268649, 0.12206842222748378, 0.007929557058374902], [0.9601800653838877, 1.1235936363680648, 1.1969674630513165, 1.1759630214906966, 1.1831316199829478, 1.2679901013916985]
Training Loss (progress: 0.08), 0.10931212076159438, [0.0005973251476709254, 0.0018534794570832464, 0.0018443950594139288, 0.052819151721321865, 0.12217833874589985, 0.007940885897847711], [0.9601507642575692, 1.1236142621625316, 1.1969659252224023, 1.1760700068811905, 1.183269983297492, 1.2681073396325453]
Training Loss (progress: 0.16), 0.10887647782836081, [0.0005914436353501302, 0.0018654229265592948, 0.001832671013981693, 0.05281968435281642, 0.12226109754717962, 0.007942786552012792], [0.9601825149650763, 1.1236203859067009, 1.1969465567104507, 1.1761178896013174, 1.183387867550135, 1.2682370619754177]
Training Loss (progress: 0.24), 0.10224534859449742, [0.0005942344008281867, 0.0018457674379703756, 0.0018266391455441068, 0.05285768216712445, 0.12232263607525105, 0.007943886769880932], [0.9601320582416287, 1.1236184206618434, 1.1969384516540411, 1.1762146776934688, 1.1835090505121588, 1.2683243641289568]
Training Loss (progress: 0.32), 0.11328118875335949, [0.0006013175759769252, 0.0018662871283922694, 0.0018196068238097035, 0.05288309908657299, 0.12235165587963664, 0.007958993007732453], [0.9600964975124245, 1.123607493346682, 1.196937984015356, 1.1762577306843485, 1.1835864408440637, 1.268434216265313]
Training Loss (progress: 0.40), 0.11088694507756053, [0.0005970120795111606, 0.0018608359235709221, 0.0018225748316918634, 0.05286376903714123, 0.12237152367982278, 0.007939291165782411], [0.9600793014684262, 1.1235769190276568, 1.1969132347623526, 1.1762794508101422, 1.1836742415784847, 1.268560978833593]
Training Loss (progress: 0.48), 0.10121489299668067, [0.0005991847215458669, 0.0018570395082615728, 0.0018202506665622432, 0.05286435533764534, 0.12247493442901247, 0.007932358552123871], [0.9600724095263767, 1.123577444629281, 1.196912889237116, 1.176334662478742, 1.1837947082562494, 1.2686112090968518]
Training Loss (progress: 0.56), 0.10720968775511873, [0.0005943689934464117, 0.00183725108365416, 0.0018312090327650436, 0.05289610870507752, 0.12255317552851934, 0.007931086060430878], [0.9599879171133197, 1.1235659939427252, 1.1968732084620997, 1.1764039645134854, 1.1839173188618872, 1.268741624108732]
Training Loss (progress: 0.64), 0.10468999254278484, [0.000597077170030477, 0.0018599085394878077, 0.0018235927860534773, 0.052907854476803344, 0.12262764898340524, 0.007925849697060616], [0.9599577969954739, 1.1235687564044559, 1.1968646762869688, 1.1764580805248106, 1.1840383842546833, 1.2688238654796973]
Training Loss (progress: 0.72), 0.11000401851064735, [0.0005967049424112621, 0.0018646155729333594, 0.0018249168782702363, 0.052883457324898964, 0.12263971469691055, 0.00794206356728619], [0.9599941983242769, 1.1235817806011743, 1.1968793421303847, 1.1764786255145816, 1.1841198148517658, 1.2689303698161967]
Training Loss (progress: 0.80), 0.09366467780323764, [0.0006026408252639032, 0.0018711536814897897, 0.001825550033368586, 0.05289038924813322, 0.1226519792904782, 0.007935858790256413], [0.959968981887938, 1.1235521820190097, 1.1968408997335394, 1.1765333270688558, 1.184189180412992, 1.2690397110362128]
Training Loss (progress: 0.88), 0.10709710365448882, [0.0005967701872567761, 0.0018782869734752662, 0.0018294779256186656, 0.052945151452824205, 0.1227542046184951, 0.007942970328807239], [0.9599154021106217, 1.1235958836086888, 1.1968074814451306, 1.1766294188018693, 1.1843337351342869, 1.2691656371544597]
Training Loss (progress: 0.96), 0.1068547953756365, [0.000612103284476738, 0.001860783697337414, 0.0018358147721203436, 0.05294441055626609, 0.12279650010729386, 0.00794270534665515], [0.9598951621924481, 1.1236032851034297, 1.1968232510031303, 1.1766856763406077, 1.1844253980012243, 1.2692858126785345]
Evaluation on validation dataset:
Step 25, mean loss 0.03033342001765204
Step 50, mean loss 0.015835965200346212
Step 75, mean loss 0.02612706101420083
Step 100, mean loss 0.030841636646808522
Step 125, mean loss 0.035619031505773155
Step 150, mean loss 0.035929180228782547
Step 175, mean loss 0.06036306308831941
Step 200, mean loss 0.19711877378995207
Step 225, mean loss 0.16580052385965777
Unrolled forward losses 1.6270239736367988
Unrolled forward base losses 2.565701273852575
Test loss: 1.4107491003154171
