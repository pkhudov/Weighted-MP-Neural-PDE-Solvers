Training on dataset data/CE_train_E2.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar
Beta parameter added to the GNN solver
Number of parameters: 1033789.0
Saved initial model at models/init6141232.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
Node: 01 (pos: 0.010): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 02 (pos: 0.020): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 03 (pos: 0.030): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 04 (pos: 0.040): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 05 (pos: 0.051): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
-
Node: 07 (pos: 0.071): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 08 (pos: 0.081): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 09 (pos: 0.091): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 10 (pos: 0.101): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 11 (pos: 0.111): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 12 (pos: 0.121): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 58 (pos: 0.586): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 59 (pos: 0.596): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 60 (pos: 0.606): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 61 (pos: 0.616): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 50 (pos: 0.505): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
-
Node: 51 (pos: 0.515): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 52 (pos: 0.525): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 53 (pos: 0.535): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 54 (pos: 0.545): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 55 (pos: 0.556): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 62 (pos: 0.626): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
=========================================================================================================
Training Loss (progress: 0.00), 1.234485568112429, [0.004495127074745559, 0.010781040986829426, 0.011450981968626494, 0.01113679244675124, 0.01158029519353957, 0.010875189765811127], [1.0060156968760015, 1.0139108694602594, 1.0140138007798367, 1.0134161985502013, 1.0141458724856827, 1.0129709387734263]
Training Loss (progress: 0.08), 0.18961618733278995, [0.006209728750419103, 0.0008153617745099878, 0.0016336729568018783, 0.004927894638665223, 0.011874553734146408, 0.014914839082198483], [1.0097470337506813, 1.0330920188496162, 1.0300557763257472, 1.0236554831056488, 1.0284410663132277, 1.043862683887517]
Training Loss (progress: 0.16), 0.15400180882235284, [0.005346252259721861, 0.000695521668866781, 0.0017148206889563205, 0.00523116613053242, 0.013739217793644605, 0.01905335358209602], [1.007959663553639, 1.0363886916577063, 1.0340772177064295, 1.0252048879253879, 1.0310059734830477, 1.0539766126720476]
Training Loss (progress: 0.24), 0.13719424759794238, [0.004933772038542892, 0.0005997937299167697, 0.0019691567452318278, 0.005735634223573487, 0.014968084991777441, 0.02177950373615508], [1.0059083822068202, 1.0400029897288599, 1.0378494570933423, 1.027314833516715, 1.0325910696141902, 1.060374713955749]
Training Loss (progress: 0.32), 0.1383504591988438, [0.004588879046604021, 0.0006344816034668269, 0.0017212527771711037, 0.005113493683941447, 0.0157979527776168, 0.02301911907395345], [1.0033887524020044, 1.0436294720006205, 1.0420893655068675, 1.0288498691008403, 1.0333393933415953, 1.0645490535853608]
Training Loss (progress: 0.40), 0.10216089523212045, [0.0044834179553609535, 0.0006065586615855869, 0.0019327050199784686, 0.00530542499867554, 0.016768551600383347, 0.0250222704941881], [1.0010087242746273, 1.0463742841713768, 1.0462309426835248, 1.0300019309713573, 1.0339673275558459, 1.068728906033568]
Training Loss (progress: 0.48), 0.11019029648235923, [0.004281754290201908, 0.0006350692911455167, 0.0018112011543440808, 0.005673745885870863, 0.01779598506898501, 0.026318974415378293], [0.9987805331028036, 1.049216065946916, 1.0500313128162286, 1.0312011685327669, 1.0345261767859264, 1.0720470041132861]
Training Loss (progress: 0.56), 0.10370751413640844, [0.0040260180195381325, 0.0004942866582449293, 0.0017433688716780916, 0.004961056784288783, 0.0190011229067061, 0.02751987646047426], [0.9963418015065226, 1.0529802917880808, 1.0538724925561074, 1.0320542318207466, 1.0349979351084655, 1.0748606489418469]
Training Loss (progress: 0.64), 0.10074790323229706, [0.0036705242076413777, 0.0005937503167663232, 0.0017063324469690033, 0.005124336691590253, 0.019980301458727593, 0.028174064542892806], [0.9939029087828232, 1.0558109723173474, 1.0585767309897358, 1.033328851168619, 1.0355626180783633, 1.077298097703387]
Training Loss (progress: 0.72), 0.08607421356591431, [0.0037369137651379657, 0.0005914456806510181, 0.0016816797979445862, 0.005875630407635704, 0.0206851852832424, 0.029603652240708798], [0.9915225518723877, 1.0575939756190489, 1.0626108616096648, 1.0350821008555136, 1.0361629834663442, 1.0799665077018703]
Training Loss (progress: 0.80), 0.08106747466621834, [0.003654193559923595, 0.0006204147278419166, 0.0016260441288643845, 0.005736538812879777, 0.02155460242733371, 0.03050790452382583], [0.9890928142879176, 1.057826148668142, 1.0664517327453198, 1.0358739193236344, 1.0366015946242393, 1.0820810319786998]
Training Loss (progress: 0.88), 0.08936588833898212, [0.0036575479854079026, 0.000640545121098509, 0.0016541509196842889, 0.006356784235251318, 0.022336044735390536, 0.03140430794770645], [0.9865433996387013, 1.0595701248498144, 1.0690912093573723, 1.036777644941012, 1.036860389631292, 1.0839395262134375]
Training Loss (progress: 0.96), 0.08015649263666894, [0.003125043290228445, 0.0006697219446545956, 0.0017420224216711947, 0.006083194090607517, 0.02384662742161265, 0.03268325554127778], [0.9839875859004843, 1.0595587399647515, 1.0717365976855255, 1.0368489876971392, 1.037745979771044, 1.086045115246601]
Evaluation on validation dataset:
Step 25, mean loss 0.1720757930092831
Step 50, mean loss 0.16497013673159616
Step 75, mean loss 0.1644677083340359
Step 100, mean loss 0.15379207202865847
Step 125, mean loss 0.16794068738781037
Step 150, mean loss 0.14667084187119045
Step 175, mean loss 0.15786002015233208
Step 200, mean loss 0.18357903542591414
Step 225, mean loss 0.2385863977646876
Unrolled forward losses 26.10322454254475
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.12179e-01, 5.68506e-03, 1.05257e-01, 5.96551e-01, 8.90397e-01, 9.73009e-01
Node: 01 (pos: 0.010): 4.43193e-01, 2.80847e-02, 2.13932e-01, 7.06560e-01, 9.33041e-01, 1.00650e+00
Node: 02 (pos: 0.020): 5.90352e-01, 1.03770e-01, 3.82205e-01, 8.11497e-01, 9.69447e-01, 1.03476e+00
Node: 03 (pos: 0.030): 7.37834e-01, 2.86772e-01, 6.00222e-01, 9.03776e-01, 9.98742e-01, 1.05729e+00
Node: 04 (pos: 0.040): 8.65238e-01, 5.92750e-01, 8.28558e-01, 9.76046e-01, 1.02021e+00, 1.07368e+00
Node: 05 (pos: 0.051): 9.52010e-01, 9.16371e-01, 1.00538e+00, 1.02215e+00, 1.03331e+00, 1.08364e+00
-
Node: 07 (pos: 0.071): 9.52010e-01, 9.16371e-01, 1.00538e+00, 1.02215e+00, 1.03331e+00, 1.08364e+00
Node: 08 (pos: 0.081): 8.65238e-01, 5.92750e-01, 8.28558e-01, 9.76046e-01, 1.02021e+00, 1.07368e+00
Node: 09 (pos: 0.091): 7.37834e-01, 2.86772e-01, 6.00222e-01, 9.03776e-01, 9.98742e-01, 1.05729e+00
Node: 10 (pos: 0.101): 5.90352e-01, 1.03770e-01, 3.82205e-01, 8.11497e-01, 9.69447e-01, 1.03476e+00
Node: 11 (pos: 0.111): 4.43193e-01, 2.80847e-02, 2.13932e-01, 7.06560e-01, 9.33041e-01, 1.00650e+00
Node: 12 (pos: 0.121): 3.12179e-01, 5.68506e-03, 1.05257e-01, 5.96551e-01, 8.90397e-01, 9.73009e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.52010e-01, 9.16371e-01, 1.00538e+00, 1.02215e+00, 1.03331e+00, 1.08364e+00
Node: 58 (pos: 0.586): 8.65238e-01, 5.92750e-01, 8.28558e-01, 9.76046e-01, 1.02021e+00, 1.07368e+00
Node: 59 (pos: 0.596): 7.37834e-01, 2.86772e-01, 6.00222e-01, 9.03776e-01, 9.98742e-01, 1.05729e+00
Node: 60 (pos: 0.606): 5.90352e-01, 1.03770e-01, 3.82205e-01, 8.11497e-01, 9.69447e-01, 1.03476e+00
Node: 61 (pos: 0.616): 4.43193e-01, 2.80847e-02, 2.13932e-01, 7.06560e-01, 9.33041e-01, 1.00650e+00
Node: 50 (pos: 0.505): 3.12179e-01, 5.68506e-03, 1.05257e-01, 5.96551e-01, 8.90397e-01, 9.73009e-01
-
Node: 51 (pos: 0.515): 4.43193e-01, 2.80847e-02, 2.13932e-01, 7.06560e-01, 9.33041e-01, 1.00650e+00
Node: 52 (pos: 0.525): 5.90352e-01, 1.03770e-01, 3.82205e-01, 8.11497e-01, 9.69447e-01, 1.03476e+00
Node: 53 (pos: 0.535): 7.37834e-01, 2.86772e-01, 6.00222e-01, 9.03776e-01, 9.98742e-01, 1.05729e+00
Node: 54 (pos: 0.545): 8.65238e-01, 5.92750e-01, 8.28558e-01, 9.76046e-01, 1.02021e+00, 1.07368e+00
Node: 55 (pos: 0.556): 9.52010e-01, 9.16371e-01, 1.00538e+00, 1.02215e+00, 1.03331e+00, 1.08364e+00
Node: 62 (pos: 0.626): 3.12179e-01, 5.68506e-03, 1.05257e-01, 5.96551e-01, 8.90397e-01, 9.73009e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.15725193756511718
Step 50, mean loss 0.15376323310445822
Step 75, mean loss 0.16343571290213185
Step 100, mean loss 0.17809598563824586
Step 125, mean loss 0.17336245184701454
Step 150, mean loss 0.18980608840318025
Step 175, mean loss 0.2006295779648885
Step 200, mean loss 0.24256760981053516
Step 225, mean loss 0.28046027109929883
Unrolled forward losses 27.116413418981807
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.2245061983867081, [0.003447432803912842, 0.0006127672078344823, 0.0019248745221983061, 0.006781627673647575, 0.023683757256087862, 0.033202188097984954], [0.9828674871660064, 1.059258457475222, 1.0728328663870654, 1.0378965934769055, 1.0372395203631852, 1.0866774700639168]
Training Loss (progress: 0.08), 0.20325133720192373, [0.004070828724004612, 0.0006674949200883683, 0.0019445946454656469, 0.008423340878530135, 0.025999836996603545, 0.04053939902429735], [0.9815249287319823, 1.0587254980374439, 1.075390211152706, 1.0386325925162387, 1.0376247279848607, 1.0912844814121656]
Training Loss (progress: 0.16), 0.18926635526292934, [0.004351208472049491, 0.0006620494989374057, 0.0018836267176168457, 0.008995024903578201, 0.028899879940680207, 0.045787343541466304], [0.9795336909284679, 1.0570920403203534, 1.0778600450811344, 1.0389973075909509, 1.0383424417124971, 1.0943752425538809]
Training Loss (progress: 0.24), 0.19682120933031558, [0.004769152268632828, 0.0007462563791623232, 0.0019229096310879237, 0.01018391996115393, 0.030639493482542683, 0.049167897744626744], [0.9780378805763259, 1.055864978908901, 1.0799500995850244, 1.0389073681265941, 1.038427005479433, 1.0957370924984342]
Training Loss (progress: 0.32), 0.172100589326067, [0.004842416967330123, 0.0007700711960679468, 0.0019641237905665917, 0.010614718786173906, 0.03175503296168837, 0.0529365201924416], [0.9759785821008868, 1.0548131196233443, 1.0827149964141434, 1.0393290419127164, 1.038031705719778, 1.097857408711696]
Training Loss (progress: 0.40), 0.17411032878629643, [0.004897427425083199, 0.0007663257239755015, 0.001849458198225743, 0.012473566020779613, 0.03382188437280326, 0.056313245093719676], [0.974958418374686, 1.053253671311149, 1.0857252237155348, 1.0402406051151074, 1.0385414432542075, 1.0997309367146173]
Training Loss (progress: 0.48), 0.16191186389085208, [0.004747431964966447, 0.0007518230609459891, 0.002105072178925354, 0.012604001229925298, 0.03532041440498817, 0.059804385347073385], [0.9736499873203498, 1.0520987087246407, 1.0885247443529515, 1.040095002690741, 1.0384546466027342, 1.1013871214268518]
Training Loss (progress: 0.56), 0.16147880412393398, [0.004921102768571322, 0.0007252769502019262, 0.0022788921787401935, 0.013810398887001658, 0.03697314278723993, 0.06284227099615479], [0.9722897746868955, 1.0504013988060512, 1.0901441295498238, 1.0408712848468085, 1.0386760801630877, 1.1031320609248945]
Training Loss (progress: 0.64), 0.1716167530105198, [0.004647392783484627, 0.0008519913533687898, 0.001731935247787862, 0.015118337410264712, 0.03892022118699946, 0.0657065003352075], [0.970969263457607, 1.049345549914952, 1.0907876521998, 1.0418000339862337, 1.0388628311767207, 1.104450138419724]
Training Loss (progress: 0.72), 0.16214926038982957, [0.004730998322279909, 0.0007753968977234077, 0.002140313329519306, 0.016719142087503847, 0.04034000079462328, 0.0679760476945778], [0.9689032316951929, 1.0477458072687982, 1.093037210755329, 1.0431364112987906, 1.0387652166882013, 1.1052848397286115]
Training Loss (progress: 0.80), 0.14075493929153315, [0.00473199441616526, 0.0008515351999363218, 0.0021205658976399177, 0.016938948241666626, 0.04173578137032986, 0.07037889640489824], [0.9670895715284232, 1.046656244996065, 1.0954829100141485, 1.0429200448192466, 1.038750052588642, 1.1061864187145831]
Training Loss (progress: 0.88), 0.14610541443395458, [0.004833540316647579, 0.0007225896328995579, 0.002357039433882632, 0.017352287379006032, 0.044395611211831026, 0.07240582140135193], [0.9657678525116236, 1.0449789437286865, 1.0966417962427053, 1.0430147771436242, 1.0395851725636078, 1.107161295667768]
Training Loss (progress: 0.96), 0.14690570525256721, [0.004946344951843135, 0.0006321728419691894, 0.0022593289151172863, 0.01896953248898318, 0.04526208999065277, 0.07474342256191535], [0.9644854705194715, 1.043696408250914, 1.0984183931075318, 1.0438190781628998, 1.0393932491673727, 1.1084888455443402]
Evaluation on validation dataset:
Step 25, mean loss 0.07703123168182194
Step 50, mean loss 0.04284338327791044
Step 75, mean loss 0.050471125250347054
Step 100, mean loss 0.06524714215837467
Step 125, mean loss 0.09330987340233676
Step 150, mean loss 0.11649461967633695
Step 175, mean loss 0.13564188344601324
Step 200, mean loss 0.153758092224563
Step 225, mean loss 0.16389998208378642
Unrolled forward losses 5.719704870425362
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.30251e-01, 1.01312e-02, 2.08463e-01, 8.71833e-01, 9.59022e-01, 1.05505e+00
Node: 01 (pos: 0.010): 5.50421e-01, 4.17478e-02, 3.46455e-01, 9.21531e-01, 9.82858e-01, 1.07098e+00
Node: 02 (pos: 0.020): 6.73315e-01, 1.32982e-01, 5.24991e-01, 9.64293e-01, 1.00280e+00, 1.08419e+00
Node: 03 (pos: 0.030): 7.87576e-01, 3.27445e-01, 7.25345e-01, 9.98920e-01, 1.01859e+00, 1.09459e+00
Node: 04 (pos: 0.040): 8.80880e-01, 6.23263e-01, 9.13745e-01, 1.02441e+00, 1.03002e+00, 1.10207e+00
Node: 05 (pos: 0.051): 9.42087e-01, 9.17047e-01, 1.04953e+00, 1.04002e+00, 1.03694e+00, 1.10658e+00
-
Node: 07 (pos: 0.071): 9.42087e-01, 9.17047e-01, 1.04953e+00, 1.04002e+00, 1.03694e+00, 1.10658e+00
Node: 08 (pos: 0.081): 8.80880e-01, 6.23263e-01, 9.13745e-01, 1.02441e+00, 1.03002e+00, 1.10207e+00
Node: 09 (pos: 0.091): 7.87576e-01, 3.27445e-01, 7.25345e-01, 9.98920e-01, 1.01859e+00, 1.09459e+00
Node: 10 (pos: 0.101): 6.73315e-01, 1.32982e-01, 5.24991e-01, 9.64293e-01, 1.00280e+00, 1.08419e+00
Node: 11 (pos: 0.111): 5.50421e-01, 4.17478e-02, 3.46455e-01, 9.21531e-01, 9.82858e-01, 1.07098e+00
Node: 12 (pos: 0.121): 4.30251e-01, 1.01312e-02, 2.08463e-01, 8.71833e-01, 9.59022e-01, 1.05505e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.42087e-01, 9.17047e-01, 1.04953e+00, 1.04002e+00, 1.03694e+00, 1.10658e+00
Node: 58 (pos: 0.586): 8.80880e-01, 6.23263e-01, 9.13745e-01, 1.02441e+00, 1.03002e+00, 1.10207e+00
Node: 59 (pos: 0.596): 7.87576e-01, 3.27445e-01, 7.25345e-01, 9.98920e-01, 1.01859e+00, 1.09459e+00
Node: 60 (pos: 0.606): 6.73315e-01, 1.32982e-01, 5.24991e-01, 9.64293e-01, 1.00280e+00, 1.08419e+00
Node: 61 (pos: 0.616): 5.50421e-01, 4.17478e-02, 3.46455e-01, 9.21531e-01, 9.82858e-01, 1.07098e+00
Node: 50 (pos: 0.505): 4.30251e-01, 1.01312e-02, 2.08463e-01, 8.71833e-01, 9.59022e-01, 1.05505e+00
-
Node: 51 (pos: 0.515): 5.50421e-01, 4.17478e-02, 3.46455e-01, 9.21531e-01, 9.82858e-01, 1.07098e+00
Node: 52 (pos: 0.525): 6.73315e-01, 1.32982e-01, 5.24991e-01, 9.64293e-01, 1.00280e+00, 1.08419e+00
Node: 53 (pos: 0.535): 7.87576e-01, 3.27445e-01, 7.25345e-01, 9.98920e-01, 1.01859e+00, 1.09459e+00
Node: 54 (pos: 0.545): 8.80880e-01, 6.23263e-01, 9.13745e-01, 1.02441e+00, 1.03002e+00, 1.10207e+00
Node: 55 (pos: 0.556): 9.42087e-01, 9.17047e-01, 1.04953e+00, 1.04002e+00, 1.03694e+00, 1.10658e+00
Node: 62 (pos: 0.626): 4.30251e-01, 1.01312e-02, 2.08463e-01, 8.71833e-01, 9.59022e-01, 1.05505e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.0680014154828126
Step 50, mean loss 0.04090775628125688
Step 75, mean loss 0.05243438479396235
Step 100, mean loss 0.07437969981059937
Step 125, mean loss 0.10872626732655193
Step 150, mean loss 0.13830343915830628
Step 175, mean loss 0.16363076630171225
Step 200, mean loss 0.1741128301224445
Step 225, mean loss 0.22697958237780386
Unrolled forward losses 6.462293354031933
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.2261763723744417, [0.004769315534807167, 0.0007540177491065883, 0.0021764551842493583, 0.020214055177992497, 0.04579641665361036, 0.075140270709753], [0.9636336610792472, 1.0429696790265204, 1.0990327641003321, 1.0451173724725824, 1.0393038069764637, 1.1083077031516262]
Training Loss (progress: 0.08), 0.2121002272032159, [0.004839180504035018, 0.0006886569319145602, 0.002260564764075977, 0.021069387163290324, 0.04748865612324228, 0.0777165507616658], [0.9639214907897149, 1.0425551362134953, 1.0991865490519404, 1.0458346168671186, 1.0404169370210332, 1.1107870542987446]
Training Loss (progress: 0.16), 0.21568179638196283, [0.004824747610802066, 0.0007112732705437515, 0.0022202978128987253, 0.022023729482728054, 0.04885541238828716, 0.07925294974182093], [0.9640193556452047, 1.0418773934719294, 1.100560986488749, 1.0466291026666312, 1.0411261641297092, 1.1122331711770381]
Training Loss (progress: 0.24), 0.18017791270905092, [0.004530084083031901, 0.0007951782673488328, 0.002112562904296182, 0.023418864701017294, 0.050225541578814245, 0.0806246983245173], [0.9630021620683953, 1.0414114229881546, 1.101761876455166, 1.0483109844633662, 1.041824201238228, 1.1134299425883614]
Training Loss (progress: 0.32), 0.21896564883846675, [0.004751146575009763, 0.0007863950175693085, 0.0021281419885068502, 0.0241118628577537, 0.051435972521290584, 0.08186540679510058], [0.9632083258091927, 1.041133172345376, 1.102793258529398, 1.0488348145328505, 1.0424081626783839, 1.1144661314938797]
Training Loss (progress: 0.40), 0.1982162655164319, [0.004708835970718153, 0.0008297541264767432, 0.0021740752974484714, 0.024969002445325572, 0.05287819588178037, 0.0834378681547385], [0.9633829998439323, 1.0405139093107756, 1.1036277114268644, 1.0496220372989045, 1.0433239919305046, 1.1158154242755713]
Training Loss (progress: 0.48), 0.19241796914287387, [0.0043597408496456325, 0.0008705812498781671, 0.002166527199383128, 0.025264522083654373, 0.05368627888775146, 0.08440637798723574], [0.9627678028906757, 1.0401274023418958, 1.1047155692753936, 1.0497768946360104, 1.0438356259508066, 1.1167469221571638]
Training Loss (progress: 0.56), 0.19265033649869628, [0.004697481754571456, 0.0008012985581597967, 0.0022074773869749918, 0.027095447948695714, 0.05530094462448125, 0.08526179459095902], [0.9629227955590577, 1.0393997389874876, 1.105740837382794, 1.0511367712035335, 1.0446107772004132, 1.1173976631905518]
Training Loss (progress: 0.64), 0.18901806121926876, [0.004667954400963957, 0.0008318536510864515, 0.0022132400103133712, 0.028553316892534004, 0.05660870987921053, 0.08646039470567159], [0.9628398281871, 1.0390142462456309, 1.1062747884652138, 1.0527713718838314, 1.0454517292036547, 1.118563568086169]
Training Loss (progress: 0.72), 0.181507977899636, [0.004587982692824941, 0.0008435301090563289, 0.002207138012481243, 0.029198584340737875, 0.05744484751930926, 0.08721659068373538], [0.9624545156875424, 1.0383853010437623, 1.1073057716764527, 1.0532377754861326, 1.045664355745079, 1.1192976362601061]
Training Loss (progress: 0.80), 0.1919902060868284, [0.004590469112850064, 0.0007148511403675719, 0.0021143959409100376, 0.030068638052661342, 0.0589149570441038, 0.08816308825310036], [0.9619299537373627, 1.0380556910306242, 1.1075550572399089, 1.0537852138775032, 1.0464463706846447, 1.1198255474499443]
Training Loss (progress: 0.88), 0.17514554862073778, [0.004549273513358251, 0.0007837457752280081, 0.0021966076042556383, 0.030923276026506387, 0.06009040257403151, 0.08929233637216967], [0.9620435004506692, 1.0375962102009628, 1.1083073324732955, 1.0546389164190537, 1.0472075676484682, 1.1209583678731818]
Training Loss (progress: 0.96), 0.21299481933288286, [0.004553655384146064, 0.000776039049505076, 0.0021796520202738948, 0.03190468244256244, 0.06102352207821938, 0.09011293083883125], [0.9614809514767568, 1.0369637894232444, 1.1089046541939995, 1.0555258670087853, 1.0474293029192563, 1.121618949280773]
Evaluation on validation dataset:
Step 25, mean loss 0.05597501250784932
Step 50, mean loss 0.021614441161970956
Step 75, mean loss 0.02637756633955835
Step 100, mean loss 0.031230562585762144
Step 125, mean loss 0.04505931383835528
Step 150, mean loss 0.04844558061445009
Step 175, mean loss 0.060630920709470265
Step 200, mean loss 0.07954272251351338
Step 225, mean loss 0.09785745702764703
Unrolled forward losses 1.9677787969062484
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.27057e-01, 4.85936e-03, 2.07496e-01, 9.41734e-01, 9.86951e-01, 1.07674e+00
Node: 01 (pos: 0.010): 5.47223e-01, 2.50177e-02, 3.46295e-01, 9.75169e-01, 1.00511e+00, 1.09025e+00
Node: 02 (pos: 0.020): 6.70293e-01, 9.56141e-02, 5.26549e-01, 1.00341e+00, 1.02021e+00, 1.10142e+00
Node: 03 (pos: 0.030): 7.84852e-01, 2.71270e-01, 7.29440e-01, 1.02593e+00, 1.03212e+00, 1.11020e+00
Node: 04 (pos: 0.040): 8.78481e-01, 5.71331e-01, 9.20655e-01, 1.04233e+00, 1.04071e+00, 1.11650e+00
Node: 05 (pos: 0.051): 9.39938e-01, 8.93263e-01, 1.05867e+00, 1.05230e+00, 1.04590e+00, 1.12031e+00
-
Node: 07 (pos: 0.071): 9.39938e-01, 8.93263e-01, 1.05867e+00, 1.05230e+00, 1.04590e+00, 1.12031e+00
Node: 08 (pos: 0.081): 8.78481e-01, 5.71331e-01, 9.20655e-01, 1.04233e+00, 1.04071e+00, 1.11650e+00
Node: 09 (pos: 0.091): 7.84852e-01, 2.71270e-01, 7.29440e-01, 1.02593e+00, 1.03212e+00, 1.11020e+00
Node: 10 (pos: 0.101): 6.70293e-01, 9.56141e-02, 5.26549e-01, 1.00341e+00, 1.02021e+00, 1.10142e+00
Node: 11 (pos: 0.111): 5.47223e-01, 2.50177e-02, 3.46295e-01, 9.75169e-01, 1.00511e+00, 1.09025e+00
Node: 12 (pos: 0.121): 4.27057e-01, 4.85936e-03, 2.07496e-01, 9.41734e-01, 9.86951e-01, 1.07674e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.39938e-01, 8.93263e-01, 1.05867e+00, 1.05230e+00, 1.04590e+00, 1.12031e+00
Node: 58 (pos: 0.586): 8.78481e-01, 5.71331e-01, 9.20655e-01, 1.04233e+00, 1.04071e+00, 1.11650e+00
Node: 59 (pos: 0.596): 7.84852e-01, 2.71270e-01, 7.29440e-01, 1.02593e+00, 1.03212e+00, 1.11020e+00
Node: 60 (pos: 0.606): 6.70293e-01, 9.56141e-02, 5.26549e-01, 1.00341e+00, 1.02021e+00, 1.10142e+00
Node: 61 (pos: 0.616): 5.47223e-01, 2.50177e-02, 3.46295e-01, 9.75169e-01, 1.00511e+00, 1.09025e+00
Node: 50 (pos: 0.505): 4.27057e-01, 4.85936e-03, 2.07496e-01, 9.41734e-01, 9.86951e-01, 1.07674e+00
-
Node: 51 (pos: 0.515): 5.47223e-01, 2.50177e-02, 3.46295e-01, 9.75169e-01, 1.00511e+00, 1.09025e+00
Node: 52 (pos: 0.525): 6.70293e-01, 9.56141e-02, 5.26549e-01, 1.00341e+00, 1.02021e+00, 1.10142e+00
Node: 53 (pos: 0.535): 7.84852e-01, 2.71270e-01, 7.29440e-01, 1.02593e+00, 1.03212e+00, 1.11020e+00
Node: 54 (pos: 0.545): 8.78481e-01, 5.71331e-01, 9.20655e-01, 1.04233e+00, 1.04071e+00, 1.11650e+00
Node: 55 (pos: 0.556): 9.39938e-01, 8.93263e-01, 1.05867e+00, 1.05230e+00, 1.04590e+00, 1.12031e+00
Node: 62 (pos: 0.626): 4.27057e-01, 4.85936e-03, 2.07496e-01, 9.41734e-01, 9.86951e-01, 1.07674e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.051368939518788556
Step 50, mean loss 0.02440507841451885
Step 75, mean loss 0.032310538369128214
Step 100, mean loss 0.040746002214163815
Step 125, mean loss 0.05075408760776602
Step 150, mean loss 0.07459629489593048
Step 175, mean loss 0.08258443395398862
Step 200, mean loss 0.11095152855523385
Step 225, mean loss 0.128445481531086
Unrolled forward losses 2.7389469248041394
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.18620177927967685, [0.004389957022113045, 0.0007369610798073929, 0.002153007099280929, 0.03227043257196454, 0.0616859404747788, 0.09008507906869406], [0.9613031711024664, 1.0366908198328273, 1.1089202119210548, 1.0556636078923947, 1.047743482217231, 1.121516748726872]
Training Loss (progress: 0.08), 0.17895507031454402, [0.004593401240739967, 0.0007562127328753555, 0.002316113096061301, 0.03316702060096229, 0.06257316126091972, 0.09120431099708763], [0.9610880536276172, 1.036159822458595, 1.1099248301052262, 1.05651541581707, 1.0481144100624684, 1.122703508372129]
Training Loss (progress: 0.16), 0.18501187256664878, [0.004500950302238979, 0.0007449460796153145, 0.0022351378848864127, 0.03377621244012333, 0.0634132924435814, 0.09148811596172766], [0.9605372583217263, 1.0355625074913295, 1.1106413306005791, 1.0567896066274431, 1.048214468328359, 1.122787941398394]
Training Loss (progress: 0.24), 0.18287298490682916, [0.004512582891885961, 0.0007978611495784926, 0.0023258214765173087, 0.0352622274141315, 0.06468591890139906, 0.0922804049065975], [0.9600661903940079, 1.035015116164914, 1.111565036264739, 1.0582673525287434, 1.0490395781975501, 1.1235960427456801]
Training Loss (progress: 0.32), 0.1725664413054823, [0.004514161300562437, 0.0006930907432870745, 0.0023255383328355506, 0.03614034515772728, 0.06571961973912396, 0.0929039811250145], [0.9596230755972956, 1.0345364829678292, 1.1120578813023714, 1.0588603979324782, 1.0495449058091502, 1.1241995790204786]
Training Loss (progress: 0.40), 0.16814660137267254, [0.004482804505503948, 0.000793288367367102, 0.0021887911946333674, 0.03661518911157085, 0.06689138871357378, 0.09361156826695326], [0.9588284814786213, 1.0341675860052877, 1.1120457350918358, 1.0593337860449026, 1.0501897004309444, 1.12509382614312]
Training Loss (progress: 0.48), 0.17550541361203273, [0.004550092651289082, 0.0007779783343197338, 0.0022292089329188033, 0.03798767672292894, 0.0680097836796228, 0.09393232011886575], [0.9582568432016021, 1.0333262965099166, 1.1126019395568305, 1.06023444804487, 1.0506094371002863, 1.1253628503584983]
Training Loss (progress: 0.56), 0.18085943158180487, [0.0043253520731542094, 0.0007809654776602213, 0.0022635121583989143, 0.038837246077268316, 0.06862252180577542, 0.09482541490826754], [0.9577781277970314, 1.0326862331384388, 1.1130375132987809, 1.060986516378674, 1.0508328452395743, 1.1263462896578251]
Training Loss (progress: 0.64), 0.18338452679689182, [0.004445607442015746, 0.000727296019248302, 0.002210834924731221, 0.0402962279926427, 0.06921397904176234, 0.09522340322540708], [0.957482919587846, 1.0323753332468235, 1.1138565698977474, 1.0617733702515584, 1.0510411737013257, 1.1267092699121852]
Training Loss (progress: 0.72), 0.16380149912558956, [0.004371280339864839, 0.0007576077232669249, 0.0021655212691068125, 0.041427460243691705, 0.07030405107880436, 0.0956809714584949], [0.9569842612716118, 1.0318660765444898, 1.114350949401094, 1.0626935455510913, 1.051469290442942, 1.127286220720354]
Training Loss (progress: 0.80), 0.16470848133056906, [0.004529965831644323, 0.0007713420992829967, 0.002310078719772085, 0.042843183527895116, 0.07132136181213969, 0.09633757720358879], [0.9565484981933159, 1.0312764335952012, 1.1148273424030348, 1.0636446787736955, 1.052225423408131, 1.1280299437308436]
Training Loss (progress: 0.88), 0.1702586546334061, [0.004403997673071526, 0.0007782567324312847, 0.0022758218447688572, 0.043909673506778615, 0.07219778659826041, 0.09692698205445707], [0.9557519208484417, 1.0307241693253435, 1.1153543668484658, 1.0647431419006341, 1.0525562260949834, 1.1285564593956552]
Training Loss (progress: 0.96), 0.1680003729342799, [0.004623271502710123, 0.0007953006555132659, 0.0022149495367794875, 0.04513523550770617, 0.07302970338637942, 0.09767749481949765], [0.9554402489771184, 1.0300740099483874, 1.1157509873865599, 1.0651204402892158, 1.0527330577289717, 1.1291882038435115]
Evaluation on validation dataset:
Step 25, mean loss 0.04638415188704052
Step 50, mean loss 0.019468520852355953
Step 75, mean loss 0.02385477072862962
Step 100, mean loss 0.03409563324142678
Step 125, mean loss 0.043640454761503716
Step 150, mean loss 0.04469164355707517
Step 175, mean loss 0.05438135128567882
Step 200, mean loss 0.07051689508169182
Step 225, mean loss 0.08846154973259565
Unrolled forward losses 1.8859791774901216
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.26875e-01, 1.14238e-02, 2.16391e-01, 9.83608e-01, 1.00212e+00, 1.08795e+00
Node: 01 (pos: 0.010): 5.46019e-01, 4.52016e-02, 3.57215e-01, 1.00798e+00, 1.01748e+00, 1.10047e+00
Node: 02 (pos: 0.020): 6.67848e-01, 1.39280e-01, 5.38321e-01, 1.02836e+00, 1.03022e+00, 1.11082e+00
Node: 03 (pos: 0.030): 7.81105e-01, 3.34208e-01, 7.40580e-01, 1.04450e+00, 1.04024e+00, 1.11894e+00
Node: 04 (pos: 0.040): 8.73582e-01, 6.24506e-01, 9.30085e-01, 1.05619e+00, 1.04746e+00, 1.12477e+00
Node: 05 (pos: 0.051): 9.34244e-01, 9.08759e-01, 1.06633e+00, 1.06326e+00, 1.05182e+00, 1.12829e+00
-
Node: 07 (pos: 0.071): 9.34244e-01, 9.08759e-01, 1.06633e+00, 1.06326e+00, 1.05182e+00, 1.12829e+00
Node: 08 (pos: 0.081): 8.73582e-01, 6.24506e-01, 9.30085e-01, 1.05619e+00, 1.04746e+00, 1.12477e+00
Node: 09 (pos: 0.091): 7.81105e-01, 3.34208e-01, 7.40580e-01, 1.04450e+00, 1.04024e+00, 1.11894e+00
Node: 10 (pos: 0.101): 6.67848e-01, 1.39280e-01, 5.38321e-01, 1.02836e+00, 1.03022e+00, 1.11082e+00
Node: 11 (pos: 0.111): 5.46019e-01, 4.52016e-02, 3.57215e-01, 1.00798e+00, 1.01748e+00, 1.10047e+00
Node: 12 (pos: 0.121): 4.26875e-01, 1.14238e-02, 2.16391e-01, 9.83608e-01, 1.00212e+00, 1.08795e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.34244e-01, 9.08759e-01, 1.06633e+00, 1.06326e+00, 1.05182e+00, 1.12829e+00
Node: 58 (pos: 0.586): 8.73582e-01, 6.24506e-01, 9.30085e-01, 1.05619e+00, 1.04746e+00, 1.12477e+00
Node: 59 (pos: 0.596): 7.81105e-01, 3.34208e-01, 7.40580e-01, 1.04450e+00, 1.04024e+00, 1.11894e+00
Node: 60 (pos: 0.606): 6.67848e-01, 1.39280e-01, 5.38321e-01, 1.02836e+00, 1.03022e+00, 1.11082e+00
Node: 61 (pos: 0.616): 5.46019e-01, 4.52016e-02, 3.57215e-01, 1.00798e+00, 1.01748e+00, 1.10047e+00
Node: 50 (pos: 0.505): 4.26875e-01, 1.14238e-02, 2.16391e-01, 9.83608e-01, 1.00212e+00, 1.08795e+00
-
Node: 51 (pos: 0.515): 5.46019e-01, 4.52016e-02, 3.57215e-01, 1.00798e+00, 1.01748e+00, 1.10047e+00
Node: 52 (pos: 0.525): 6.67848e-01, 1.39280e-01, 5.38321e-01, 1.02836e+00, 1.03022e+00, 1.11082e+00
Node: 53 (pos: 0.535): 7.81105e-01, 3.34208e-01, 7.40580e-01, 1.04450e+00, 1.04024e+00, 1.11894e+00
Node: 54 (pos: 0.545): 8.73582e-01, 6.24506e-01, 9.30085e-01, 1.05619e+00, 1.04746e+00, 1.12477e+00
Node: 55 (pos: 0.556): 9.34244e-01, 9.08759e-01, 1.06633e+00, 1.06326e+00, 1.05182e+00, 1.12829e+00
Node: 62 (pos: 0.626): 4.26875e-01, 1.14238e-02, 2.16391e-01, 9.83608e-01, 1.00212e+00, 1.08795e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.043167221364389524
Step 50, mean loss 0.020720784291748808
Step 75, mean loss 0.02798636932775324
Step 100, mean loss 0.03552604353887959
Step 125, mean loss 0.04627420436963407
Step 150, mean loss 0.06722943342321147
Step 175, mean loss 0.07939600597509555
Step 200, mean loss 0.0953553134726596
Step 225, mean loss 0.11303518076348064
Unrolled forward losses 2.49669730231428
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.15669187312325947, [0.004566615816466018, 0.0007267634042514782, 0.0022463046305342582, 0.045888533969507116, 0.07376384012606943, 0.09797985718178677], [0.9552656850956787, 1.0297474251659708, 1.1160077274685922, 1.0656151454445189, 1.0532498419710048, 1.1294310088341746]
Training Loss (progress: 0.08), 0.1662114146842579, [0.0042680969053243145, 0.0006716559733610002, 0.0021643996581613, 0.04719729769484378, 0.07437431874985721, 0.09789475264450842], [0.9543834624319156, 1.029066176881996, 1.1164065172035773, 1.066295126946316, 1.053202244974882, 1.1294023181243626]
Training Loss (progress: 0.16), 0.1424475647001043, [0.004456804217643492, 0.0007624888511436132, 0.002167058437358429, 0.04796916731558847, 0.07520952108759853, 0.09853590835329024], [0.9541123585621318, 1.0283971136868346, 1.1167258838884306, 1.066758179417389, 1.0535882842323667, 1.1302425429675846]
Training Loss (progress: 0.24), 0.15072026754789689, [0.0045610872276644, 0.0007400134755763031, 0.002280389388153874, 0.04897518876565971, 0.0761058588681564, 0.09893690583494012], [0.9537556508611972, 1.0279245292858226, 1.1175338113673325, 1.0673357805094572, 1.0537726348756449, 1.1305426800842358]
Training Loss (progress: 0.32), 0.13530619120951998, [0.0043704315124700605, 0.0007826105322191726, 0.0021967909870091134, 0.05028589623463115, 0.07715355365210315, 0.09939521335178023], [0.9531721632055445, 1.0273434859759036, 1.1178929345640074, 1.067953936159745, 1.0543805214935285, 1.1312403763407797]
Training Loss (progress: 0.40), 0.15814270640394884, [0.004404661992033242, 0.0007993587796042472, 0.0022364467720510733, 0.05119773116432356, 0.07794882444167091, 0.09985972120675261], [0.9525696844875418, 1.026596430490433, 1.1185048908186734, 1.068807637379678, 1.0547128288252854, 1.1316806536986603]
Training Loss (progress: 0.48), 0.16901312097051993, [0.004510608020319222, 0.0007381328487367414, 0.0022577231572488443, 0.052196987639119785, 0.07864916538051177, 0.10020835208876114], [0.951860830171882, 1.0258965309382748, 1.118805133250187, 1.0691814128041204, 1.0550344488496761, 1.1319898728810331]
Training Loss (progress: 0.56), 0.15072216540130143, [0.004341862462614263, 0.0007915844382619378, 0.0022994553302841674, 0.05347791777798191, 0.07942087481571942, 0.10023162933401795], [0.9515719844046486, 1.025402169515199, 1.1193127487059533, 1.0702322181107875, 1.0553089732760865, 1.132263435294604]
Training Loss (progress: 0.64), 0.15897770351156743, [0.004466158114924487, 0.0008473561370004845, 0.002352750059875986, 0.054874826328574496, 0.08033301371274039, 0.10087339981090912], [0.9508928660585092, 1.0247690750439837, 1.1196139379785204, 1.0713660990109877, 1.0558287056780227, 1.1326344219526308]
Training Loss (progress: 0.72), 0.16190160288544161, [0.004446274169386724, 0.0007267621534611461, 0.0021923391006128585, 0.056083382467736885, 0.08120020392453509, 0.10144514514334552], [0.9498724643128892, 1.0239921266065217, 1.1200930821714796, 1.0720698678869907, 1.0560764098895288, 1.1333147619818453]
Training Loss (progress: 0.80), 0.1530169425065277, [0.004554483994728507, 0.0008223618428411647, 0.0023119229988633532, 0.057426099610490784, 0.08186385512436613, 0.10177145741912266], [0.94945188116424, 1.0233144786714123, 1.120378041138762, 1.072984687398376, 1.0563278414927568, 1.133595842541079]
Training Loss (progress: 0.88), 0.1570736917010365, [0.004413467578623649, 0.0007716225573224251, 0.002173349648799766, 0.05860772517992061, 0.08262931863500465, 0.10221278625134368], [0.9484215506946724, 1.0226750469775843, 1.1208927218469527, 1.0738336641517146, 1.0563136284516517, 1.1341441643789911]
Training Loss (progress: 0.96), 0.15698036330574616, [0.004399393415004561, 0.0007510133783776518, 0.0022554879618201036, 0.059548912240679326, 0.08312985122339822, 0.10225416362672227], [0.9478868280957512, 1.022154915350412, 1.120938573276918, 1.0742010592510323, 1.0565392671855076, 1.1342358609004082]
Evaluation on validation dataset:
Step 25, mean loss 0.03308004013639371
Step 50, mean loss 0.015766448956499476
Step 75, mean loss 0.02308431472882446
Step 100, mean loss 0.031016644623054212
Step 125, mean loss 0.03900101822197008
Step 150, mean loss 0.0416077349076177
Step 175, mean loss 0.047815438760242
Step 200, mean loss 0.06405296631993848
Step 225, mean loss 0.08431781947882969
Unrolled forward losses 1.9787675176392119
Unrolled forward base losses 1.1720234445357585
Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.1439039338623216, [0.004487034070199842, 0.0007912181828078327, 0.002196634649835025, 0.059991761030069125, 0.0836038336071769, 0.1024405149845052], [0.9477189330624912, 1.021930561781844, 1.1209833480043134, 1.0745415293447222, 1.056751783285674, 1.134530328894885]
Training Loss (progress: 0.08), 0.13275120383837966, [0.004450766385850769, 0.0007897614629890166, 0.0022752463330984016, 0.06109230482384098, 0.08419420995689642, 0.1027135078446226], [0.9478203743005521, 1.0216422822119722, 1.121413291254484, 1.0754414166712871, 1.057342315633074, 1.135257616754148]
Training Loss (progress: 0.16), 0.11425312721137275, [0.0044000068623829105, 0.000797816053887484, 0.002239870870615002, 0.06157467250392933, 0.0847730670081265, 0.10279007704012363], [0.9477162634593451, 1.0214196309675228, 1.1215776390963115, 1.0757620308576723, 1.0578257184406896, 1.1355946759054458]
Training Loss (progress: 0.24), 0.13313431931595335, [0.004256152744357904, 0.0007657638696881094, 0.00230605574903832, 0.06218776363340703, 0.08513611643176361, 0.10299109496725986], [0.947605933299787, 1.0213642519404575, 1.1217518726185214, 1.0763506600238677, 1.0581214251921787, 1.1359459449354339]
Training Loss (progress: 0.32), 0.12778205982521934, [0.004328167350958518, 0.0007931913750732568, 0.002263093335177985, 0.06283818000158393, 0.08549661453112403, 0.10316548507055973], [0.9475426242100546, 1.0212539190761782, 1.1221313221974212, 1.076802174307212, 1.0584146426921042, 1.136433953328008]
Training Loss (progress: 0.40), 0.12882908830414955, [0.004305914321108769, 0.0007856111373913602, 0.0022048382034999476, 0.06357239655065472, 0.08592741537093902, 0.10311657218696613], [0.9474330297515012, 1.0209401836078702, 1.1222075651680086, 1.077422929330473, 1.0587079226333076, 1.1366848330974468]
Training Loss (progress: 0.48), 0.1308744430197988, [0.004372782567669052, 0.0008022303069451924, 0.0021994556197245125, 0.06449346007956332, 0.08631856604030513, 0.10345906961904396], [0.9474734173026631, 1.0207121974018343, 1.1225993914042844, 1.0782321066426819, 1.0590815993413385, 1.1373528172324174]
Training Loss (progress: 0.56), 0.12923742725167683, [0.004243750007770085, 0.0007974755664016059, 0.0022896362459537732, 0.06520106706565465, 0.08681003418068907, 0.10356401916021821], [0.9472295382842537, 1.020600183654866, 1.1229102856290374, 1.0787723579050108, 1.0594713178388655, 1.1375913866095353]
Training Loss (progress: 0.64), 0.1308952304534638, [0.004356204855643588, 0.0008192164060482717, 0.00226312083129155, 0.06565686464330364, 0.08716860286740297, 0.10382010493147427], [0.9472483638403494, 1.0204810053821174, 1.123206518493391, 1.0790401851426255, 1.0598496050610358, 1.1381299602846338]
Training Loss (progress: 0.72), 0.12308809756569702, [0.004363273166510268, 0.0008010109307913083, 0.002291133223045861, 0.06612511814719174, 0.08765809914965847, 0.10383311199460991], [0.9471841349623926, 1.0203298097417126, 1.1233277037518876, 1.0793940318372053, 1.0602618896991158, 1.138493136531411]
Training Loss (progress: 0.80), 0.13812300541183867, [0.004297797627143028, 0.00081914857033409, 0.0022161466009467307, 0.06700195714543626, 0.08785495540066196, 0.10401813019107907], [0.9471279356983003, 1.0202144884769317, 1.123568225520837, 1.0801416219925017, 1.0603435924252933, 1.1389095939654033]
Training Loss (progress: 0.88), 0.11924582870418815, [0.004323627347254973, 0.0008117352545737071, 0.002244704338599641, 0.06744912258519363, 0.08818372697292391, 0.10414198535307304], [0.9468697933580105, 1.0202006421866072, 1.1238431027885079, 1.0805268726802326, 1.0605788027348089, 1.139249332246358]
Training Loss (progress: 0.96), 0.1267900911129234, [0.004269253746511898, 0.0007897373475300233, 0.002269735816276839, 0.06790458170163022, 0.08840803203885436, 0.10423449910708042], [0.9467564591240557, 1.0198067087352578, 1.1241409669110334, 1.0809942753551647, 1.0607053956669323, 1.139553268681994]
Evaluation on validation dataset:
Step 25, mean loss 0.02766734560227525
Step 50, mean loss 0.012899776947106572
Step 75, mean loss 0.015315263288310208
Step 100, mean loss 0.020141664628313585
Step 125, mean loss 0.03067690577418911
Step 150, mean loss 0.03273444979395409
Step 175, mean loss 0.039015631363163925
Step 200, mean loss 0.054206501767397726
Step 225, mean loss 0.07032511642821757
Unrolled forward losses 1.492990105977671
Unrolled forward base losses 1.1720234445357585
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.01713e-01, 7.90219e-03, 2.12934e-01, 1.02435e+00, 1.01784e+00, 1.10013e+00
Node: 01 (pos: 0.010): 5.22010e-01, 3.48895e-02, 3.54025e-01, 1.04138e+00, 1.03080e+00, 1.11203e+00
Node: 02 (pos: 0.020): 6.46780e-01, 1.17592e-01, 5.36637e-01, 1.05553e+00, 1.04153e+00, 1.12187e+00
Node: 03 (pos: 0.030): 7.64102e-01, 3.02552e-01, 7.41622e-01, 1.06667e+00, 1.04996e+00, 1.12958e+00
Node: 04 (pos: 0.040): 8.60719e-01, 5.94236e-01, 9.34419e-01, 1.07469e+00, 1.05602e+00, 1.13512e+00
Node: 05 (pos: 0.051): 9.24458e-01, 8.90950e-01, 1.07339e+00, 1.07954e+00, 1.05967e+00, 1.13846e+00
-
Node: 07 (pos: 0.071): 9.24458e-01, 8.90950e-01, 1.07339e+00, 1.07954e+00, 1.05967e+00, 1.13846e+00
Node: 08 (pos: 0.081): 8.60719e-01, 5.94236e-01, 9.34419e-01, 1.07469e+00, 1.05602e+00, 1.13512e+00
Node: 09 (pos: 0.091): 7.64102e-01, 3.02552e-01, 7.41622e-01, 1.06667e+00, 1.04996e+00, 1.12958e+00
Node: 10 (pos: 0.101): 6.46780e-01, 1.17592e-01, 5.36637e-01, 1.05553e+00, 1.04153e+00, 1.12187e+00
Node: 11 (pos: 0.111): 5.22010e-01, 3.48895e-02, 3.54025e-01, 1.04138e+00, 1.03080e+00, 1.11203e+00
Node: 12 (pos: 0.121): 4.01713e-01, 7.90219e-03, 2.12934e-01, 1.02435e+00, 1.01784e+00, 1.10013e+00
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.24458e-01, 8.90950e-01, 1.07339e+00, 1.07954e+00, 1.05967e+00, 1.13846e+00
Node: 58 (pos: 0.586): 8.60719e-01, 5.94236e-01, 9.34419e-01, 1.07469e+00, 1.05602e+00, 1.13512e+00
Node: 59 (pos: 0.596): 7.64102e-01, 3.02552e-01, 7.41622e-01, 1.06667e+00, 1.04996e+00, 1.12958e+00
Node: 60 (pos: 0.606): 6.46780e-01, 1.17592e-01, 5.36637e-01, 1.05553e+00, 1.04153e+00, 1.12187e+00
Node: 61 (pos: 0.616): 5.22010e-01, 3.48895e-02, 3.54025e-01, 1.04138e+00, 1.03080e+00, 1.11203e+00
Node: 50 (pos: 0.505): 4.01713e-01, 7.90219e-03, 2.12934e-01, 1.02435e+00, 1.01784e+00, 1.10013e+00
-
Node: 51 (pos: 0.515): 5.22010e-01, 3.48895e-02, 3.54025e-01, 1.04138e+00, 1.03080e+00, 1.11203e+00
Node: 52 (pos: 0.525): 6.46780e-01, 1.17592e-01, 5.36637e-01, 1.05553e+00, 1.04153e+00, 1.12187e+00
Node: 53 (pos: 0.535): 7.64102e-01, 3.02552e-01, 7.41622e-01, 1.06667e+00, 1.04996e+00, 1.12958e+00
Node: 54 (pos: 0.545): 8.60719e-01, 5.94236e-01, 9.34419e-01, 1.07469e+00, 1.05602e+00, 1.13512e+00
Node: 55 (pos: 0.556): 9.24458e-01, 8.90950e-01, 1.07339e+00, 1.07954e+00, 1.05967e+00, 1.13846e+00
Node: 62 (pos: 0.626): 4.01713e-01, 7.90219e-03, 2.12934e-01, 1.02435e+00, 1.01784e+00, 1.10013e+00
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.02625202947473559
Step 50, mean loss 0.013603510835665768
Step 75, mean loss 0.017182561680486538
Step 100, mean loss 0.023335416743761307
Step 125, mean loss 0.029971860334154367
Step 150, mean loss 0.05066169860728678
Step 175, mean loss 0.06170261915176348
Step 200, mean loss 0.06985482542908918
Step 225, mean loss 0.09514277092198728
Unrolled forward losses 1.872216641000386
Unrolled forward base losses 1.4476105995166209
Saved model at models/GNN_euclidean_CE_E2_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time6141232.tar

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.1259458422015351, [0.004323409733852951, 0.0007563132871938434, 0.002182882595848798, 0.06803328075767028, 0.08862196433459434, 0.10424670965980129], [0.9467789024974391, 1.0197198507543932, 1.1241776606646763, 1.081156476291776, 1.0608296318740207, 1.1395666688987323]
Training Loss (progress: 0.08), 0.12232757736298115, [0.004247600104403037, 0.0007393324845506957, 0.0022461947891970753, 0.06879091654891681, 0.08913548810095208, 0.10443216349097975], [0.9466097172098464, 1.0194222407800664, 1.1243854123576822, 1.0817276761163095, 1.0612625787454741, 1.1400217228403262]
Training Loss (progress: 0.16), 0.12279468944315114, [0.004347009313166719, 0.0008108189859025693, 0.0021758662529704504, 0.06966629760241932, 0.08942029420501023, 0.1043524626200094], [0.9464282130656123, 1.019330573957057, 1.124787065331037, 1.0823476410997304, 1.0613794546500341, 1.1400937738820935]
Training Loss (progress: 0.24), 0.12340340558275362, [0.004187383038290985, 0.0007896173168215245, 0.0022121260737456255, 0.070316357868493, 0.08980894808147433, 0.10453165735386577], [0.9462847167093141, 1.0192364515529955, 1.1249276577824971, 1.0829454178089633, 1.0617635431809636, 1.1405413700883549]
Training Loss (progress: 0.32), 0.1330434333036108, [0.004260878239276656, 0.0007956458306836113, 0.0022376328119329686, 0.0712585030178257, 0.09016114861434778, 0.10479822132199931], [0.9462785469189482, 1.018975205669407, 1.1250707015659005, 1.0835393225620233, 1.0619131646250617, 1.141037764889664]
Training Loss (progress: 0.40), 0.11275795509115613, [0.0043202621645952125, 0.0007913909571141382, 0.002191086513243963, 0.07182966572842456, 0.09042022933741259, 0.10488416868397384], [0.9462269502889613, 1.018859769823176, 1.125259039729404, 1.0839642085645653, 1.062030646778592, 1.1412169357142448]
Training Loss (progress: 0.48), 0.12345566784323952, [0.004325558189674638, 0.0007769158043382573, 0.0021781914788791376, 0.07258840025602321, 0.09086976316811356, 0.10508801531619143], [0.9461018975890922, 1.0186827516967714, 1.1255451327411836, 1.0845445245200633, 1.062312658414095, 1.141622184163458]
Training Loss (progress: 0.56), 0.1207067570745728, [0.004188447378467546, 0.0007740427924371704, 0.0022067978663134753, 0.07321044731893607, 0.09118700172416341, 0.10513589455042058], [0.9459470967474196, 1.0184970456715245, 1.1258355065121377, 1.0849620825237982, 1.0626233403830576, 1.1419880012484955]
Training Loss (progress: 0.64), 0.12991517120334165, [0.004106631852008171, 0.0007523922474804071, 0.0022524152262998753, 0.07402700920675724, 0.09166561063956194, 0.1051765199888202], [0.9456507068029919, 1.018238187976796, 1.1259409669124345, 1.085574069372704, 1.0628977507113069, 1.1421789378610954]
Training Loss (progress: 0.72), 0.129947827993911, [0.004188296162900037, 0.000792380578936728, 0.0022117302976240793, 0.0745895146874773, 0.09187614519682602, 0.10515361726735467], [0.9455897164377477, 1.0182271681612387, 1.126340003914316, 1.0862500913900555, 1.0630746471693049, 1.1425122766261306]
Training Loss (progress: 0.80), 0.13574443256784885, [0.004305219885652068, 0.0007825871591879753, 0.0021620540619627744, 0.07526173489329839, 0.09215947764208023, 0.10541022348612329], [0.9453372864534496, 1.01803438873637, 1.1263695503951314, 1.0866422162907916, 1.0632485844177337, 1.142905509402736]
Training Loss (progress: 0.88), 0.12598262186353662, [0.0042992480766048296, 0.0008079834749195103, 0.0021849975945826485, 0.07598354362642233, 0.09254802089219877, 0.10550082006781106], [0.9452421903009699, 1.0177767546512795, 1.1267272423896335, 1.0872090960453618, 1.0635087023172998, 1.1431722166460196]
Training Loss (progress: 0.96), 0.1249391799760297, [0.004249954401803661, 0.0008252324527263554, 0.0022406554010135703, 0.07664719436164405, 0.0928501378303642, 0.10549864321588723], [0.9450494386400007, 1.0175936801323275, 1.1270178377917603, 1.0877000748401673, 1.0637070251580678, 1.1434113074738934]
Evaluation on validation dataset:
Step 25, mean loss 0.02327064459346119
Step 50, mean loss 0.01158462151837231
Step 75, mean loss 0.016840271037874832
Step 100, mean loss 0.02319109433670489
Step 125, mean loss 0.03258267298425503
Step 150, mean loss 0.03491488134480552
Step 175, mean loss 0.038695231694964555
Step 200, mean loss 0.0517525171770581
Step 225, mean loss 0.06870856574880153
Unrolled forward losses 1.5304112668210563
Unrolled forward base losses 1.1720234445357585
Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.11564769424343695, [0.004191632361980962, 0.0007901867834387157, 0.002163974502289922, 0.07686153912931583, 0.092872909808929, 0.1055158301732044], [0.9450255857933818, 1.0175051359476575, 1.1269703953036418, 1.0878883974141296, 1.0636749270081929, 1.1434897318168307]
Training Loss (progress: 0.08), 0.13525166108939327, [0.0042984563678015635, 0.0007501078017604192, 0.00224884681208257, 0.0773245215894284, 0.09351900718543994, 0.10588883744815507], [0.9449943310904411, 1.0174085584218278, 1.1271364624503573, 1.0882636046822864, 1.0640967871153708, 1.143829434744071]
Training Loss (progress: 0.16), 0.12535470879797647, [0.004182881182076617, 0.0008283701680369427, 0.0022250000122438383, 0.07797281256712947, 0.09387610927318374, 0.10586728019783705], [0.9448614284113622, 1.0172171712743372, 1.1274214057246252, 1.088685086297476, 1.064337493546166, 1.1441172298295474]
Training Loss (progress: 0.24), 0.11766844964337446, [0.004196137782220546, 0.0008162714758611418, 0.002194899364450685, 0.07854971858769337, 0.09415393951830969, 0.10612861534930924], [0.9446505869756638, 1.0170189641617782, 1.1274433603084857, 1.089153860544452, 1.0645112023892174, 1.1445589189954966]
Training Loss (progress: 0.32), 0.12349416682289129, [0.0042034439579455305, 0.0007833671720265331, 0.0021864575603102263, 0.07925254124241157, 0.09436686061593741, 0.10601318632274415], [0.9445018065637281, 1.0167869155898308, 1.1278079539260444, 1.0895806535208041, 1.0645975537608814, 1.144628770434754]
Training Loss (progress: 0.40), 0.12342197374987132, [0.004252966653512717, 0.0007692820033364414, 0.0021793871115968182, 0.07993085744523333, 0.09479293983442662, 0.10625257385196074], [0.9444607603104884, 1.0164805275879458, 1.1278251468004736, 1.0901940648889903, 1.0648182560236095, 1.144906311628516]
Training Loss (progress: 0.48), 0.13194779297180048, [0.004036656956046997, 0.0007994474101821172, 0.0022234083869183616, 0.08067576779249214, 0.09532763706885558, 0.10625566213828175], [0.9442706294178242, 1.0164362019072395, 1.1283462148101135, 1.0907600525617274, 1.0651803334811856, 1.1450962357891152]
Training Loss (progress: 0.56), 0.11133701355107249, [0.004093848420308059, 0.0007808742903160056, 0.0022241050364320046, 0.08110488526668627, 0.09563042872497604, 0.1063432819134801], [0.9440568382153044, 1.0161841867794266, 1.1283749654047692, 1.0911114929472463, 1.0652923682189406, 1.145520348337072]
Training Loss (progress: 0.64), 0.10855831600650405, [0.0041262894978879704, 0.000744946650726503, 0.002229237353563986, 0.08215828561301153, 0.09591537292911584, 0.10652723466739066], [0.9440927797801701, 1.0160444801165944, 1.1284574946050758, 1.0919727556550491, 1.0654063024685845, 1.1457749312810246]
Training Loss (progress: 0.72), 0.11845497240220834, [0.004194144179380473, 0.0007890376659086661, 0.0022355849592840473, 0.0822259558501925, 0.09620971858143085, 0.10657651119750298], [0.9440138961596075, 1.015727877470819, 1.1287391881621918, 1.0919987355394685, 1.0654843006220844, 1.1459352589014034]
Training Loss (progress: 0.80), 0.11997349433626221, [0.004235545949208603, 0.0007904552520703515, 0.0021810147519887757, 0.08323267532336054, 0.09657040250288454, 0.10684202242031812], [0.9437554396296184, 1.015692452084762, 1.1287237697344934, 1.0927498550458417, 1.0656886974649096, 1.1463853555560901]
Training Loss (progress: 0.88), 0.11556743914563941, [0.004156134876569966, 0.000823179886463826, 0.0021972904536720537, 0.08360878215366581, 0.09705007291383408, 0.10703388337758103], [0.9435279014043125, 1.0154577994425058, 1.129116451588709, 1.0931704106659879, 1.0660163350832774, 1.1467193143137717]
Training Loss (progress: 0.96), 0.12600148792070906, [0.0041784372215258915, 0.0008011875273452862, 0.0022593301180067254, 0.08437334224894312, 0.09734405059561745, 0.10725089537289707], [0.9434549903965008, 1.0152769879506944, 1.1291687089312397, 1.0936563378207165, 1.0661971324931887, 1.1470288223290166]
Evaluation on validation dataset:
Step 25, mean loss 0.024194889712845842
Step 50, mean loss 0.012673563527257752
Step 75, mean loss 0.01855419084648912
Step 100, mean loss 0.0226454509869692
Step 125, mean loss 0.031419791226364936
Step 150, mean loss 0.034717815622240585
Step 175, mean loss 0.03923393252393016
Step 200, mean loss 0.05474685751977946
Step 225, mean loss 0.07103385195621983
Unrolled forward losses 1.7214630398353978
Unrolled forward base losses 1.1720234445357585
Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.13691499104002297, [0.0040485377079226195, 0.0008350073892986723, 0.0021568750941649106, 0.08486892720439355, 0.09752208773218378, 0.10724273258696418], [0.943249214234794, 1.0153874539175078, 1.1293072543547071, 1.0939903167686849, 1.066337132021203, 1.1471190854520084]
Training Loss (progress: 0.08), 0.11514149167573684, [0.00417399238639234, 0.0008384355212135554, 0.002207021941568961, 0.08568156449045052, 0.09787812932931686, 0.10732215263273003], [0.9430293699675606, 1.0151605491833287, 1.1296401852034792, 1.094502993408834, 1.0664936299923695, 1.147403727254745]
Training Loss (progress: 0.16), 0.11244351130596403, [0.004143963398677546, 0.0007993327639286175, 0.0022626999336565422, 0.08615869494537547, 0.0981715988315843, 0.10740394290121522], [0.942947160921659, 1.014839731104782, 1.1295592005386585, 1.0949798605657222, 1.06660711636785, 1.147535413412195]
Training Loss (progress: 0.24), 0.11913307984935684, [0.004183447745448515, 0.0007700550284040845, 0.0022686870638931067, 0.08702978674278525, 0.09864523708007855, 0.10772482511442877], [0.9428296954981487, 1.014590142957416, 1.1298323127357714, 1.0956404444025438, 1.0669686722999692, 1.1480819688563308]
Training Loss (progress: 0.32), 0.11583876089680469, [0.004072796681410968, 0.0007882246401742653, 0.0022148159178479827, 0.08761860353351743, 0.09890484455043053, 0.10793718453517277], [0.9424628453462162, 1.014536066599442, 1.1300217848308283, 1.0960771485812812, 1.0670914100274023, 1.1485070027579154]
Training Loss (progress: 0.40), 0.11674272290631255, [0.004085246188498179, 0.0007960070944110514, 0.0022661510259384923, 0.08816369292945453, 0.09926632307287628, 0.10795612473751785], [0.942363993483063, 1.0143778550806852, 1.1301863517665578, 1.0965565620187925, 1.0673634637792542, 1.1486160946546542]
Training Loss (progress: 0.48), 0.10504617495683244, [0.004188436591945643, 0.0007830878584424418, 0.0022330922049111266, 0.08903386020881006, 0.09971087001015784, 0.10818984617225624], [0.9423288834049234, 1.0141185040578256, 1.1303053967506254, 1.0972021227606508, 1.067581355913882, 1.1490328339866815]
Training Loss (progress: 0.56), 0.10468958712850733, [0.004146679532967205, 0.0007906215627320452, 0.0022028442663099483, 0.08965139774135247, 0.09991088384158334, 0.10820144185348238], [0.9422025033216488, 1.0140220736330356, 1.130585604322351, 1.0976244963005155, 1.0676230615199027, 1.149221514742189]
Training Loss (progress: 0.64), 0.12787470886265634, [0.004131794183008612, 0.0008000632230454584, 0.0022710196942033662, 0.09024897635941523, 0.10019881262070537, 0.10832661471546136], [0.942191408789329, 1.0138886691418927, 1.1307820134440265, 1.0980390769274366, 1.0677714042443247, 1.1494256788872046]
Training Loss (progress: 0.72), 0.11034623917074951, [0.004232691208064734, 0.0008082727485617688, 0.0022471691002275427, 0.09063689957201017, 0.10063262822326574, 0.10853852933714662], [0.9419885199988717, 1.0137769680585604, 1.1311384151760344, 1.0984355915835855, 1.0680463443444796, 1.1499264206127382]
Training Loss (progress: 0.80), 0.11712902167498412, [0.004152649415870031, 0.0007896350839568287, 0.00223941825963408, 0.09141103254112544, 0.10087930424125713, 0.10859516027184611], [0.9416001796469989, 1.0134515824910142, 1.1310917013890835, 1.099252940742353, 1.0681179379422128, 1.1501840460481743]
Training Loss (progress: 0.88), 0.11670468020198242, [0.004052992255857384, 0.0008098156258614708, 0.002225940018099327, 0.09209150577874269, 0.10112001456907808, 0.10852854400046044], [0.9415058591064914, 1.0133069282924438, 1.1312964588145242, 1.099758117318224, 1.0681822938440635, 1.150243905244394]
Training Loss (progress: 0.96), 0.10907740749789005, [0.00410406014385426, 0.0007736654548500828, 0.002182570647666672, 0.09225322417636041, 0.10159553259316592, 0.10872241998377541], [0.9412812840325945, 1.0130681600309015, 1.131353552832207, 1.0999458417116932, 1.068468848064557, 1.150694010137274]
Evaluation on validation dataset:
Step 25, mean loss 0.019225722260759078
Step 50, mean loss 0.009810094492776325
Step 75, mean loss 0.01549372054148513
Step 100, mean loss 0.022689979263780864
Step 125, mean loss 0.03351134099432764
Step 150, mean loss 0.036476866376303575
Step 175, mean loss 0.0408699419498963
Step 200, mean loss 0.051851193278679014
Step 225, mean loss 0.06851751236180591
Unrolled forward losses 1.6754767380786557
Unrolled forward base losses 1.1720234445357585
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.11532511532819342, [0.004063863556284185, 0.0008230923029800278, 0.0021909164295912885, 0.09278750319255206, 0.10177918519541752, 0.10878681808895477], [0.9412377364947654, 1.013004249745534, 1.1314692568352962, 1.100266119140997, 1.068530093067686, 1.1507429062743049]
Training Loss (progress: 0.08), 0.11599081544781673, [0.00410252983706518, 0.0007826943826076739, 0.002165532738242365, 0.09319843133832902, 0.10219997976621484, 0.10912836654990414], [0.9410030705822634, 1.0128104899757748, 1.1317580195521637, 1.1005961604422367, 1.0688026673677389, 1.1511867108243592]
Training Loss (progress: 0.16), 0.11465823477090997, [0.004187746922689838, 0.0007914451448438142, 0.0022056734687995803, 0.09384251111590179, 0.10251411821992105, 0.10921958566822078], [0.9407904210217805, 1.0126078633510145, 1.1318119007484404, 1.1010808241484193, 1.0689333163912613, 1.1514312268463396]
Training Loss (progress: 0.24), 0.10478392926452372, [0.004116824696205511, 0.0007952627528084595, 0.002190895257592285, 0.09438215436978234, 0.10278234891604461, 0.10932960285667657], [0.9406454932490035, 1.012317071618089, 1.1318425127782017, 1.101502899162828, 1.0690249284878632, 1.1516536785632339]
Training Loss (progress: 0.32), 0.13099037647184394, [0.004137619973844769, 0.0008007538323478833, 0.002282649936745846, 0.09520808354602867, 0.1031257838578024, 0.10953168002682623], [0.9404856996079823, 1.0122702865153388, 1.1322586156029448, 1.1020831650295027, 1.0691939929947074, 1.1520060477079417]
Training Loss (progress: 0.40), 0.1226922383243904, [0.004217647145891387, 0.0007765131129483439, 0.002202252027954049, 0.09574032903791742, 0.10352072794233108, 0.1097290401364293], [0.9404122720276896, 1.0121086858942507, 1.1321820379969636, 1.1024391413201908, 1.0693847150820095, 1.152268428885715]
Training Loss (progress: 0.48), 0.12043850541070664, [0.004179473434631786, 0.0007860112540114486, 0.0021990423207875415, 0.09651654713047511, 0.10386458086250712, 0.1098867586770421], [0.9400976701604157, 1.0119745106585591, 1.1323431858967126, 1.1030327602001218, 1.069575454268607, 1.1526350948949338]
Training Loss (progress: 0.56), 0.12385632017928436, [0.004094220281955859, 0.0007931621265669673, 0.002211273057083712, 0.09717708065870337, 0.10425348664526429, 0.10993074766728284], [0.9397680695866184, 1.0118674861268944, 1.1324875795753109, 1.1034194378539095, 1.0698169658258445, 1.1526948233227454]
Training Loss (progress: 0.64), 0.11432783831506062, [0.004173954746601433, 0.0007733691208514848, 0.002181324392489766, 0.09783053557103835, 0.10451235212649913, 0.10985788781261986], [0.9397826059949003, 1.0116183198118192, 1.1326224260899296, 1.1039365928477236, 1.0698829124524287, 1.1528112380623823]
Training Loss (progress: 0.72), 0.1138989413244802, [0.004119308932801165, 0.0007926174308425407, 0.0022759065904467187, 0.09867720128392346, 0.1050834768428057, 0.11019878103730871], [0.9395259160650857, 1.0113912678397161, 1.1330305713820301, 1.1045158973869014, 1.0701616229599178, 1.1533483622741263]
Training Loss (progress: 0.80), 0.10717221810542576, [0.004122546502267083, 0.0007854117390303237, 0.0022220954430547343, 0.09912883413966204, 0.10536608825478631, 0.11028021197530836], [0.9393262463953326, 1.0113205120926998, 1.1329826897360802, 1.1049213788842347, 1.070376935296911, 1.15355234205138]
Training Loss (progress: 0.88), 0.11217268585217743, [0.004037115261089942, 0.0008054404674564269, 0.002269007304833307, 0.0999743811620981, 0.10564731292193204, 0.11047159038452314], [0.9391486323204415, 1.0110859170688564, 1.133268860639251, 1.1056214568783722, 1.0705065983940814, 1.1537827495995328]
