Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time4301237.tar
Number of parameters: 1031651.0
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
Node: 01 (pos: 0.010): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 02 (pos: 0.020): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 03 (pos: 0.030): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 04 (pos: 0.040): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 05 (pos: 0.051): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
-
Node: 07 (pos: 0.071): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 08 (pos: 0.081): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 09 (pos: 0.091): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 10 (pos: 0.101): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 11 (pos: 0.111): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 12 (pos: 0.121): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 58 (pos: 0.586): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 59 (pos: 0.596): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 60 (pos: 0.606): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 61 (pos: 0.616): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 50 (pos: 0.505): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
-
Node: 51 (pos: 0.515): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 52 (pos: 0.525): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 53 (pos: 0.535): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 54 (pos: 0.545): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 55 (pos: 0.556): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 62 (pos: 0.626): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
=========================================================================================================
Training Loss (progress: 0.00), 1.3400528445856565, [0.003472711753832865, 0.009791715406623722, 0.009991090796752453, 0.010062506198848593, 0.010453523384827888, 0.010067003434533888]
Training Loss (progress: 0.08), 0.2383564555751654, [0.003913455142506277, 0.0010268870823063388, 0.002734735862132257, 0.000741152102227571, 0.000873017693791207, 0.010775422853312821]
Training Loss (progress: 0.16), 0.19141202022054246, [0.0032799657651418753, 0.000958917572683537, 0.0033259255150632143, 0.0006694984885474871, 0.0009935138149950143, 0.01215599285856846]
Training Loss (progress: 0.24), 0.14832532721617186, [0.0031587830359875234, 0.0009982861514468418, 0.00394789641950018, 0.0008174239799840333, 0.0011026126094679856, 0.013844452613198055]
Training Loss (progress: 0.32), 0.13672847418867207, [0.003000339615912826, 0.000845477262082451, 0.00446800481078601, 0.0008107917100680848, 0.0011335023485232053, 0.014711886071394087]
Training Loss (progress: 0.40), 0.12479589251037365, [0.0029670930607301033, 0.000895878047877087, 0.004207399421329754, 0.0007896121762577215, 0.0012627642921982899, 0.016000863069483492]
Training Loss (progress: 0.48), 0.11289934130556603, [0.00294861082029045, 0.0009075347508526976, 0.00440939232961581, 0.0007985852284601374, 0.001266707164852848, 0.01759239355950243]
Training Loss (progress: 0.56), 0.11271110873810447, [0.003111591656784909, 0.0008735617411408921, 0.004690745926214361, 0.0007008401022732086, 0.0012565340805858608, 0.01828654406835995]
Training Loss (progress: 0.64), 0.11506406148479922, [0.003160382620017329, 0.0012940695295651328, 0.004774700924928331, 0.0008897485750337234, 0.0012326711815766702, 0.01955913924399698]
Training Loss (progress: 0.72), 0.10449391079093526, [0.003119831668124549, 0.000892302513606636, 0.004741680487069663, 0.0008464805744929715, 0.0012303281268722314, 0.020592629747412227]
Training Loss (progress: 0.80), 0.10570921554123126, [0.0031588560768085142, 0.000965447700568443, 0.005037316881722348, 0.0007378896828905075, 0.0012869115967909692, 0.022127297220560595]
Training Loss (progress: 0.88), 0.10070065055061025, [0.002941125776178694, 0.0008830257257647915, 0.005026702568129144, 0.0008784260091564697, 0.0011503174436401442, 0.02255412721394039]
Training Loss (progress: 0.96), 0.09253700816131681, [0.003190996575044781, 0.0007363993374515206, 0.005346955763960685, 0.0008240911181234812, 0.0013415905622820563, 0.023605958645424054]
Evaluation on validation dataset:
Step 25, mean loss 0.07208022855772186
Step 50, mean loss 0.08348476878795333
Step 75, mean loss 0.08432401748310314
Step 100, mean loss 0.22463192068494486
Step 125, mean loss 0.15074874455949092
Step 150, mean loss 0.11842112483027958
Step 175, mean loss 0.17080670481153104
Step 200, mean loss 0.3463094742396917
Step 225, mean loss 0.36223101207154096
Unrolled forward losses 17.217251004447238
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.97960e-01, 1.92567e-02, 5.16810e-01, 7.86187e-03, 6.27526e-02, 8.57602e-01
Node: 01 (pos: 0.010): 4.31352e-01, 6.43781e-02, 6.32302e-01, 3.45589e-02, 1.46225e-01, 8.98816e-01
Node: 02 (pos: 0.020): 5.83838e-01, 1.72819e-01, 7.45747e-01, 1.16059e-01, 2.92155e-01, 9.34005e-01
Node: 03 (pos: 0.030): 7.38821e-01, 3.72517e-01, 8.47877e-01, 2.97770e-01, 5.00504e-01, 9.62324e-01
Node: 04 (pos: 0.040): 8.74124e-01, 6.44760e-01, 9.29283e-01, 5.83673e-01, 7.35197e-01, 9.83076e-01
Node: 05 (pos: 0.051): 9.66926e-01, 8.96086e-01, 9.81832e-01, 8.74062e-01, 9.25978e-01, 9.95742e-01
-
Node: 07 (pos: 0.071): 9.66926e-01, 8.96086e-01, 9.81832e-01, 8.74062e-01, 9.25978e-01, 9.95742e-01
Node: 08 (pos: 0.081): 8.74124e-01, 6.44760e-01, 9.29283e-01, 5.83673e-01, 7.35197e-01, 9.83076e-01
Node: 09 (pos: 0.091): 7.38821e-01, 3.72517e-01, 8.47877e-01, 2.97770e-01, 5.00504e-01, 9.62324e-01
Node: 10 (pos: 0.101): 5.83838e-01, 1.72819e-01, 7.45747e-01, 1.16059e-01, 2.92155e-01, 9.34005e-01
Node: 11 (pos: 0.111): 4.31352e-01, 6.43781e-02, 6.32302e-01, 3.45589e-02, 1.46225e-01, 8.98816e-01
Node: 12 (pos: 0.121): 2.97960e-01, 1.92567e-02, 5.16810e-01, 7.86187e-03, 6.27526e-02, 8.57602e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.66926e-01, 8.96086e-01, 9.81832e-01, 8.74062e-01, 9.25978e-01, 9.95742e-01
Node: 58 (pos: 0.586): 8.74124e-01, 6.44760e-01, 9.29283e-01, 5.83673e-01, 7.35197e-01, 9.83076e-01
Node: 59 (pos: 0.596): 7.38821e-01, 3.72517e-01, 8.47877e-01, 2.97770e-01, 5.00504e-01, 9.62324e-01
Node: 60 (pos: 0.606): 5.83838e-01, 1.72819e-01, 7.45747e-01, 1.16059e-01, 2.92155e-01, 9.34005e-01
Node: 61 (pos: 0.616): 4.31352e-01, 6.43781e-02, 6.32302e-01, 3.45589e-02, 1.46225e-01, 8.98816e-01
Node: 50 (pos: 0.505): 2.97960e-01, 1.92567e-02, 5.16810e-01, 7.86187e-03, 6.27526e-02, 8.57602e-01
-
Node: 51 (pos: 0.515): 4.31352e-01, 6.43781e-02, 6.32302e-01, 3.45589e-02, 1.46225e-01, 8.98816e-01
Node: 52 (pos: 0.525): 5.83838e-01, 1.72819e-01, 7.45747e-01, 1.16059e-01, 2.92155e-01, 9.34005e-01
Node: 53 (pos: 0.535): 7.38821e-01, 3.72517e-01, 8.47877e-01, 2.97770e-01, 5.00504e-01, 9.62324e-01
Node: 54 (pos: 0.545): 8.74124e-01, 6.44760e-01, 9.29283e-01, 5.83673e-01, 7.35197e-01, 9.83076e-01
Node: 55 (pos: 0.556): 9.66926e-01, 8.96086e-01, 9.81832e-01, 8.74062e-01, 9.25978e-01, 9.95742e-01
Node: 62 (pos: 0.626): 2.97960e-01, 1.92567e-02, 5.16810e-01, 7.86187e-03, 6.27526e-02, 8.57602e-01
=========================================================================================================
