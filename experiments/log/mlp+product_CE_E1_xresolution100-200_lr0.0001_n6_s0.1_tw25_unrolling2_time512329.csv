Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar
Number of parameters: 1035593.0
Epoch 0
Starting epoch 0...
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.98366e-01, 4.81725e-01, 5.02992e-01, 4.90773e-01, 5.13781e-01, 5.00561e-01
Node: 01 (pos: 0.010): 4.96600e-01, 4.79460e-01, 5.02364e-01, 4.88469e-01, 5.12408e-01, 4.97841e-01
Node: 02 (pos: 0.020): 4.94856e-01, 4.77223e-01, 5.01702e-01, 4.86168e-01, 5.11055e-01, 4.95154e-01
Node: 03 (pos: 0.030): 4.93134e-01, 4.75014e-01, 5.01006e-01, 4.83869e-01, 5.09722e-01, 4.92500e-01
Node: 04 (pos: 0.040): 4.91435e-01, 4.72834e-01, 5.00276e-01, 4.81572e-01, 5.08409e-01, 4.89879e-01
Node: 05 (pos: 0.051): 4.89757e-01, 4.70682e-01, 4.99513e-01, 4.79278e-01, 5.07116e-01, 4.87291e-01
-
Node: 07 (pos: 0.071): 4.86468e-01, 4.66464e-01, 4.97886e-01, 4.74697e-01, 5.04591e-01, 4.82215e-01
Node: 08 (pos: 0.081): 4.84857e-01, 4.64397e-01, 4.97023e-01, 4.72411e-01, 5.03358e-01, 4.79727e-01
Node: 09 (pos: 0.091): 4.83267e-01, 4.62359e-01, 4.96127e-01, 4.70128e-01, 5.02146e-01, 4.77273e-01
Node: 10 (pos: 0.101): 4.81700e-01, 4.60349e-01, 4.95198e-01, 4.67848e-01, 5.00954e-01, 4.74852e-01
Node: 11 (pos: 0.111): 4.80155e-01, 4.58368e-01, 4.94237e-01, 4.65572e-01, 4.99782e-01, 4.72465e-01
Node: 12 (pos: 0.121): 4.78632e-01, 4.56415e-01, 4.93244e-01, 4.63299e-01, 4.98630e-01, 4.70112e-01

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 8.34347e-02, 9.82157e-02, 7.96579e-02, 8.99426e-02, 7.13818e-02, 8.16256e-02
Node: 01 (pos: 0.010): 8.49138e-02, 1.00378e-01, 8.01627e-02, 9.19947e-02, 7.23948e-02, 8.38723e-02
Node: 02 (pos: 0.020): 8.63949e-02, 1.02549e-01, 8.06975e-02, 9.40816e-02, 7.34044e-02, 8.61402e-02
Node: 03 (pos: 0.030): 8.78772e-02, 1.04728e-01, 8.12626e-02, 9.62035e-02, 7.44101e-02, 8.84281e-02
Node: 04 (pos: 0.040): 8.93603e-02, 1.06915e-01, 8.18585e-02, 9.83605e-02, 7.54115e-02, 9.07346e-02
Node: 05 (pos: 0.051): 9.08434e-02, 1.09108e-01, 8.24856e-02, 1.00553e-01, 7.64081e-02, 9.30585e-02
-
Node: 07 (pos: 0.071): 9.38076e-02, 1.13508e-01, 8.38351e-02, 1.05044e-01, 7.83854e-02, 9.77528e-02
Node: 08 (pos: 0.081): 9.52873e-02, 1.15712e-01, 8.45582e-02, 1.07343e-01, 7.93651e-02, 1.00120e-01
Node: 09 (pos: 0.091): 9.67647e-02, 1.17919e-01, 8.53141e-02, 1.09678e-01, 8.03385e-02, 1.02500e-01
Node: 10 (pos: 0.101): 9.82391e-02, 1.20126e-01, 8.61032e-02, 1.12049e-01, 8.13049e-02, 1.04890e-01
Node: 11 (pos: 0.111): 9.97100e-02, 1.22332e-01, 8.69258e-02, 1.14455e-01, 8.22641e-02, 1.07288e-01
Node: 12 (pos: 0.121): 1.01177e-01, 1.24538e-01, 8.77822e-02, 1.16898e-01, 8.32156e-02, 1.09695e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 4.54332e-01, 2.60707e-01, 6.07318e-01, 3.52056e-01, 5.80797e-01, 3.57866e-01
Node: 01 (pos: 0.010): 4.53784e-01, 2.59086e-01, 6.06379e-01, 3.50268e-01, 5.80158e-01, 3.56246e-01
Node: 02 (pos: 0.020): 4.53244e-01, 2.57486e-01, 6.05420e-01, 3.48484e-01, 5.79535e-01, 3.54662e-01
Node: 03 (pos: 0.030): 4.52712e-01, 2.55910e-01, 6.04443e-01, 3.46702e-01, 5.78928e-01, 3.53115e-01
Node: 04 (pos: 0.040): 4.52189e-01, 2.54355e-01, 6.03447e-01, 3.44924e-01, 5.78337e-01, 3.51603e-01
Node: 05 (pos: 0.051): 4.58406e-01, 2.72716e-01, 6.13319e-01, 3.64646e-01, 5.85741e-01, 3.70248e-01
-
Node: 07 (pos: 0.071): 4.57798e-01, 2.70929e-01, 6.12526e-01, 3.62839e-01, 5.84984e-01, 3.68365e-01
Node: 08 (pos: 0.081): 4.57199e-01, 2.69166e-01, 6.11711e-01, 3.61035e-01, 5.84244e-01, 3.66521e-01
Node: 09 (pos: 0.091): 4.56608e-01, 2.67427e-01, 6.10874e-01, 3.59233e-01, 5.83521e-01, 3.64715e-01
Node: 10 (pos: 0.101): 4.56026e-01, 2.65712e-01, 6.10016e-01, 3.57435e-01, 5.82815e-01, 3.62947e-01
Node: 11 (pos: 0.111): 4.55453e-01, 2.64020e-01, 6.09137e-01, 3.55639e-01, 5.82126e-01, 3.61216e-01
Node: 12 (pos: 0.121): 4.51674e-01, 2.52822e-01, 6.02434e-01, 3.43148e-01, 5.77761e-01, 3.50127e-01

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 1.26923e-01, 5.06777e-01, 2.50132e-02, 2.89549e-01, 3.42779e-02, 2.77848e-01
Node: 58 (pos: 0.586): 1.27557e-01, 5.11068e-01, 2.53000e-02, 2.93206e-01, 3.45333e-02, 2.81081e-01
Node: 59 (pos: 0.596): 1.28183e-01, 5.15307e-01, 2.55956e-02, 2.96885e-01, 3.47838e-02, 2.84263e-01
Node: 60 (pos: 0.606): 1.28801e-01, 5.19495e-01, 2.59000e-02, 3.00585e-01, 3.50293e-02, 2.87394e-01
Node: 61 (pos: 0.616): 1.29413e-01, 5.23633e-01, 2.62133e-02, 3.04306e-01, 3.52697e-02, 2.90472e-01
Node: 50 (pos: 0.505): 1.22290e-01, 4.75332e-01, 2.32465e-02, 2.64565e-01, 3.23571e-02, 2.53895e-01
-
Node: 51 (pos: 0.515): 1.22973e-01, 4.79973e-01, 2.34735e-02, 2.68066e-01, 3.26452e-02, 2.57450e-01
Node: 52 (pos: 0.525): 1.23649e-01, 4.84566e-01, 2.37089e-02, 2.71590e-01, 3.29289e-02, 2.60963e-01
Node: 53 (pos: 0.535): 1.24318e-01, 4.89109e-01, 2.39527e-02, 2.75137e-01, 3.32080e-02, 2.64432e-01
Node: 54 (pos: 0.545): 1.24980e-01, 4.93601e-01, 2.42050e-02, 2.78707e-01, 3.34826e-02, 2.67856e-01
Node: 55 (pos: 0.556): 1.25635e-01, 4.98044e-01, 2.44658e-02, 2.82299e-01, 3.37525e-02, 2.71235e-01
Node: 62 (pos: 0.626): 1.30017e-01, 5.27719e-01, 2.65357e-02, 3.08047e-01, 3.55050e-02, 2.93496e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.313272230969263, [0.11123211358553946, 0.11788778549664534, 0.11645483316432761, 0.11578161815579692, 0.11600728557634409, 0.11629017769194289]
Training Loss (progress: 0.08), 0.2852279834251369, [0.1253571715071361, 0.1384278170761175, 0.13225737427173023, 0.13490325868890293, 0.1363189255071886, 0.17139940943838552]
Training Loss (progress: 0.16), 0.21605717615712214, [0.12758351905733503, 0.13085548847497727, 0.1351277162548116, 0.13524121282340917, 0.14617327188855506, 0.18428172599506007]
Training Loss (progress: 0.24), 0.18788613301441925, [0.12836449365996908, 0.1238115822272652, 0.1394030010816706, 0.13485647823109914, 0.15754576689487873, 0.1987595056247965]
Training Loss (progress: 0.32), 0.15853322919935658, [0.12905026745342024, 0.1193844072525297, 0.1430036738672444, 0.134996438608705, 0.1653065113444301, 0.20471831373142935]
Training Loss (progress: 0.40), 0.15316356684142068, [0.12954951356532468, 0.11657700952229073, 0.1461133988111481, 0.13527383548785377, 0.17243808679281197, 0.2046202575399918]
Training Loss (progress: 0.48), 0.13859950275635907, [0.12976291771474607, 0.11507325780944126, 0.14795422280904816, 0.13532533317989182, 0.1784469629046871, 0.20566433757034638]
Training Loss (progress: 0.56), 0.12185433317056882, [0.13058428655324686, 0.11249763986784861, 0.1492310085426149, 0.1346355110587609, 0.18499547045571013, 0.20773052351899965]
Training Loss (progress: 0.64), 0.12127944816215089, [0.13098063152349815, 0.11108044686451803, 0.14990948011420704, 0.13490890462960586, 0.1906105711629924, 0.2113603400287207]
Training Loss (progress: 0.72), 0.12144355852185139, [0.13117237997323156, 0.11069732864106947, 0.1501990215793008, 0.13506981554896766, 0.1947406913368216, 0.21450128282483122]
Training Loss (progress: 0.80), 0.11385155217795588, [0.13199794774404466, 0.10897566625612054, 0.1506851024077577, 0.1344024815628055, 0.19864313688139998, 0.2181556202507463]
Training Loss (progress: 0.88), 0.10632545709877965, [0.13260930840991533, 0.10893231152277902, 0.1510117067082637, 0.13504973369387285, 0.20228243656323575, 0.21991785283601276]
Training Loss (progress: 0.96), 0.10088435846648776, [0.13307489553318957, 0.10744688326355714, 0.15065655761290475, 0.13404728533605095, 0.20663684149130762, 0.22580660807570427]
Evaluation on validation dataset:
Step 25, mean loss 0.08374248985353006
Step 50, mean loss 0.07351260083190234
Step 75, mean loss 0.09727071189110245
Step 100, mean loss 0.152706161880122
Step 125, mean loss 0.15539694332920806
Step 150, mean loss 0.15683163000608458
Step 175, mean loss 0.30860409435673913
Step 200, mean loss 0.6714584746723117
Step 225, mean loss 0.45933484558998317
Unrolled forward losses 14.949185693065493
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.22786e-01, 2.37919e-01, 3.83894e-01, 3.06130e-01, 2.16094e-01, 6.66537e-02
Node: 01 (pos: 0.010): 4.20861e-01, 2.27890e-01, 3.78274e-01, 2.96153e-01, 2.14208e-01, 6.28778e-02
Node: 02 (pos: 0.020): 4.18961e-01, 2.18196e-01, 3.72645e-01, 2.86415e-01, 2.12326e-01, 5.93161e-02
Node: 03 (pos: 0.030): 4.17086e-01, 2.08835e-01, 3.67007e-01, 2.76920e-01, 2.10450e-01, 5.59573e-02
Node: 04 (pos: 0.040): 4.15237e-01, 1.99806e-01, 3.61364e-01, 2.67669e-01, 2.08578e-01, 5.27904e-02
Node: 05 (pos: 0.051): 4.13412e-01, 1.91103e-01, 3.55717e-01, 2.58661e-01, 2.06711e-01, 4.98051e-02
-
Node: 07 (pos: 0.071): 4.09837e-01, 1.74658e-01, 3.44423e-01, 2.41379e-01, 2.02992e-01, 4.43396e-02
Node: 08 (pos: 0.081): 4.08086e-01, 1.66904e-01, 3.38780e-01, 2.33102e-01, 2.01140e-01, 4.18408e-02
Node: 09 (pos: 0.091): 4.06360e-01, 1.59455e-01, 3.33143e-01, 2.25065e-01, 1.99294e-01, 3.94864e-02
Node: 10 (pos: 0.101): 4.04658e-01, 1.52303e-01, 3.27514e-01, 2.17267e-01, 1.97453e-01, 3.72682e-02
Node: 11 (pos: 0.111): 4.02979e-01, 1.45440e-01, 3.21897e-01, 2.09704e-01, 1.95617e-01, 3.51783e-02
Node: 12 (pos: 0.121): 4.01325e-01, 1.38860e-01, 3.16294e-01, 2.02374e-01, 1.93786e-01, 3.32094e-02

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.61837e-01, 5.89637e-01, 3.78062e-01, 4.95422e-01, 7.99482e-01, 9.80563e-01
Node: 01 (pos: 0.010): 2.65045e-01, 6.15912e-01, 3.88901e-01, 5.18244e-01, 8.02598e-01, 9.82684e-01
Node: 02 (pos: 0.020): 2.68235e-01, 6.41276e-01, 3.99905e-01, 5.40751e-01, 8.05690e-01, 9.84575e-01
Node: 03 (pos: 0.030): 2.71405e-01, 6.65649e-01, 4.11064e-01, 5.62868e-01, 8.08760e-01, 9.86261e-01
Node: 04 (pos: 0.040): 2.74555e-01, 6.88969e-01, 4.22368e-01, 5.84528e-01, 8.11806e-01, 9.87763e-01
Node: 05 (pos: 0.051): 2.77685e-01, 7.11194e-01, 4.33808e-01, 6.05669e-01, 8.14828e-01, 9.89100e-01
-
Node: 07 (pos: 0.071): 2.83880e-01, 7.52256e-01, 4.57051e-01, 6.46195e-01, 8.20799e-01, 9.91351e-01
Node: 08 (pos: 0.081): 2.86944e-01, 7.71079e-01, 4.68830e-01, 6.65497e-01, 8.23748e-01, 9.92295e-01
Node: 09 (pos: 0.091): 2.89984e-01, 7.88773e-01, 4.80698e-01, 6.84118e-01, 8.26672e-01, 9.93135e-01
Node: 10 (pos: 0.101): 2.93001e-01, 8.05357e-01, 4.92641e-01, 7.02034e-01, 8.29571e-01, 9.93882e-01
Node: 11 (pos: 0.111): 2.95994e-01, 8.20861e-01, 5.04646e-01, 7.19230e-01, 8.32445e-01, 9.94547e-01
Node: 12 (pos: 0.121): 2.98962e-01, 8.35319e-01, 5.16699e-01, 7.35697e-01, 8.35294e-01, 9.95139e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.74727e-01, 1.44149e-05, 3.58541e-01, 4.79943e-03, 1.43968e-01, 1.00631e-04
Node: 01 (pos: 0.010): 3.74024e-01, 1.36856e-05, 3.51997e-01, 4.64666e-03, 1.41884e-01, 9.55113e-05
Node: 02 (pos: 0.020): 3.73326e-01, 1.29946e-05, 3.45492e-01, 4.49897e-03, 1.39823e-01, 9.06674e-05
Node: 03 (pos: 0.030): 3.72633e-01, 1.23398e-05, 3.39028e-01, 4.35619e-03, 1.37784e-01, 8.60835e-05
Node: 04 (pos: 0.040): 3.71946e-01, 1.17193e-05, 3.32609e-01, 4.21813e-03, 1.35768e-01, 8.17448e-05
Node: 05 (pos: 0.051): 3.79809e-01, 2.08005e-05, 4.05173e-01, 6.02839e-03, 1.59188e-01, 1.45732e-04
-
Node: 07 (pos: 0.071): 3.79065e-01, 1.97318e-05, 3.98445e-01, 5.83417e-03, 1.56946e-01, 1.38151e-04
Node: 08 (pos: 0.081): 3.78328e-01, 1.87202e-05, 3.91734e-01, 5.64656e-03, 1.54727e-01, 1.30987e-04
Node: 09 (pos: 0.091): 3.77596e-01, 1.77627e-05, 3.85042e-01, 5.46532e-03, 1.52530e-01, 1.24216e-04
Node: 10 (pos: 0.101): 3.76870e-01, 1.68562e-05, 3.78374e-01, 5.29021e-03, 1.50356e-01, 1.17816e-04
Node: 11 (pos: 0.111): 3.76150e-01, 1.59978e-05, 3.71733e-01, 5.12100e-03, 1.48204e-01, 1.11765e-04
Node: 12 (pos: 0.121): 3.71264e-01, 1.11312e-05, 3.26236e-01, 4.08462e-03, 1.33775e-01, 7.76376e-05

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 3.48996e-01, 1.00000e+00, 4.28071e-01, 9.99827e-01, 9.05442e-01, 1.00000e+00
Node: 58 (pos: 0.586): 3.50376e-01, 1.00000e+00, 4.41412e-01, 9.99838e-01, 9.08030e-01, 1.00000e+00
Node: 59 (pos: 0.596): 3.51748e-01, 1.00000e+00, 4.54831e-01, 9.99848e-01, 9.10561e-01, 1.00000e+00
Node: 60 (pos: 0.606): 3.53113e-01, 1.00000e+00, 4.68309e-01, 9.99858e-01, 9.13034e-01, 1.00000e+00
Node: 61 (pos: 0.616): 3.54470e-01, 1.00000e+00, 4.81827e-01, 9.99867e-01, 9.15450e-01, 1.00000e+00
Node: 50 (pos: 0.505): 3.39105e-01, 1.00000e+00, 3.38402e-01, 9.99728e-01, 8.85640e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 3.40543e-01, 1.00000e+00, 3.50696e-01, 9.99745e-01, 8.88653e-01, 1.00000e+00
Node: 52 (pos: 0.525): 3.41972e-01, 1.00000e+00, 3.63189e-01, 9.99761e-01, 8.91604e-01, 1.00000e+00
Node: 53 (pos: 0.535): 3.43394e-01, 1.00000e+00, 3.75864e-01, 9.99776e-01, 8.94493e-01, 1.00000e+00
Node: 54 (pos: 0.545): 3.44806e-01, 1.00000e+00, 3.88707e-01, 9.99790e-01, 8.97321e-01, 1.00000e+00
Node: 55 (pos: 0.556): 3.46211e-01, 1.00000e+00, 4.01701e-01, 9.99803e-01, 9.00088e-01, 1.00000e+00
Node: 62 (pos: 0.626): 3.55819e-01, 1.00000e+00, 4.95366e-01, 9.99875e-01, 9.17811e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 1.0761369164310786), ('0.bias', 1.3390670097366035), ('2.weight', 9.14570422540262), ('2.bias', 1.8111068174333882)] 

GNN_Layer 1 gradients:
[('0.weight', 0.006969334964809687), ('0.bias', 0.17599491063226516), ('2.weight', 0.060061270741857985), ('2.bias', 0.14110896287205793)] 

GNN_Layer 2 gradients:
[('0.weight', 0.2579909309258456), ('0.bias', 0.5740984356687266), ('2.weight', 1.8208596597309001), ('2.bias', 0.6405870750977822)] 

GNN_Layer 3 gradients:
[('0.weight', 0.0058467896090934595), ('0.bias', 0.006885800220923658), ('2.weight', 0.04133006170280312), ('2.bias', 0.004943693090919524)] 

GNN_Layer 4 gradients:
[('0.weight', 0.07558917215508854), ('0.bias', 0.17088827230680562), ('2.weight', 0.5409205459872846), ('2.bias', 0.1930027077616773)] 

GNN_Layer 5 gradients:
[('0.weight', 0.00022212195504067486), ('0.bias', 0.002594735337928598), ('2.weight', 0.0025251584199471938), ('2.bias', 0.002043624563486479)] 

Evaluation on test dataset:
Step 25, mean loss 0.07122732379063748
Step 50, mean loss 0.05701981847409475
Step 75, mean loss 0.0827690654860701
Step 100, mean loss 0.11752381413270702
Step 125, mean loss 0.14202402427801125
Step 150, mean loss 0.18225058056407545
Step 175, mean loss 0.3041451221779776
Step 200, mean loss 0.24839505202678364
Step 225, mean loss 0.2433270734968634
Unrolled forward losses 14.579390442787808
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.2367033094773671, [0.13329874639295952, 0.10727717518275111, 0.15146062956544667, 0.13370218108153303, 0.20919402975125737, 0.2263325659354322]
Training Loss (progress: 0.08), 0.22130481078888944, [0.13473254771294377, 0.10470694076330914, 0.15010820229243957, 0.13073897543146262, 0.21288998329203254, 0.2291605188929742]
Training Loss (progress: 0.16), 0.1812957702444861, [0.1354213280379612, 0.1042921902285432, 0.14894589761599378, 0.12888351296383666, 0.21529006699213385, 0.2347677679724903]
Training Loss (progress: 0.24), 0.1937981755992542, [0.136383497702159, 0.10208857132386291, 0.14816820646172413, 0.1281636848193095, 0.21807529611278187, 0.23835534309572198]
Training Loss (progress: 0.32), 0.21109699238387641, [0.1372958788707461, 0.10268190973915581, 0.14662098315368968, 0.12917822447242341, 0.22167879287547362, 0.24452645347459764]
Training Loss (progress: 0.40), 0.18931978299355975, [0.1379568273471562, 0.1012290828785781, 0.14568338231751712, 0.12834994236264458, 0.22427485897522484, 0.24800316012161355]
Training Loss (progress: 0.48), 0.17623474537976877, [0.13863498988735526, 0.10111100340167288, 0.14367736933122857, 0.12739420518598157, 0.22666596391062624, 0.251669806540211]
Training Loss (progress: 0.56), 0.1675841487290118, [0.13890790068342143, 0.10034419597917524, 0.1418116755927398, 0.1270743645434651, 0.22954799820484525, 0.2560713058354728]
Training Loss (progress: 0.64), 0.17557518094763144, [0.13934519250010696, 0.10038259030028931, 0.1401572244760739, 0.12624914001523746, 0.23118399272041282, 0.25866924908517475]
Training Loss (progress: 0.72), 0.18002759803750076, [0.1399333563926055, 0.09924247890485405, 0.13882768431604733, 0.12531492772961234, 0.2337275862979239, 0.26133220276880387]
Training Loss (progress: 0.80), 0.16836176837753344, [0.140288072776402, 0.09953133007057885, 0.13710039376266023, 0.12554580149529912, 0.23668368766290734, 0.2649351607164242]
Training Loss (progress: 0.88), 0.16643781292511764, [0.1402846252875687, 0.09874141273825457, 0.1355537238013111, 0.12601644673217624, 0.23937584221171604, 0.2663374768084789]
Training Loss (progress: 0.96), 0.1510299519792105, [0.14007544195680585, 0.09860748466722614, 0.13313685027366315, 0.1262944652038219, 0.24254378420893039, 0.2704178958721071]
Evaluation on validation dataset:
Step 25, mean loss 0.08800422502239141
Step 50, mean loss 0.06668382102303567
Step 75, mean loss 0.09108906161309596
Step 100, mean loss 0.115867431460163
Step 125, mean loss 0.12994937938582057
Step 150, mean loss 0.12145726633084039
Step 175, mean loss 0.2100219051881833
Step 200, mean loss 0.5685313489863674
Step 225, mean loss 0.3751732766400807
Unrolled forward losses 4.8925259324911305
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.12590e-01, 2.25113e-01, 4.09645e-01, 2.99345e-01, 1.17662e-01, 1.76538e-02
Node: 01 (pos: 0.010): 4.10091e-01, 2.15499e-01, 3.99380e-01, 2.86845e-01, 1.16446e-01, 1.66259e-02
Node: 02 (pos: 0.020): 4.07618e-01, 2.06214e-01, 3.89154e-01, 2.74715e-01, 1.15233e-01, 1.56612e-02
Node: 03 (pos: 0.030): 4.05169e-01, 1.97255e-01, 3.78975e-01, 2.62959e-01, 1.14024e-01, 1.47558e-02
Node: 04 (pos: 0.040): 4.02746e-01, 1.88620e-01, 3.68854e-01, 2.51581e-01, 1.12817e-01, 1.39057e-02
Node: 05 (pos: 0.051): 4.00349e-01, 1.80303e-01, 3.58799e-01, 2.40582e-01, 1.11614e-01, 1.31076e-02
-
Node: 07 (pos: 0.071): 3.95628e-01, 1.64603e-01, 3.38927e-01, 2.19720e-01, 1.09219e-01, 1.16542e-02
Node: 08 (pos: 0.081): 3.93305e-01, 1.57208e-01, 3.29127e-01, 2.09852e-01, 1.08026e-01, 1.09929e-02
Node: 09 (pos: 0.091): 3.91006e-01, 1.50107e-01, 3.19430e-01, 2.00355e-01, 1.06837e-01, 1.03716e-02
Node: 10 (pos: 0.101): 3.88732e-01, 1.43293e-01, 3.09844e-01, 1.91223e-01, 1.05653e-01, 9.78773e-03
Node: 11 (pos: 0.111): 3.86482e-01, 1.36759e-01, 3.00378e-01, 1.82450e-01, 1.04472e-01, 9.23893e-03
Node: 12 (pos: 0.121): 3.84256e-01, 1.30497e-01, 2.91038e-01, 1.74029e-01, 1.03295e-01, 8.72300e-03

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.96992e-01, 5.97011e-01, 2.80899e-01, 4.89059e-01, 9.44576e-01, 9.98850e-01
Node: 01 (pos: 0.010): 3.01379e-01, 6.23318e-01, 2.99117e-01, 5.18516e-01, 9.45684e-01, 9.98980e-01
Node: 02 (pos: 0.020): 3.05758e-01, 6.48663e-01, 3.17937e-01, 5.47491e-01, 9.46779e-01, 9.99095e-01
Node: 03 (pos: 0.030): 3.10128e-01, 6.72972e-01, 3.37313e-01, 5.75823e-01, 9.47861e-01, 9.99196e-01
Node: 04 (pos: 0.040): 3.14488e-01, 6.96189e-01, 3.57196e-01, 6.03371e-01, 9.48930e-01, 9.99286e-01
Node: 05 (pos: 0.051): 3.18837e-01, 7.18276e-01, 3.77528e-01, 6.30014e-01, 9.49986e-01, 9.99366e-01
-
Node: 07 (pos: 0.071): 3.27497e-01, 7.58977e-01, 4.19289e-01, 6.80206e-01, 9.52058e-01, 9.99499e-01
Node: 08 (pos: 0.081): 3.31806e-01, 7.77587e-01, 4.40582e-01, 7.03616e-01, 9.53074e-01, 9.99554e-01
Node: 09 (pos: 0.091): 3.36100e-01, 7.95051e-01, 4.62055e-01, 7.25840e-01, 9.54077e-01, 9.99603e-01
Node: 10 (pos: 0.101): 3.40377e-01, 8.11395e-01, 4.83633e-01, 7.46857e-01, 9.55067e-01, 9.99646e-01
Node: 11 (pos: 0.111): 3.44638e-01, 8.26650e-01, 5.05242e-01, 7.66658e-01, 9.56044e-01, 9.99685e-01
Node: 12 (pos: 0.121): 3.48880e-01, 8.40853e-01, 5.26808e-01, 7.85249e-01, 9.57007e-01, 9.99719e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.50631e-01, 4.90394e-06, 3.60529e-01, 1.15704e-03, 7.19173e-02, 1.95732e-05
Node: 01 (pos: 0.010): 3.49288e-01, 4.65259e-06, 3.49314e-01, 1.10683e-03, 7.05481e-02, 1.86079e-05
Node: 02 (pos: 0.020): 3.47953e-01, 4.41457e-06, 3.38247e-01, 1.05886e-03, 6.91998e-02, 1.76936e-05
Node: 03 (pos: 0.030): 3.46626e-01, 4.18913e-06, 3.27339e-01, 1.01304e-03, 6.78722e-02, 1.68276e-05
Node: 04 (pos: 0.040): 3.45306e-01, 3.97560e-06, 3.16600e-01, 9.69255e-04, 6.65652e-02, 1.60071e-05
Node: 05 (pos: 0.051): 3.60248e-01, 7.10818e-06, 4.42151e-01, 1.58159e-03, 8.20950e-02, 2.80449e-05
-
Node: 07 (pos: 0.071): 3.58851e-01, 6.73890e-06, 4.30249e-01, 1.51218e-03, 8.05769e-02, 2.66241e-05
Node: 08 (pos: 0.081): 3.57461e-01, 6.38950e-06, 4.18405e-01, 1.44593e-03, 7.90802e-02, 2.52805e-05
Node: 09 (pos: 0.091): 3.56079e-01, 6.05887e-06, 4.06632e-01, 1.38269e-03, 7.76050e-02, 2.40096e-05
Node: 10 (pos: 0.101): 3.54705e-01, 5.74596e-06, 3.94945e-01, 1.32231e-03, 7.61512e-02, 2.28073e-05
Node: 11 (pos: 0.111): 3.53339e-01, 5.44977e-06, 3.83356e-01, 1.26466e-03, 7.47187e-02, 2.16695e-05
Node: 12 (pos: 0.121): 3.43994e-01, 3.77331e-06, 3.06038e-01, 9.27419e-04, 6.52788e-02, 1.52296e-05

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.16114e-01, 1.00000e+00, 3.73990e-01, 9.99989e-01, 9.78924e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.18912e-01, 1.00000e+00, 3.97211e-01, 9.99990e-01, 9.79710e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.21702e-01, 1.00000e+00, 4.20752e-01, 9.99991e-01, 9.80471e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.24484e-01, 1.00000e+00, 4.44513e-01, 9.99992e-01, 9.81206e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.27257e-01, 1.00000e+00, 4.68393e-01, 9.99993e-01, 9.81916e-01, 1.00000e+00
Node: 50 (pos: 0.505): 3.96311e-01, 1.00000e+00, 2.27804e-01, 9.99980e-01, 9.72624e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 3.99163e-01, 1.00000e+00, 2.46424e-01, 9.99982e-01, 9.73614e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.02007e-01, 1.00000e+00, 2.65899e-01, 9.99983e-01, 9.74573e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.04844e-01, 1.00000e+00, 2.86177e-01, 9.99985e-01, 9.75501e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.07673e-01, 1.00000e+00, 3.07199e-01, 9.99986e-01, 9.76399e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.10494e-01, 1.00000e+00, 3.28895e-01, 9.99987e-01, 9.77269e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.30022e-01, 1.00000e+00, 4.92288e-01, 9.99993e-01, 9.82603e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.13672245921627865), ('0.bias', 0.17346077028382692), ('2.weight', 1.1187351031262103), ('2.bias', 0.24779647081806513)] 

GNN_Layer 1 gradients:
[('0.weight', 0.004210144930448686), ('0.bias', 0.12425299813820151), ('2.weight', 0.034504812907054656), ('2.bias', 0.0919433436267043)] 

GNN_Layer 2 gradients:
[('0.weight', 0.039936365724830454), ('0.bias', 0.11008087019730439), ('2.weight', 0.22210903226720052), ('2.bias', 0.10059482166454821)] 

GNN_Layer 3 gradients:
[('0.weight', 0.007337593006616792), ('0.bias', 0.024735669002564156), ('2.weight', 0.04909450423796526), ('2.bias', 0.026446129796419682)] 

GNN_Layer 4 gradients:
[('0.weight', 0.011894793603028623), ('0.bias', 0.027542293344327135), ('2.weight', 0.07586460035553627), ('2.bias', 0.027304908256816824)] 

GNN_Layer 5 gradients:
[('0.weight', 6.2073092529686e-06), ('0.bias', 2.8227172454586797e-05), ('2.weight', 4.010473530721114e-05), ('2.bias', 1.836827810615176e-05)] 

Evaluation on test dataset:
Step 25, mean loss 0.07057604700490033
Step 50, mean loss 0.04946351538389055
Step 75, mean loss 0.07204543947942182
Step 100, mean loss 0.1116828566462756
Step 125, mean loss 0.10344849608589499
Step 150, mean loss 0.12649384517693152
Step 175, mean loss 0.22451541985892676
Step 200, mean loss 0.20685757809522656
Step 225, mean loss 0.2323163511250056
Unrolled forward losses 4.153009091519698
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.20908869935473742, [0.14060310753924718, 0.09782166063070268, 0.1318898283795675, 0.1253460034131132, 0.24308156947263326, 0.27105535015980076]
Training Loss (progress: 0.08), 0.2097216701017406, [0.1410100961131611, 0.09846370486654712, 0.1317527482154821, 0.12620188916927294, 0.2444181654669845, 0.2721727068609698]
Training Loss (progress: 0.16), 0.1948044573313325, [0.1411016957175641, 0.09877732764282675, 0.13145578659389306, 0.12678786969960912, 0.24574356511149845, 0.2743249825282583]
Training Loss (progress: 0.24), 0.17506992166064408, [0.1412514412200613, 0.09811720075891257, 0.13044212041628883, 0.12674055386641753, 0.2465666058186025, 0.2750693908114609]
Training Loss (progress: 0.32), 0.17711045427572983, [0.14136691584454514, 0.09877280353383441, 0.12988315850566626, 0.12627225985337606, 0.24785706293398568, 0.2762534074545134]
Training Loss (progress: 0.40), 0.19288770827219573, [0.1417971411181459, 0.09818590884324443, 0.1293590512181284, 0.12688151234665893, 0.24908236710737716, 0.2777120608364777]
Training Loss (progress: 0.48), 0.20158854288008773, [0.14190858373772336, 0.09811805572168292, 0.12878786601074174, 0.12644696445283535, 0.2500175042907623, 0.2776918906540599]
Training Loss (progress: 0.56), 0.18983888168731403, [0.14205792809742823, 0.0982643953876046, 0.12810426985269874, 0.12663920932752515, 0.25063184932846877, 0.2786941375980559]
Training Loss (progress: 0.64), 0.19415877024019643, [0.14209794202099996, 0.09785669696882036, 0.12757095551415965, 0.12692921525757342, 0.2517681754951739, 0.2792976114668238]
Training Loss (progress: 0.72), 0.180321230106199, [0.1418758640059924, 0.09800620095609215, 0.12718154237231588, 0.12697311207063888, 0.2531649709791206, 0.2805800380426494]
Training Loss (progress: 0.80), 0.2036924943468789, [0.14186548269600396, 0.09877966462930163, 0.12660157732622876, 0.1276061951474014, 0.2544765762933339, 0.2807117100665338]
Training Loss (progress: 0.88), 0.1762380073285578, [0.14220813324848325, 0.09796372779084057, 0.1261047046991753, 0.1268982913332203, 0.25488736152397795, 0.2819257309215426]
Training Loss (progress: 0.96), 0.18879581129247738, [0.14169351678143824, 0.0980241604279933, 0.12575203143030728, 0.1269801461676768, 0.25584005399361737, 0.28292372021078716]
Evaluation on validation dataset:
Step 25, mean loss 0.07047005291294392
Step 50, mean loss 0.03658199086318025
Step 75, mean loss 0.05555500432498036
Step 100, mean loss 0.05949436106194575
Step 125, mean loss 0.07344123696267194
Step 150, mean loss 0.06533641168442385
Step 175, mean loss 0.1114520126024603
Step 200, mean loss 0.34804450469729736
Step 225, mean loss 0.261524441935954
Unrolled forward losses 2.8217618070695076
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.09335e-01, 2.27362e-01, 4.11151e-01, 2.92052e-01, 8.68285e-02, 1.15300e-02
Node: 01 (pos: 0.010): 4.06662e-01, 2.18021e-01, 3.99619e-01, 2.79395e-01, 8.56165e-02, 1.08443e-02
Node: 02 (pos: 0.020): 4.04015e-01, 2.08987e-01, 3.88148e-01, 2.67131e-01, 8.44133e-02, 1.02019e-02
Node: 03 (pos: 0.030): 4.01396e-01, 2.00257e-01, 3.76752e-01, 2.55264e-01, 8.32190e-02, 9.59971e-03
Node: 04 (pos: 0.040): 3.98803e-01, 1.91829e-01, 3.65444e-01, 2.43797e-01, 8.20336e-02, 9.03528e-03
Node: 05 (pos: 0.051): 3.96236e-01, 1.83698e-01, 3.54235e-01, 2.32731e-01, 8.08571e-02, 8.50610e-03
-
Node: 07 (pos: 0.071): 3.91181e-01, 1.68310e-01, 3.32165e-01, 2.11796e-01, 7.85313e-02, 7.54443e-03
Node: 08 (pos: 0.081): 3.88692e-01, 1.61042e-01, 3.21328e-01, 2.01920e-01, 7.73820e-02, 7.10779e-03
Node: 09 (pos: 0.091): 3.86230e-01, 1.54051e-01, 3.10637e-01, 1.92432e-01, 7.62419e-02, 6.69810e-03
Node: 10 (pos: 0.101): 3.83793e-01, 1.47330e-01, 3.00103e-01, 1.83325e-01, 7.51109e-02, 6.31359e-03
Node: 11 (pos: 0.111): 3.81382e-01, 1.40873e-01, 2.89735e-01, 1.74594e-01, 7.39892e-02, 5.95265e-03
Node: 12 (pos: 0.121): 3.78996e-01, 1.34673e-01, 2.79544e-01, 1.66228e-01, 7.28768e-02, 5.61375e-03

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.07377e-01, 5.90665e-01, 2.59919e-01, 5.11426e-01, 9.71007e-01, 9.99530e-01
Node: 01 (pos: 0.010): 3.12133e-01, 6.16231e-01, 2.80029e-01, 5.41349e-01, 9.71799e-01, 9.99584e-01
Node: 02 (pos: 0.020): 3.16884e-01, 6.40925e-01, 3.00941e-01, 5.70640e-01, 9.72575e-01, 9.99632e-01
Node: 03 (pos: 0.030): 3.21626e-01, 6.64676e-01, 3.22593e-01, 5.99139e-01, 9.73335e-01, 9.99674e-01
Node: 04 (pos: 0.040): 3.26360e-01, 6.87429e-01, 3.44913e-01, 6.26709e-01, 9.74080e-01, 9.99711e-01
Node: 05 (pos: 0.051): 3.31083e-01, 7.09144e-01, 3.67818e-01, 6.53236e-01, 9.74809e-01, 9.99744e-01
-
Node: 07 (pos: 0.071): 3.40492e-01, 7.49367e-01, 4.15022e-01, 7.02821e-01, 9.76220e-01, 9.99799e-01
Node: 08 (pos: 0.081): 3.45176e-01, 7.67861e-01, 4.39124e-01, 7.25764e-01, 9.76903e-01, 9.99821e-01
Node: 09 (pos: 0.091): 3.49845e-01, 7.85283e-01, 4.63419e-01, 7.47429e-01, 9.77571e-01, 9.99841e-01
Node: 10 (pos: 0.101): 3.54498e-01, 8.01651e-01, 4.87803e-01, 7.67808e-01, 9.78224e-01, 9.99859e-01
Node: 11 (pos: 0.111): 3.59133e-01, 8.16991e-01, 5.12168e-01, 7.86906e-01, 9.78863e-01, 9.99875e-01
Node: 12 (pos: 0.121): 3.63749e-01, 8.31331e-01, 5.36407e-01, 8.04743e-01, 9.79487e-01, 9.99888e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.47428e-01, 6.31723e-06, 3.51840e-01, 1.01046e-03, 3.96885e-02, 1.08458e-05
Node: 01 (pos: 0.010): 3.45942e-01, 6.00329e-06, 3.39571e-01, 9.64868e-04, 3.87534e-02, 1.02983e-05
Node: 02 (pos: 0.020): 3.44466e-01, 5.70550e-06, 3.27499e-01, 9.21396e-04, 3.78374e-02, 9.78029e-06
Node: 03 (pos: 0.030): 3.42998e-01, 5.42300e-06, 3.15636e-01, 8.79941e-04, 3.69400e-02, 9.29021e-06
Node: 04 (pos: 0.040): 3.41539e-01, 5.15496e-06, 3.03994e-01, 8.40405e-04, 3.60611e-02, 8.82643e-06
Node: 05 (pos: 0.051): 3.58088e-01, 9.05110e-06, 4.41888e-01, 1.39890e-03, 4.67855e-02, 1.56759e-05
-
Node: 07 (pos: 0.071): 3.56537e-01, 8.59522e-06, 4.28698e-01, 1.33506e-03, 4.57108e-02, 1.48632e-05
Node: 08 (pos: 0.081): 3.54995e-01, 8.16316e-06, 4.15586e-01, 1.27424e-03, 4.46568e-02, 1.40955e-05
Node: 09 (pos: 0.091): 3.53463e-01, 7.75362e-06, 4.02571e-01, 1.21630e-03, 4.36233e-02, 1.33702e-05
Node: 10 (pos: 0.101): 3.51941e-01, 7.36538e-06, 3.89672e-01, 1.16108e-03, 4.26099e-02, 1.26849e-05
Node: 11 (pos: 0.111): 3.50427e-01, 6.99728e-06, 3.76906e-01, 1.10845e-03, 4.16165e-02, 1.20372e-05
Node: 12 (pos: 0.121): 3.40088e-01, 4.90063e-06, 2.92584e-01, 8.02696e-04, 3.52003e-02, 8.38745e-06

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.27483e-01, 1.00000e+00, 3.72809e-01, 9.99992e-01, 9.93872e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.30595e-01, 1.00000e+00, 3.98886e-01, 9.99993e-01, 9.94156e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.33697e-01, 1.00000e+00, 4.25330e-01, 9.99993e-01, 9.94428e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.36789e-01, 1.00000e+00, 4.51999e-01, 9.99994e-01, 9.94689e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.39872e-01, 1.00000e+00, 4.78749e-01, 9.99994e-01, 9.94938e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.05437e-01, 1.00000e+00, 2.10899e-01, 9.99985e-01, 9.91494e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.08613e-01, 1.00000e+00, 2.31113e-01, 9.99986e-01, 9.91879e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.11781e-01, 1.00000e+00, 2.52432e-01, 9.99987e-01, 9.92248e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.14939e-01, 1.00000e+00, 2.74792e-01, 9.99988e-01, 9.92601e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.18089e-01, 1.00000e+00, 2.98112e-01, 9.99989e-01, 9.92940e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.21230e-01, 1.00000e+00, 3.22296e-01, 9.99990e-01, 9.93264e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.42944e-01, 1.00000e+00, 5.05440e-01, 9.99995e-01, 9.95176e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.13346105721539286), ('0.bias', 0.19885200066530462), ('2.weight', 1.068747404029308), ('2.bias', 0.29354714875110255)] 

GNN_Layer 1 gradients:
[('0.weight', 0.0011310495522277046), ('0.bias', 0.008156002774521866), ('2.weight', 0.005571049637200498), ('2.bias', 0.005979528374398014)] 

GNN_Layer 2 gradients:
[('0.weight', 0.1888658260895942), ('0.bias', 0.0777296703297253), ('2.weight', 1.0370002863407413), ('2.bias', 0.031796305074528776)] 

GNN_Layer 3 gradients:
[('0.weight', 0.0032528241292008125), ('0.bias', 0.05091013286998756), ('2.weight', 0.030531998416945434), ('2.bias', 0.053507160306193485)] 

GNN_Layer 4 gradients:
[('0.weight', 0.004188414520172045), ('0.bias', 0.006051952670854537), ('2.weight', 0.02485534500491611), ('2.bias', 0.0056191319014758725)] 

GNN_Layer 5 gradients:
[('0.weight', 2.4926596854060448e-06), ('0.bias', 1.2954267022823545e-05), ('2.weight', 1.0534946968622164e-05), ('2.bias', 8.194956804710006e-06)] 

Evaluation on test dataset:
Step 25, mean loss 0.054001379800330415
Step 50, mean loss 0.02594722415086674
Step 75, mean loss 0.03378827460244854
Step 100, mean loss 0.04385457049814661
Step 125, mean loss 0.057044337306498105
Step 150, mean loss 0.07916092071998923
Step 175, mean loss 0.13650440442417378
Step 200, mean loss 0.13654112023602533
Step 225, mean loss 0.13377508274056613
Unrolled forward losses 2.3101407763969415
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.19999232657695945, [0.14190402773505456, 0.09816618310244597, 0.12535393813892817, 0.12687083926574166, 0.25621805306228873, 0.28301592209026305]
Training Loss (progress: 0.08), 0.2001826873010281, [0.14230415687078538, 0.09843691875880078, 0.12481319613124987, 0.1266447803751558, 0.2575528653229057, 0.2840730404360078]
Training Loss (progress: 0.16), 0.18250350665499776, [0.14210248051451774, 0.0978931141145805, 0.12453555171514552, 0.12658947202150103, 0.2594129318085587, 0.28390717302566776]
Training Loss (progress: 0.24), 0.18444254337610652, [0.14236654144002153, 0.09825308316406674, 0.12433858798908619, 0.12659128519282065, 0.25976486022999684, 0.28461440009339894]
Training Loss (progress: 0.32), 0.1784871725408005, [0.14225124157929092, 0.0979215518429141, 0.1237019207396116, 0.1268753474610245, 0.2605490694801886, 0.2857969683961822]
Training Loss (progress: 0.40), 0.18544559060575314, [0.1423261158313094, 0.0978843272035119, 0.12318124647174818, 0.126096303056062, 0.262339956991251, 0.2873729826123556]
Training Loss (progress: 0.48), 0.18858517379420767, [0.14261884953589365, 0.09812151249866878, 0.12295518110853557, 0.12580814959286327, 0.2635972914692696, 0.2890040462397778]
Training Loss (progress: 0.56), 0.16557370192009563, [0.14249083781662147, 0.09788095544787147, 0.12230623378369411, 0.12638596876696673, 0.26448469766513005, 0.28947751437493635]
Training Loss (progress: 0.64), 0.18682736046147608, [0.1426319674909552, 0.09760131665244832, 0.12205872162674687, 0.12619446551421812, 0.26551510895997993, 0.28955750449428536]
Training Loss (progress: 0.72), 0.18219329588764346, [0.1428337500298749, 0.09789991317354571, 0.12176328938874034, 0.12583375127464247, 0.26633305491994963, 0.29002934506319317]
Training Loss (progress: 0.80), 0.17899631077240175, [0.14221866748101786, 0.0977694375245909, 0.12131612415885121, 0.12600504392026168, 0.26800741575718656, 0.2909377402673771]
Training Loss (progress: 0.88), 0.17147548665464737, [0.14274026328992045, 0.09784995607476793, 0.12100080414817685, 0.12606200914120472, 0.26921194294094014, 0.2917297262822491]
Training Loss (progress: 0.96), 0.1888999095232119, [0.1427112621993698, 0.09756494752865452, 0.12017229790331639, 0.12631708956108872, 0.2703207800872113, 0.2918900450236255]
Evaluation on validation dataset:
Step 25, mean loss 0.06826539754614945
Step 50, mean loss 0.03512124480050095
Step 75, mean loss 0.05059195457171767
Step 100, mean loss 0.053294538943780036
Step 125, mean loss 0.06147593682758965
Step 150, mean loss 0.06383633043673129
Step 175, mean loss 0.11825834278260869
Step 200, mean loss 0.3741774467776793
Step 225, mean loss 0.2671391455071037
Unrolled forward losses 2.7528716718525326
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.07941e-01, 2.24086e-01, 4.11090e-01, 2.92637e-01, 6.02798e-02, 7.72523e-03
Node: 01 (pos: 0.010): 4.05136e-01, 2.15038e-01, 3.98687e-01, 2.79709e-01, 5.92485e-02, 7.25106e-03
Node: 02 (pos: 0.020): 4.02357e-01, 2.06283e-01, 3.86364e-01, 2.67188e-01, 5.82288e-02, 6.80766e-03
Node: 03 (pos: 0.030): 3.99605e-01, 1.97819e-01, 3.74137e-01, 2.55078e-01, 5.72206e-02, 6.39294e-03
Node: 04 (pos: 0.040): 3.96880e-01, 1.89642e-01, 3.62023e-01, 2.43382e-01, 5.62239e-02, 6.00497e-03
Node: 05 (pos: 0.051): 3.94182e-01, 1.81750e-01, 3.50036e-01, 2.32102e-01, 5.52387e-02, 5.64195e-03
-
Node: 07 (pos: 0.071): 3.88866e-01, 1.66799e-01, 3.26500e-01, 2.10783e-01, 5.33026e-02, 4.98416e-03
Node: 08 (pos: 0.081): 3.86247e-01, 1.59731e-01, 3.14979e-01, 2.00737e-01, 5.23517e-02, 4.68638e-03
Node: 09 (pos: 0.091): 3.83655e-01, 1.52928e-01, 3.03640e-01, 1.91094e-01, 5.14121e-02, 4.40750e-03
Node: 10 (pos: 0.101): 3.81089e-01, 1.46382e-01, 2.92494e-01, 1.81847e-01, 5.04839e-02, 4.14627e-03
Node: 11 (pos: 0.111): 3.78549e-01, 1.40089e-01, 2.81553e-01, 1.72987e-01, 4.95669e-02, 3.90151e-03
Node: 12 (pos: 0.121): 3.76035e-01, 1.34041e-01, 2.70827e-01, 1.64507e-01, 4.86613e-02, 3.67214e-03

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.12040e-01, 5.98100e-01, 2.44808e-01, 5.06147e-01, 9.86642e-01, 9.99796e-01
Node: 01 (pos: 0.010): 3.17062e-01, 6.22926e-01, 2.66165e-01, 5.36819e-01, 9.87093e-01, 9.99820e-01
Node: 02 (pos: 0.020): 3.22080e-01, 6.46896e-01, 2.88494e-01, 5.66859e-01, 9.87530e-01, 9.99842e-01
Node: 03 (pos: 0.030): 3.27093e-01, 6.69946e-01, 3.11719e-01, 5.96095e-01, 9.87956e-01, 9.99860e-01
Node: 04 (pos: 0.040): 3.32099e-01, 6.92027e-01, 3.35750e-01, 6.24377e-01, 9.88369e-01, 9.99877e-01
Node: 05 (pos: 0.051): 3.37097e-01, 7.13105e-01, 3.60484e-01, 6.51581e-01, 9.88771e-01, 9.99891e-01
-
Node: 07 (pos: 0.071): 3.47062e-01, 7.52174e-01, 4.11595e-01, 7.02382e-01, 9.89540e-01, 9.99915e-01
Node: 08 (pos: 0.081): 3.52027e-01, 7.70155e-01, 4.37722e-01, 7.25854e-01, 9.89908e-01, 9.99925e-01
Node: 09 (pos: 0.091): 3.56978e-01, 7.87108e-01, 4.64052e-01, 7.47994e-01, 9.90266e-01, 9.99934e-01
Node: 10 (pos: 0.101): 3.61914e-01, 8.03052e-01, 4.90451e-01, 7.68789e-01, 9.90612e-01, 9.99941e-01
Node: 11 (pos: 0.111): 3.66834e-01, 8.18009e-01, 5.16785e-01, 7.88248e-01, 9.90949e-01, 9.99948e-01
Node: 12 (pos: 0.121): 3.71737e-01, 8.32010e-01, 5.42922e-01, 8.06392e-01, 9.91275e-01, 9.99954e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.45875e-01, 6.20767e-06, 3.45707e-01, 9.14349e-04, 2.21719e-02, 5.88953e-06
Node: 01 (pos: 0.010): 3.44251e-01, 5.90416e-06, 3.32736e-01, 8.71897e-04, 2.15666e-02, 5.57996e-06
Node: 02 (pos: 0.020): 3.42637e-01, 5.61600e-06, 3.19998e-01, 8.31474e-04, 2.09760e-02, 5.28773e-06
Node: 03 (pos: 0.030): 3.41033e-01, 5.34240e-06, 3.07508e-01, 7.92981e-04, 2.03999e-02, 5.01180e-06
Node: 04 (pos: 0.040): 3.39439e-01, 5.08258e-06, 2.95280e-01, 7.56320e-04, 1.98380e-02, 4.75121e-06
Node: 05 (pos: 0.051): 3.57528e-01, 8.84074e-06, 4.41509e-01, 1.27816e-03, 2.68447e-02, 8.64368e-06
-
Node: 07 (pos: 0.071): 3.55833e-01, 8.40279e-06, 4.27430e-01, 1.21814e-03, 2.61285e-02, 8.17763e-06
Node: 08 (pos: 0.081): 3.54147e-01, 7.98733e-06, 4.13446e-01, 1.16103e-03, 2.54290e-02, 7.73832e-06
Node: 09 (pos: 0.091): 3.52472e-01, 7.59317e-06, 3.99579e-01, 1.10670e-03, 2.47459e-02, 7.32412e-06
Node: 10 (pos: 0.101): 3.50807e-01, 7.21916e-06, 3.85851e-01, 1.05499e-03, 2.40789e-02, 6.93352e-06
Node: 11 (pos: 0.111): 3.49153e-01, 6.86424e-06, 3.72283e-01, 1.00578e-03, 2.34277e-02, 6.56510e-06
Node: 12 (pos: 0.121): 3.37854e-01, 4.83584e-06, 2.83327e-01, 7.21402e-04, 1.92899e-02, 4.50506e-06

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.32923e-01, 1.00000e+00, 3.69638e-01, 9.99993e-01, 9.98182e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.36332e-01, 1.00000e+00, 3.97744e-01, 9.99994e-01, 9.98280e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.39730e-01, 1.00000e+00, 4.26259e-01, 9.99995e-01, 9.98373e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.43118e-01, 1.00000e+00, 4.55006e-01, 9.99995e-01, 9.98461e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.46494e-01, 1.00000e+00, 4.83808e-01, 9.99995e-01, 9.98545e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.08786e-01, 1.00000e+00, 1.97256e-01, 9.99987e-01, 9.97337e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.12262e-01, 1.00000e+00, 2.18411e-01, 9.99988e-01, 9.97477e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.15729e-01, 1.00000e+00, 2.40880e-01, 9.99989e-01, 9.97610e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.19187e-01, 1.00000e+00, 2.64591e-01, 9.99990e-01, 9.97736e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.22635e-01, 1.00000e+00, 2.89447e-01, 9.99991e-01, 9.97857e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.26075e-01, 1.00000e+00, 3.15332e-01, 9.99992e-01, 9.97971e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.49860e-01, 1.00000e+00, 5.12490e-01, 9.99996e-01, 9.98624e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.13531407744818721), ('0.bias', 0.44711463095425724), ('2.weight', 1.1526606813309748), ('2.bias', 0.6978104676905308)] 

GNN_Layer 1 gradients:
[('0.weight', 0.0064847255342459385), ('0.bias', 0.09494904383147305), ('2.weight', 0.03327950804308637), ('2.bias', 0.06928308201820893)] 

GNN_Layer 2 gradients:
[('0.weight', 0.4556199262073498), ('0.bias', 0.16423671159085942), ('2.weight', 2.398207365540064), ('2.bias', 0.06718875733588636)] 

GNN_Layer 3 gradients:
[('0.weight', 0.014897820598207821), ('0.bias', 0.20771830628510837), ('2.weight', 0.1282136130971536), ('2.bias', 0.21703377951586578)] 

GNN_Layer 4 gradients:
[('0.weight', 0.0012169250089521736), ('0.bias', 0.006449945867657444), ('2.weight', 0.009156856312684444), ('2.bias', 0.005528704776339556)] 

GNN_Layer 5 gradients:
[('0.weight', 5.098794334898307e-06), ('0.bias', 0.00042483219529196884), ('2.weight', 0.00035602216001099946), ('2.bias', 0.0002600343967124767)] 

Evaluation on test dataset:
Step 25, mean loss 0.057136235942390506
Step 50, mean loss 0.02414759345341727
Step 75, mean loss 0.03575411203273268
Step 100, mean loss 0.04470383419340121
Step 125, mean loss 0.05624315011147923
Step 150, mean loss 0.08510114122833537
Step 175, mean loss 0.11273271718027478
Step 200, mean loss 0.14371221983055388
Step 225, mean loss 0.11918327775453585
Unrolled forward losses 2.1922177108023773
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.1742261621807209, [0.14257498612060154, 0.09747680626761468, 0.11994292302264499, 0.12595825836427518, 0.2702796434904063, 0.2933054649939402]
Training Loss (progress: 0.08), 0.19361928753984858, [0.14251280823679777, 0.09786860151390192, 0.11982280167411265, 0.12631580660553618, 0.27157960045338175, 0.29443479315294674]
Training Loss (progress: 0.16), 0.180279749536491, [0.14298581924871429, 0.0975221515207014, 0.11953010835938194, 0.12563780338824343, 0.27314282958165353, 0.29437109703201353]
Training Loss (progress: 0.24), 0.17390950206266678, [0.14274672107758418, 0.0971997679755572, 0.11898405898091917, 0.12573273767175938, 0.2740654488922548, 0.2954550509288505]
Training Loss (progress: 0.32), 0.1666612661986822, [0.14258901888135977, 0.09730884509491239, 0.11839542707105821, 0.12586487944670507, 0.2751727305524189, 0.29588274982859414]
Training Loss (progress: 0.40), 0.15293437351279393, [0.14266005560775627, 0.09744428717489145, 0.11813003372454815, 0.12563944722703024, 0.2762953913598513, 0.2958054577602379]
Training Loss (progress: 0.48), 0.18766237834234376, [0.14243289811444612, 0.09708795104552048, 0.11793151447646365, 0.12550312984633138, 0.2781018210651466, 0.2972155490704403]
Training Loss (progress: 0.56), 0.17496359086729493, [0.1424762926201215, 0.096922911167604, 0.11734934289132778, 0.12539621783352856, 0.2788726508415268, 0.29922724609926654]
Training Loss (progress: 0.64), 0.1613920043060345, [0.1425780052054355, 0.09739440982038912, 0.11705675181408878, 0.12572742373371343, 0.27983203131388146, 0.2989096289304648]
Training Loss (progress: 0.72), 0.15080263129271737, [0.14275583620110446, 0.09758368389215033, 0.11681110493930484, 0.12570296340817672, 0.2808066140668766, 0.2996167145573431]
Training Loss (progress: 0.80), 0.16686541734871582, [0.1429075391781049, 0.09721596968617861, 0.11631860672901449, 0.12555581677257216, 0.28242670606459236, 0.3010896611263102]
Training Loss (progress: 0.88), 0.1450696350887656, [0.14304144895525644, 0.09706429143069233, 0.11598819787096042, 0.12563468067858766, 0.28269987045943396, 0.3011842923745274]
Training Loss (progress: 0.96), 0.15841865572245342, [0.14270613869402918, 0.09708450666874108, 0.11570060846368445, 0.12561200655426713, 0.28471868793229216, 0.3029411738844079]
Evaluation on validation dataset:
Step 25, mean loss 0.0568382404862728
Step 50, mean loss 0.025692484961538226
Step 75, mean loss 0.041222699238126784
Step 100, mean loss 0.044348354597415554
Step 125, mean loss 0.05844720710848893
Step 150, mean loss 0.055164786035936685
Step 175, mean loss 0.09540541154482748
Step 200, mean loss 0.29262266178957547
Step 225, mean loss 0.22407108142937834
Unrolled forward losses 2.296436773551066
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.08934e-01, 2.21987e-01, 4.09494e-01, 2.88608e-01, 3.87112e-02, 4.99844e-03
Node: 01 (pos: 0.010): 4.05996e-01, 2.13213e-01, 3.96452e-01, 2.75714e-01, 3.79370e-02, 4.68413e-03
Node: 02 (pos: 0.020): 4.03085e-01, 2.04717e-01, 3.83509e-01, 2.63232e-01, 3.71743e-02, 4.39069e-03
Node: 03 (pos: 0.030): 4.00200e-01, 1.96498e-01, 3.70682e-01, 2.51168e-01, 3.64230e-02, 4.11667e-03
Node: 04 (pos: 0.040): 3.97342e-01, 1.88553e-01, 3.57990e-01, 2.39524e-01, 3.56830e-02, 3.86072e-03
Node: 05 (pos: 0.051): 3.94511e-01, 1.80879e-01, 3.45448e-01, 2.28300e-01, 3.49542e-02, 3.62161e-03
-
Node: 07 (pos: 0.071): 3.88928e-01, 1.66324e-01, 3.20882e-01, 2.07105e-01, 3.35299e-02, 3.18934e-03
Node: 08 (pos: 0.081): 3.86177e-01, 1.59434e-01, 3.08889e-01, 1.97128e-01, 3.28341e-02, 2.99411e-03
Node: 09 (pos: 0.091): 3.83452e-01, 1.52796e-01, 2.97107e-01, 1.87555e-01, 3.21492e-02, 2.81154e-03
Node: 10 (pos: 0.101): 3.80753e-01, 1.46404e-01, 2.85549e-01, 1.78382e-01, 3.14750e-02, 2.64079e-03
Node: 11 (pos: 0.111): 3.78081e-01, 1.40253e-01, 2.74228e-01, 1.69598e-01, 3.08115e-02, 2.48104e-03
Node: 12 (pos: 0.121): 3.75434e-01, 1.34335e-01, 2.63153e-01, 1.61196e-01, 3.01586e-02, 2.33155e-03

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.09320e-01, 6.02653e-01, 2.34309e-01, 5.15800e-01, 9.94758e-01, 9.99918e-01
Node: 01 (pos: 0.010): 3.14561e-01, 6.26773e-01, 2.56620e-01, 5.46512e-01, 9.94966e-01, 9.99928e-01
Node: 02 (pos: 0.020): 3.19803e-01, 6.50064e-01, 2.80048e-01, 5.76527e-01, 9.95165e-01, 9.99936e-01
Node: 03 (pos: 0.030): 3.25045e-01, 6.72471e-01, 3.04501e-01, 6.05677e-01, 9.95358e-01, 9.99944e-01
Node: 04 (pos: 0.040): 3.30285e-01, 6.93948e-01, 3.29875e-01, 6.33817e-01, 9.95545e-01, 9.99951e-01
Node: 05 (pos: 0.051): 3.35522e-01, 7.14465e-01, 3.56046e-01, 6.60829e-01, 9.95724e-01, 9.99957e-01
-
Node: 07 (pos: 0.071): 3.45978e-01, 7.52550e-01, 4.10228e-01, 7.11119e-01, 9.96065e-01, 9.99966e-01
Node: 08 (pos: 0.081): 3.51194e-01, 7.70109e-01, 4.37938e-01, 7.34285e-01, 9.96226e-01, 9.99970e-01
Node: 09 (pos: 0.091): 3.56401e-01, 7.86688e-01, 4.65850e-01, 7.56093e-01, 9.96382e-01, 9.99974e-01
Node: 10 (pos: 0.101): 3.61596e-01, 8.02302e-01, 4.93804e-01, 7.76539e-01, 9.96532e-01, 9.99977e-01
Node: 11 (pos: 0.111): 3.66778e-01, 8.16973e-01, 5.21642e-01, 7.95634e-01, 9.96676e-01, 9.99980e-01
Node: 12 (pos: 0.121): 3.71946e-01, 8.30729e-01, 5.49211e-01, 8.13406e-01, 9.96815e-01, 9.99982e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.43986e-01, 6.40858e-06, 3.39767e-01, 8.23550e-04, 1.20293e-02, 2.99430e-06
Node: 01 (pos: 0.010): 3.42209e-01, 6.10125e-06, 3.26300e-01, 7.84892e-04, 1.16618e-02, 2.83193e-06
Node: 02 (pos: 0.020): 3.40443e-01, 5.80919e-06, 3.13100e-01, 7.48104e-04, 1.13044e-02, 2.67890e-06
Node: 03 (pos: 0.030): 3.38689e-01, 5.53159e-06, 3.00181e-01, 7.13090e-04, 1.09571e-02, 2.53464e-06
Node: 04 (pos: 0.040): 3.36946e-01, 5.26771e-06, 2.87559e-01, 6.79761e-04, 1.06194e-02, 2.39863e-06
Node: 05 (pos: 0.051): 3.56737e-01, 9.06317e-06, 4.39765e-01, 1.15559e-03, 1.49088e-02, 4.44905e-06
-
Node: 07 (pos: 0.071): 3.54881e-01, 8.62292e-06, 4.25025e-01, 1.10073e-03, 1.44628e-02, 4.20176e-06
Node: 08 (pos: 0.081): 3.53037e-01, 8.20484e-06, 4.10395e-01, 1.04856e-03, 1.40289e-02, 3.96904e-06
Node: 09 (pos: 0.091): 3.51204e-01, 7.80778e-06, 3.95902e-01, 9.98946e-04, 1.36066e-02, 3.74999e-06
Node: 10 (pos: 0.101): 3.49382e-01, 7.43063e-06, 3.81570e-01, 9.51760e-04, 1.31957e-02, 3.54375e-06
Node: 11 (pos: 0.111): 3.47572e-01, 7.07236e-06, 3.67421e-01, 9.06875e-04, 1.27961e-02, 3.34954e-06
Node: 12 (pos: 0.121): 3.35213e-01, 5.01686e-06, 2.75246e-01, 6.48033e-04, 1.02913e-02, 2.27037e-06

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.35938e-01, 1.00000e+00, 3.68245e-01, 9.99995e-01, 9.99493e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.39683e-01, 1.00000e+00, 3.97966e-01, 9.99995e-01, 9.99523e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.43417e-01, 1.00000e+00, 4.28123e-01, 9.99996e-01, 9.99552e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.47139e-01, 1.00000e+00, 4.58506e-01, 9.99996e-01, 9.99579e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.50850e-01, 1.00000e+00, 4.88907e-01, 9.99996e-01, 9.99605e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.09445e-01, 1.00000e+00, 1.87573e-01, 9.99989e-01, 9.99221e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.13257e-01, 1.00000e+00, 2.09449e-01, 9.99990e-01, 9.99267e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.17060e-01, 1.00000e+00, 2.32815e-01, 9.99991e-01, 9.99310e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.20855e-01, 1.00000e+00, 2.57590e-01, 9.99992e-01, 9.99351e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.24640e-01, 1.00000e+00, 2.83667e-01, 9.99993e-01, 9.99390e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.28416e-01, 1.00000e+00, 3.10910e-01, 9.99993e-01, 9.99426e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.54550e-01, 1.00000e+00, 5.19123e-01, 9.99997e-01, 9.99629e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.38655485741915746), ('0.bias', 0.3104060720603243), ('2.weight', 3.154175820361464), ('2.bias', 0.3581065054094117)] 

GNN_Layer 1 gradients:
[('0.weight', 0.006955016751031014), ('0.bias', 0.08246644150880636), ('2.weight', 0.03044467339366362), ('2.bias', 0.058216888085449836)] 

GNN_Layer 2 gradients:
[('0.weight', 0.5579229001537831), ('0.bias', 0.20666301804566095), ('2.weight', 2.788894853403878), ('2.bias', 0.0089665417878253)] 

GNN_Layer 3 gradients:
[('0.weight', 0.008768905472902757), ('0.bias', 0.0806459440067606), ('2.weight', 0.06351538748627947), ('2.bias', 0.08351500771627238)] 

GNN_Layer 4 gradients:
[('0.weight', 0.0007234388949115477), ('0.bias', 0.005714473954163067), ('2.weight', 0.006782989734282873), ('2.bias', 0.004553594009817438)] 

GNN_Layer 5 gradients:
[('0.weight', 1.7485714887099303e-06), ('0.bias', 2.4675293861355913e-05), ('2.weight', 2.3677035365748613e-05), ('2.bias', 1.445535480844583e-05)] 

Evaluation on test dataset:
Step 25, mean loss 0.04601418546131361
Step 50, mean loss 0.017980438237396378
Step 75, mean loss 0.02850184381795145
Step 100, mean loss 0.0349160469023851
Step 125, mean loss 0.04148850961418481
Step 150, mean loss 0.06512190841826342
Step 175, mean loss 0.11635863926510004
Step 200, mean loss 0.12058173264780386
Step 225, mean loss 0.10183425712299561
Unrolled forward losses 1.749082539367694
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.14764558025585436, [0.1426810476337496, 0.09698392196611845, 0.11547613302243938, 0.12581186595629643, 0.2851753622945313, 0.3032419923838333]
Training Loss (progress: 0.08), 0.1530697745447076, [0.1428733368122355, 0.09704749243213698, 0.1154434507760744, 0.12583990913437243, 0.2856043451199186, 0.30314849906840635]
Training Loss (progress: 0.16), 0.14586688881766896, [0.14297877661230035, 0.09701881901754014, 0.11533755739161625, 0.12572888362556264, 0.28624609927878975, 0.3037068910428052]
Training Loss (progress: 0.24), 0.13676781145326838, [0.14313141427830856, 0.09711926852410212, 0.11540074192905993, 0.1256762497554301, 0.2870257154563753, 0.3039670114783135]
Training Loss (progress: 0.32), 0.13868227087487503, [0.14319168869575705, 0.09703660550022289, 0.11529619850979177, 0.12600388984837094, 0.2870950647605618, 0.30474258678231975]
Training Loss (progress: 0.40), 0.1297683718598591, [0.14308722722332773, 0.09744928231473485, 0.11511472962539306, 0.1260576101285131, 0.2876895216863026, 0.3052299934001913]
Training Loss (progress: 0.48), 0.1516591081980165, [0.14322752211536263, 0.0971530932555229, 0.11507706765993116, 0.1260809804469435, 0.28816657238808213, 0.3056177350714372]
Training Loss (progress: 0.56), 0.13181121792792846, [0.1431270561096535, 0.0972104662695762, 0.11483565142591222, 0.12611491774617622, 0.2887360981199069, 0.30546185385344227]
Training Loss (progress: 0.64), 0.14233897547612714, [0.14335244493899876, 0.097209808943309, 0.11476174508677164, 0.12603876885346285, 0.28907299694915706, 0.30556447233996675]
Training Loss (progress: 0.72), 0.14766108467806302, [0.14338978276802797, 0.09710209399239557, 0.11464206417108783, 0.12603659518553115, 0.28970982546903945, 0.30600219926438443]
Training Loss (progress: 0.80), 0.1319932589573539, [0.14334453836199995, 0.09734539591380548, 0.11465957732358042, 0.12602919363046908, 0.28989267540798297, 0.3063538119209871]
Training Loss (progress: 0.88), 0.13931451430959724, [0.14348855898160368, 0.09714197751630675, 0.11433419976830093, 0.12617681309203438, 0.2905789530569981, 0.3065851896847615]
Training Loss (progress: 0.96), 0.131764890133946, [0.14347698047721158, 0.09718294658842973, 0.11430028205104747, 0.12604640038532916, 0.29107679470458536, 0.30659802097316374]
Evaluation on validation dataset:
Step 25, mean loss 0.05093415591197843
Step 50, mean loss 0.025782967005842466
Step 75, mean loss 0.04365725695038912
Step 100, mean loss 0.05371802501648922
Step 125, mean loss 0.05900101081079377
Step 150, mean loss 0.06401595658013577
Step 175, mean loss 0.09644898036912294
Step 200, mean loss 0.3144829488488277
Step 225, mean loss 0.24489305446863657
Unrolled forward losses 2.9067996158944354
Unrolled forward base losses 2.565701273852575
Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.1472379487494394, [0.14346059455248958, 0.0974037287550912, 0.11423116496119122, 0.12621341229167146, 0.29167983185811447, 0.3067233788829089]
Training Loss (progress: 0.08), 0.1381950454718587, [0.14339459027756093, 0.0971847232031037, 0.11416087720499188, 0.1260336988842967, 0.2921463347865708, 0.307335345969698]
Training Loss (progress: 0.16), 0.14216858298627416, [0.14356099031199582, 0.09718807812616012, 0.11401713263340699, 0.12625744901004113, 0.2925578741722678, 0.3073187019695759]
Training Loss (progress: 0.24), 0.14032354231256147, [0.143603015002402, 0.09742250329171542, 0.11387701621304973, 0.12645394267098156, 0.29313373556272954, 0.3078777550801295]
Training Loss (progress: 0.32), 0.14256374924023738, [0.14370336412387652, 0.09723912274776148, 0.11387645626863092, 0.12630416305856396, 0.29289920573654243, 0.3081397288440711]
Training Loss (progress: 0.40), 0.12903526070118146, [0.14393525755693498, 0.09734681254074092, 0.1137172475146606, 0.12616078998633223, 0.29324626770423645, 0.3083104780821272]
Training Loss (progress: 0.48), 0.1305248438785332, [0.14394752060529467, 0.09739492870310648, 0.11370825477959019, 0.12654729119018648, 0.2937213027941053, 0.30824635760587377]
Training Loss (progress: 0.56), 0.14150327764514053, [0.14382267974582744, 0.09729061516690272, 0.11349209054871529, 0.1263985589276478, 0.294297348089408, 0.30891915748058785]
Training Loss (progress: 0.64), 0.13714803206276485, [0.1439091406985094, 0.09745750677479267, 0.11343651368997605, 0.12634107635055333, 0.2949881159861597, 0.30922449835961474]
Training Loss (progress: 0.72), 0.11930670297945105, [0.1439498149973742, 0.09716892088141851, 0.11331584871177539, 0.12608809659154496, 0.2952418783452037, 0.3094931100508687]
Training Loss (progress: 0.80), 0.1399470147694215, [0.14389135170729714, 0.09712460410804676, 0.11329056806967949, 0.12601599636357325, 0.29591143750762217, 0.31008151797096184]
Training Loss (progress: 0.88), 0.12133477731684494, [0.14399802316906754, 0.09734154029068684, 0.11331292653926113, 0.12637964987541056, 0.2967596407623312, 0.31070508458917684]
Training Loss (progress: 0.96), 0.12344360242719259, [0.1440150807688499, 0.09723156650126473, 0.11315307078865487, 0.1265280960108213, 0.2969904856114674, 0.3110722597037798]
Evaluation on validation dataset:
Step 25, mean loss 0.04371535079261196
Step 50, mean loss 0.02359706178666495
Step 75, mean loss 0.034164921746945874
Step 100, mean loss 0.0416744047147231
Step 125, mean loss 0.049608675954677245
Step 150, mean loss 0.04908830164707344
Step 175, mean loss 0.08793787766085669
Step 200, mean loss 0.29593413675428404
Step 225, mean loss 0.23611190005451083
Unrolled forward losses 2.469302720189806
Unrolled forward base losses 2.565701273852575
Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.1370323944421907, [0.1440624032498683, 0.09717065166739886, 0.11306713661991229, 0.12641833405500436, 0.2972772395566662, 0.3112429419930144]
Training Loss (progress: 0.08), 0.14653095551375162, [0.14390614856208217, 0.09721937418871383, 0.1129909442837455, 0.1265514211665458, 0.2979565731339483, 0.3114377810276307]
Training Loss (progress: 0.16), 0.13011777130163107, [0.14393539076535256, 0.0972537899098595, 0.11287558828248152, 0.12654394750794945, 0.2986632262017009, 0.31188004068932274]
Training Loss (progress: 0.24), 0.13732585060224956, [0.14385264342855597, 0.09726092257053064, 0.11279180387019974, 0.12656781380630178, 0.29918025602715365, 0.31219560278191183]
Training Loss (progress: 0.32), 0.1182981849072815, [0.14410682145482856, 0.09725662794392026, 0.11268375809717855, 0.12648596425287098, 0.2993966226565257, 0.3125247833710694]
Training Loss (progress: 0.40), 0.13538392212053418, [0.14420255971943263, 0.09717392287026895, 0.112610493646097, 0.1265568343425096, 0.3000736496183257, 0.3123881755653874]
Training Loss (progress: 0.48), 0.1364232966962471, [0.14414112784642455, 0.09707682849379268, 0.11248162048649046, 0.12656760687018298, 0.30046372811328015, 0.312625653760732]
Training Loss (progress: 0.56), 0.14868585906077605, [0.14419104203111308, 0.0970398127356094, 0.11229666271538476, 0.12643245622471017, 0.3009871280435498, 0.31332171503019796]
Training Loss (progress: 0.64), 0.1340951005265744, [0.14420775329928434, 0.09728439628638301, 0.11238518523206355, 0.1265166474123164, 0.3014326103809457, 0.31388568567525527]
Training Loss (progress: 0.72), 0.12890688776784578, [0.14420286283691847, 0.09723022189987567, 0.11221638588579916, 0.12647540416398675, 0.3017478900347966, 0.3137084471716693]
Training Loss (progress: 0.80), 0.13055887886319587, [0.14429847072555782, 0.09732522965202658, 0.1121078816839137, 0.12640722372890553, 0.3025987411248648, 0.31435064621746667]
Training Loss (progress: 0.88), 0.14765611850684984, [0.14429339217712747, 0.09710049030801908, 0.11208132146429814, 0.1265066905036435, 0.30298979217654587, 0.3149955994065009]
Training Loss (progress: 0.96), 0.14162708161831708, [0.14431687372192753, 0.09708966170370703, 0.11205124906804251, 0.12658111599008878, 0.3036509956906133, 0.3154666397347844]
Evaluation on validation dataset:
Step 25, mean loss 0.03997935109946513
Step 50, mean loss 0.022331594025257098
Step 75, mean loss 0.029287872003717602
Step 100, mean loss 0.038029228825103574
Step 125, mean loss 0.041594986513542584
Step 150, mean loss 0.04466077652942067
Step 175, mean loss 0.07139819414157018
Step 200, mean loss 0.2683562546125214
Step 225, mean loss 0.23346036693400346
Unrolled forward losses 1.7470837352351265
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.05536e-01, 2.20995e-01, 4.01136e-01, 2.82632e-01, 2.06480e-02, 2.95315e-03
Node: 01 (pos: 0.010): 4.02487e-01, 2.12684e-01, 3.87575e-01, 2.69820e-01, 2.00802e-02, 2.75904e-03
Node: 02 (pos: 0.020): 3.99467e-01, 2.04625e-01, 3.74140e-01, 2.57429e-01, 1.95256e-02, 2.57834e-03
Node: 03 (pos: 0.030): 3.96473e-01, 1.96816e-01, 3.60852e-01, 2.45462e-01, 1.89840e-02, 2.41009e-03
Node: 04 (pos: 0.040): 3.93506e-01, 1.89254e-01, 3.47730e-01, 2.33922e-01, 1.84552e-02, 2.25338e-03
Node: 05 (pos: 0.051): 3.90567e-01, 1.81937e-01, 3.34793e-01, 2.22808e-01, 1.79389e-02, 2.10740e-03
-
Node: 07 (pos: 0.071): 3.84769e-01, 1.68023e-01, 3.09540e-01, 2.01847e-01, 1.69429e-02, 1.84461e-03
Node: 08 (pos: 0.081): 3.81911e-01, 1.61417e-01, 2.97257e-01, 1.91991e-01, 1.64628e-02, 1.72643e-03
Node: 09 (pos: 0.091): 3.79079e-01, 1.55040e-01, 2.85222e-01, 1.82543e-01, 1.59943e-02, 1.61623e-03
Node: 10 (pos: 0.101): 3.76274e-01, 1.48887e-01, 2.73447e-01, 1.73496e-01, 1.55371e-02, 1.51345e-03
Node: 11 (pos: 0.111): 3.73496e-01, 1.42953e-01, 2.61945e-01, 1.64841e-01, 1.50911e-02, 1.41757e-03
Node: 12 (pos: 0.121): 3.70745e-01, 1.37234e-01, 2.50726e-01, 1.56568e-01, 1.46561e-02, 1.32810e-03

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.19765e-01, 6.04768e-01, 2.37876e-01, 5.31802e-01, 9.98598e-01, 9.99972e-01
Node: 01 (pos: 0.010): 3.25272e-01, 6.27636e-01, 2.61701e-01, 5.62406e-01, 9.98674e-01, 9.99976e-01
Node: 02 (pos: 0.020): 3.30781e-01, 6.49753e-01, 2.86727e-01, 5.92215e-01, 9.98746e-01, 9.99979e-01
Node: 03 (pos: 0.030): 3.36291e-01, 6.71070e-01, 3.12839e-01, 6.21070e-01, 9.98814e-01, 9.99982e-01
Node: 04 (pos: 0.040): 3.41800e-01, 6.91549e-01, 3.39905e-01, 6.48834e-01, 9.98880e-01, 9.99984e-01
Node: 05 (pos: 0.051): 3.47305e-01, 7.11163e-01, 3.67774e-01, 6.75401e-01, 9.98941e-01, 9.99986e-01
-
Node: 07 (pos: 0.071): 3.58299e-01, 7.47733e-01, 4.25249e-01, 7.24640e-01, 9.99056e-01, 9.99989e-01
Node: 08 (pos: 0.081): 3.63784e-01, 7.64677e-01, 4.54497e-01, 7.47221e-01, 9.99108e-01, 9.99991e-01
Node: 09 (pos: 0.091): 3.69259e-01, 7.80734e-01, 4.83839e-01, 7.68418e-01, 9.99158e-01, 9.99992e-01
Node: 10 (pos: 0.101): 3.74723e-01, 7.95913e-01, 5.13093e-01, 7.88236e-01, 9.99206e-01, 9.99993e-01
Node: 11 (pos: 0.111): 3.80174e-01, 8.10233e-01, 5.42080e-01, 8.06696e-01, 9.99251e-01, 9.99994e-01
Node: 12 (pos: 0.121): 3.85609e-01, 8.23715e-01, 5.70632e-01, 8.23832e-01, 9.99293e-01, 9.99994e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.41484e-01, 7.69268e-06, 3.27238e-01, 7.42401e-04, 3.69089e-03, 1.28769e-06
Node: 01 (pos: 0.010): 3.39582e-01, 7.33972e-06, 3.13526e-01, 7.07103e-04, 3.54837e-03, 1.21376e-06
Node: 02 (pos: 0.020): 3.37691e-01, 7.00354e-06, 3.00117e-01, 6.73533e-04, 3.41102e-03, 1.14430e-06
Node: 03 (pos: 0.030): 3.35813e-01, 6.68333e-06, 2.87029e-01, 6.41602e-04, 3.27866e-03, 1.07904e-06
Node: 04 (pos: 0.040): 3.33946e-01, 6.37828e-06, 2.74275e-01, 6.11228e-04, 3.15112e-03, 1.01769e-06
Node: 05 (pos: 0.051): 3.55149e-01, 1.07135e-05, 4.29976e-01, 1.04641e-03, 4.84810e-03, 1.95890e-06
-
Node: 07 (pos: 0.071): 3.53159e-01, 1.02156e-05, 4.14746e-01, 9.96085e-04, 4.66436e-03, 1.84381e-06
Node: 08 (pos: 0.081): 3.51183e-01, 9.74171e-06, 3.99656e-01, 9.48265e-04, 4.48708e-03, 1.73585e-06
Node: 09 (pos: 0.091): 3.49218e-01, 9.29066e-06, 3.84734e-01, 9.02819e-04, 4.31608e-03, 1.63454e-06
Node: 10 (pos: 0.101): 3.47266e-01, 8.86129e-06, 3.70007e-01, 8.59622e-04, 4.15114e-03, 1.53945e-06
Node: 11 (pos: 0.111): 3.45327e-01, 8.45252e-06, 3.55500e-01, 8.18560e-04, 3.99209e-03, 1.45020e-06
Node: 12 (pos: 0.121): 3.32092e-01, 6.08766e-06, 2.61867e-01, 5.82332e-04, 3.02826e-03, 9.60028e-07

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.45549e-01, 1.00000e+00, 3.84560e-01, 9.99996e-01, 9.99955e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.49569e-01, 1.00000e+00, 4.15928e-01, 9.99996e-01, 9.99959e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.53578e-01, 1.00000e+00, 4.47619e-01, 9.99996e-01, 9.99962e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.57574e-01, 1.00000e+00, 4.79395e-01, 9.99997e-01, 9.99965e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.61557e-01, 1.00000e+00, 5.11022e-01, 9.99997e-01, 9.99967e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.17094e-01, 1.00000e+00, 1.92067e-01, 9.99991e-01, 9.99923e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.21189e-01, 1.00000e+00, 2.15434e-01, 9.99992e-01, 9.99928e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.25274e-01, 1.00000e+00, 2.40405e-01, 9.99993e-01, 9.99934e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.29350e-01, 1.00000e+00, 2.66875e-01, 9.99994e-01, 9.99939e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.33416e-01, 1.00000e+00, 2.94705e-01, 9.99994e-01, 9.99943e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.37471e-01, 1.00000e+00, 3.23728e-01, 9.99995e-01, 9.99948e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.65526e-01, 1.00000e+00, 5.42277e-01, 9.99997e-01, 9.99970e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.4618709936869853), ('0.bias', 0.30685775955865463), ('2.weight', 3.758773621441097), ('2.bias', 0.34035945623274144)] 

GNN_Layer 1 gradients:
[('0.weight', 0.0056555060998363955), ('0.bias', 0.05876761115248499), ('2.weight', 0.03576692193028823), ('2.bias', 0.04407877968724488)] 

GNN_Layer 2 gradients:
[('0.weight', 0.11512644500615123), ('0.bias', 0.05131466036919702), ('2.weight', 0.5563857455133724), ('2.bias', 0.02737996492538247)] 

GNN_Layer 3 gradients:
[('0.weight', 0.0076126902883928175), ('0.bias', 0.09330710330067973), ('2.weight', 0.06611087256980788), ('2.bias', 0.09659811498366289)] 

GNN_Layer 4 gradients:
[('0.weight', 2.549879627422984e-05), ('0.bias', 0.0002700711644371028), ('2.weight', 0.00029535816065953067), ('2.bias', 0.0001876741788306154)] 

GNN_Layer 5 gradients:
[('0.weight', 2.104158199778025e-07), ('0.bias', 1.1351516468941714e-05), ('2.weight', 9.901382141622554e-06), ('2.bias', 6.392259297296255e-06)] 

Evaluation on test dataset:
Step 25, mean loss 0.03173185852483995
Step 50, mean loss 0.01596474778796174
Step 75, mean loss 0.02069666633939401
Step 100, mean loss 0.026554415144908517
Step 125, mean loss 0.03624947533538966
Step 150, mean loss 0.05386405446706937
Step 175, mean loss 0.06901579847727335
Step 200, mean loss 0.09098617324054067
Step 225, mean loss 0.08276969552370902
Unrolled forward losses 1.398782739828376
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.13405622617041166, [0.14424381982246318, 0.09715440463721739, 0.11202996954667661, 0.1265600215711171, 0.30388885806372845, 0.3155011923072933]
Training Loss (progress: 0.08), 0.12629249613280513, [0.14424490592459482, 0.09693287900692654, 0.11184820129374887, 0.12641499326893837, 0.304197399325209, 0.3158311349149545]
Training Loss (progress: 0.16), 0.12181660563527115, [0.1443770916217528, 0.09693321156248762, 0.11178699866169936, 0.12631468994023362, 0.3044788292807113, 0.3160415646645171]
Training Loss (progress: 0.24), 0.12613376686134092, [0.14416766692697777, 0.09707621373116876, 0.111687657946106, 0.12647428049332463, 0.3051961315863393, 0.31652271588423636]
Training Loss (progress: 0.32), 0.11911967597747267, [0.14442097359847017, 0.09713705342562613, 0.11165730271153397, 0.12681067343906585, 0.30546888699465885, 0.3167274072716233]
Training Loss (progress: 0.40), 0.12493877699746711, [0.144364743653843, 0.09706807857910948, 0.11160940641849416, 0.12659263547020594, 0.30596331145874295, 0.31728110590142605]
Training Loss (progress: 0.48), 0.12654849007680707, [0.14432580193999606, 0.09718098615216189, 0.11128403954469428, 0.12679449755817615, 0.3064946308932835, 0.3174136660696384]
Training Loss (progress: 0.56), 0.12648929775813203, [0.14448982813921185, 0.09700523959199975, 0.11126137171949409, 0.1265407858058494, 0.3069495776236222, 0.3178266854818326]
Training Loss (progress: 0.64), 0.11166055067953505, [0.1445161420821915, 0.09720145530728182, 0.11116679391267134, 0.12681705293704743, 0.30764379893362426, 0.3180257453127586]
Training Loss (progress: 0.72), 0.11810649332870529, [0.14446224690740403, 0.09706471841911614, 0.11100874227325393, 0.12674126698386712, 0.3078985816131387, 0.31813995103869225]
Training Loss (progress: 0.80), 0.1236674659648528, [0.1446202198254735, 0.09719886634831414, 0.11099803747534726, 0.12692261437957028, 0.30885276943169887, 0.3186355647841822]
Training Loss (progress: 0.88), 0.12356419125581218, [0.14456674564958624, 0.0969660937503883, 0.1108899666765273, 0.12665269700805523, 0.30944130583088875, 0.31901508844850057]
Training Loss (progress: 0.96), 0.12109236704715276, [0.14461105504351718, 0.09710925492308917, 0.11077854269278377, 0.12673232562241668, 0.3099418959785207, 0.3197431816303784]
Evaluation on validation dataset:
Step 25, mean loss 0.04063828530404236
Step 50, mean loss 0.020910004953470052
Step 75, mean loss 0.03121313210025778
Step 100, mean loss 0.039582184680917794
Step 125, mean loss 0.04534940613109833
Step 150, mean loss 0.04885495200680607
Step 175, mean loss 0.06904083608629707
Step 200, mean loss 0.3092375086568145
Step 225, mean loss 0.2233300144235433
Unrolled forward losses 1.9512476190055115
Unrolled forward base losses 2.565701273852575
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.12980058435314965, [0.14473918377243727, 0.09713742581545039, 0.11069261169797007, 0.12662434177984797, 0.3098708219627362, 0.3194464232456686]
Training Loss (progress: 0.08), 0.12070343772669831, [0.14475829233546575, 0.09696798746077581, 0.11070429515784078, 0.12662292941568065, 0.31097588122534386, 0.32061720956713785]
Training Loss (progress: 0.16), 0.11857419484762827, [0.14454076992764509, 0.09697095435889465, 0.11051259021358062, 0.12681671673373415, 0.3111553372244761, 0.3205203067304745]
Training Loss (progress: 0.24), 0.1284934166395771, [0.14471226662287956, 0.09702382585285313, 0.11049072692904, 0.1265528706945206, 0.31190917498739984, 0.32096956527632337]
Training Loss (progress: 0.32), 0.12230870015083137, [0.14467391318300435, 0.09678582449861099, 0.11033086971362745, 0.1265731799531192, 0.31213482118390873, 0.3209963648961692]
Training Loss (progress: 0.40), 0.12164662991351925, [0.14476314799542198, 0.09695882553505568, 0.1102238187207904, 0.12656358919556768, 0.3126754918372831, 0.32182877158291784]
Training Loss (progress: 0.48), 0.11787616584147863, [0.14471899707672217, 0.09694301345870779, 0.11030062817928171, 0.1266171731946591, 0.31314476452675183, 0.3222748374644456]
Training Loss (progress: 0.56), 0.11816090689231522, [0.14469162469814278, 0.09698417978296159, 0.11021891116492388, 0.12673951728009544, 0.31369583287640057, 0.32279129721273025]
Training Loss (progress: 0.64), 0.11726305589135161, [0.14476274623451066, 0.096818035755901, 0.10998610983888607, 0.12679274474666355, 0.3141785578193167, 0.3228579718511107]
Training Loss (progress: 0.72), 0.13176716758956755, [0.1448478988535662, 0.09685070047039804, 0.10998899567582822, 0.12670317574637172, 0.3144043022735449, 0.3232802792867343]
Training Loss (progress: 0.80), 0.1252839413218527, [0.14481882249155015, 0.09688072599515411, 0.10984129397146336, 0.12665405444728775, 0.3146330631245736, 0.3236536879614087]
Training Loss (progress: 0.88), 0.11229714884319143, [0.144867656439814, 0.0967190235119741, 0.1098162908696642, 0.12663867628789768, 0.3156977354474742, 0.3246191814491437]
Training Loss (progress: 0.96), 0.1043935933389818, [0.14483589087020193, 0.0966095145603584, 0.10966060588298754, 0.12650087300664975, 0.3158674552359333, 0.32480691234580755]
Evaluation on validation dataset:
Step 25, mean loss 0.03778844412521379
Step 50, mean loss 0.021902134789199147
Step 75, mean loss 0.03393157367831466
Step 100, mean loss 0.03935978464143003
Step 125, mean loss 0.04755137116435028
Step 150, mean loss 0.04631656415262643
Step 175, mean loss 0.07180573081474642
Step 200, mean loss 0.2556487996432027
Step 225, mean loss 0.20344181831372243
Unrolled forward losses 2.4111592654571523
Unrolled forward base losses 2.565701273852575
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.11966166270541112, [0.14478887972419938, 0.09675708477636874, 0.10964350984208454, 0.12683135140632346, 0.3160555547363456, 0.3248718834102297]
Training Loss (progress: 0.08), 0.11696779291444706, [0.14492396373855201, 0.0967896193763206, 0.10972747249781849, 0.12667687981704642, 0.3163644422207935, 0.32483391213741825]
Training Loss (progress: 0.16), 0.10545614875250942, [0.14491121625172096, 0.09672350097696214, 0.10967273657360602, 0.12675605304747206, 0.3166334791699387, 0.3251151323693221]
Training Loss (progress: 0.24), 0.10690988325621409, [0.14501740356991275, 0.09682675390266622, 0.10964612543532906, 0.12671978271555712, 0.3169530839966123, 0.32523363662263205]
Training Loss (progress: 0.32), 0.11012522863216134, [0.14500961675944138, 0.09677772859266194, 0.10968159964179654, 0.12676677033881234, 0.3172894588045244, 0.32542930061523123]
Training Loss (progress: 0.40), 0.11709293381244276, [0.14502571963089067, 0.096740533175532, 0.10960864947972777, 0.1269694188171821, 0.3173953175359355, 0.32567398034895356]
Training Loss (progress: 0.48), 0.10635373833744738, [0.14500362851110435, 0.09673427998108004, 0.1096079730615466, 0.12687647179145917, 0.31754671288548736, 0.32565550617431627]
Training Loss (progress: 0.56), 0.1096332458408401, [0.14519076272032436, 0.09670292391343811, 0.10952138453506707, 0.12688432827814883, 0.31760683154829106, 0.32589959088322823]
Training Loss (progress: 0.64), 0.10879606963884277, [0.1451953436361084, 0.09684294326599782, 0.10953323451277258, 0.12681565020233027, 0.31791326484274646, 0.3262063989129161]
Training Loss (progress: 0.72), 0.11335730167947143, [0.14520823083824191, 0.09676334781260743, 0.10957130043020001, 0.12699888911410917, 0.3183569772640336, 0.3264620391356174]
Training Loss (progress: 0.80), 0.1159991057693193, [0.14519080383476193, 0.09677509389311657, 0.1094851103113731, 0.12700290988130672, 0.3185659290653682, 0.3266515440089045]
Training Loss (progress: 0.88), 0.11288138228149976, [0.14514599735133038, 0.09680229686662904, 0.10948556471518868, 0.12686377706785368, 0.31864596540270446, 0.32660466583239933]
Training Loss (progress: 0.96), 0.11893780101387211, [0.14526753824779362, 0.09681930132096393, 0.10946073751438778, 0.12701835636587716, 0.3189917842814303, 0.32674488530747164]
Evaluation on validation dataset:
Step 25, mean loss 0.032487479699240096
Step 50, mean loss 0.016871357713142656
Step 75, mean loss 0.025803855403005314
Step 100, mean loss 0.032628454088214964
Step 125, mean loss 0.03760810695055728
Step 150, mean loss 0.04067310845982026
Step 175, mean loss 0.06261296797787119
Step 200, mean loss 0.2597192485629006
Step 225, mean loss 0.19390490227327625
Unrolled forward losses 1.8516548401162942
Unrolled forward base losses 2.565701273852575
Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.11779841698247392, [0.14527860229096468, 0.09683303930873102, 0.1094324498094155, 0.12710170049983735, 0.31918160231388565, 0.3270019686067318]
Training Loss (progress: 0.08), 0.10639615061120808, [0.1452934544496469, 0.09684852282442816, 0.10944340160191146, 0.12696492576664514, 0.31929218459050135, 0.3270440306277475]
Training Loss (progress: 0.16), 0.10322066551982846, [0.14531265001454147, 0.09682319282705677, 0.10940264836621097, 0.12698341682578293, 0.31931887714274004, 0.32715692701957316]
Training Loss (progress: 0.24), 0.11443228144648751, [0.14532524166242694, 0.09678246774132188, 0.10941472640383362, 0.12703113141433914, 0.319359600913511, 0.3274147279778949]
Training Loss (progress: 0.32), 0.11819302340317109, [0.1454487109987482, 0.0967944033142007, 0.10934739775048796, 0.12707492084018396, 0.31969234334762214, 0.32764602078306243]
Training Loss (progress: 0.40), 0.11053225138658952, [0.14531644944215708, 0.09682053120894969, 0.10928671743819325, 0.12703683860336226, 0.3199165482224463, 0.32798953418569843]
Training Loss (progress: 0.48), 0.10921887573613354, [0.14542175044156463, 0.09681895871781715, 0.10927070452816381, 0.12692726066882562, 0.3201191052724148, 0.32782892213834497]
Training Loss (progress: 0.56), 0.10697852650694913, [0.14534631284608399, 0.09682746791935136, 0.1092680549089065, 0.12697237256668337, 0.3202992126746563, 0.3281102248150968]
Training Loss (progress: 0.64), 0.10717450951092519, [0.14538817309985871, 0.0967406963690335, 0.10921206987289488, 0.12692756053749982, 0.3206047781211733, 0.3283050031584875]
Training Loss (progress: 0.72), 0.11402510991879279, [0.14548316852767337, 0.09677840788341342, 0.1091694354836429, 0.1269702627801236, 0.32071701877365744, 0.32840835161235454]
Training Loss (progress: 0.80), 0.11665058048913293, [0.14545150168550983, 0.09677087799097389, 0.10917613938333452, 0.1269973619439892, 0.32089929559352864, 0.32876125393429306]
Training Loss (progress: 0.88), 0.10165176642798246, [0.14545346626075764, 0.09687163442614366, 0.10911444820945897, 0.12697836355617653, 0.3210867601717253, 0.3289178347617686]
Training Loss (progress: 0.96), 0.12165097051590848, [0.14552667668184416, 0.09684482435792716, 0.10910232828806163, 0.12706086408047007, 0.32138857569601115, 0.32915424599232257]
Evaluation on validation dataset:
Step 25, mean loss 0.03259853383362469
Step 50, mean loss 0.017548222726579566
Step 75, mean loss 0.024661127565032134
Step 100, mean loss 0.03295067023853421
Step 125, mean loss 0.036338724967855965
Step 150, mean loss 0.041631673177854364
Step 175, mean loss 0.06441945703613303
Step 200, mean loss 0.26869789727265053
Step 225, mean loss 0.19708361974581645
Unrolled forward losses 1.7044354338028918
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.02764e-01, 2.19514e-01, 3.94457e-01, 2.77338e-01, 1.04749e-02, 1.59117e-03
Node: 01 (pos: 0.010): 3.99642e-01, 2.11513e-01, 3.80573e-01, 2.64677e-01, 1.01031e-02, 1.48116e-03
Node: 02 (pos: 0.020): 3.96546e-01, 2.03748e-01, 3.66838e-01, 2.52439e-01, 9.74313e-03, 1.37910e-03
Node: 03 (pos: 0.030): 3.93478e-01, 1.96216e-01, 3.53274e-01, 2.40627e-01, 9.39478e-03, 1.28440e-03
Node: 04 (pos: 0.040): 3.90437e-01, 1.88916e-01, 3.39900e-01, 2.29242e-01, 9.05769e-03, 1.19650e-03
Node: 05 (pos: 0.051): 3.87423e-01, 1.81844e-01, 3.26735e-01, 2.18282e-01, 8.73155e-03, 1.11491e-03
-
Node: 07 (pos: 0.071): 3.81476e-01, 1.68375e-01, 3.01106e-01, 1.97628e-01, 8.11085e-03, 9.68762e-04
Node: 08 (pos: 0.081): 3.78543e-01, 1.61969e-01, 2.88673e-01, 1.87924e-01, 7.81569e-03, 9.03381e-04
Node: 09 (pos: 0.091): 3.75637e-01, 1.55778e-01, 2.76514e-01, 1.78625e-01, 7.53026e-03, 8.42626e-04
Node: 10 (pos: 0.101): 3.72758e-01, 1.49797e-01, 2.64642e-01, 1.69725e-01, 7.25429e-03, 7.86156e-04
Node: 11 (pos: 0.111): 3.69906e-01, 1.44021e-01, 2.53067e-01, 1.61214e-01, 6.98749e-03, 7.33656e-04
Node: 12 (pos: 0.121): 3.67080e-01, 1.38447e-01, 2.41800e-01, 1.53082e-01, 6.72961e-03, 6.84834e-04

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.28068e-01, 6.08236e-01, 2.40248e-01, 5.45809e-01, 9.99659e-01, 9.99992e-01
Node: 01 (pos: 0.010): 3.33764e-01, 6.30269e-01, 2.65149e-01, 5.76104e-01, 9.99683e-01, 9.99993e-01
Node: 02 (pos: 0.020): 3.39464e-01, 6.51593e-01, 2.91307e-01, 6.05531e-01, 9.99705e-01, 9.99994e-01
Node: 03 (pos: 0.030): 3.45165e-01, 6.72164e-01, 3.18589e-01, 6.33940e-01, 9.99726e-01, 9.99995e-01
Node: 04 (pos: 0.040): 3.50865e-01, 6.91948e-01, 3.46842e-01, 6.61206e-01, 9.99745e-01, 9.99996e-01
Node: 05 (pos: 0.051): 3.56563e-01, 7.10923e-01, 3.75893e-01, 6.87234e-01, 9.99763e-01, 9.99996e-01
-
Node: 07 (pos: 0.071): 3.67942e-01, 7.46383e-01, 4.35627e-01, 7.35314e-01, 9.99795e-01, 9.99997e-01
Node: 08 (pos: 0.081): 3.73621e-01, 7.62858e-01, 4.65910e-01, 7.57294e-01, 9.99810e-01, 9.99998e-01
Node: 09 (pos: 0.091): 3.79289e-01, 7.78501e-01, 4.96199e-01, 7.77888e-01, 9.99824e-01, 9.99998e-01
Node: 10 (pos: 0.101): 3.84946e-01, 7.93321e-01, 5.26295e-01, 7.97107e-01, 9.99836e-01, 9.99998e-01
Node: 11 (pos: 0.111): 3.90589e-01, 8.07334e-01, 5.56008e-01, 8.14979e-01, 9.99848e-01, 9.99998e-01
Node: 12 (pos: 0.121): 3.96218e-01, 8.20557e-01, 5.85160e-01, 8.31543e-01, 9.99859e-01, 9.99999e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.38056e-01, 8.53554e-06, 3.17506e-01, 6.86123e-04, 1.06367e-03, 4.73250e-07
Node: 01 (pos: 0.010): 3.36055e-01, 8.15443e-06, 3.03682e-01, 6.53340e-04, 1.01384e-03, 4.44317e-07
Node: 02 (pos: 0.020): 3.34067e-01, 7.79097e-06, 2.90190e-01, 6.22169e-04, 9.66230e-04, 4.17236e-07
Node: 03 (pos: 0.030): 3.32093e-01, 7.44431e-06, 2.77046e-01, 5.92528e-04, 9.20757e-04, 3.91883e-07
Node: 04 (pos: 0.040): 3.30131e-01, 7.11364e-06, 2.64264e-01, 5.64339e-04, 8.77329e-04, 3.68143e-07
Node: 05 (pos: 0.051): 3.52427e-01, 1.17791e-05, 4.21802e-01, 9.68758e-04, 1.48347e-03, 7.40134e-07
-
Node: 07 (pos: 0.071): 3.50334e-01, 1.12465e-05, 4.06271e-01, 9.21942e-04, 1.41513e-03, 6.93906e-07
Node: 08 (pos: 0.081): 3.48255e-01, 1.07389e-05, 3.90904e-01, 8.77465e-04, 1.34977e-03, 6.50699e-07
Node: 09 (pos: 0.091): 3.46188e-01, 1.02551e-05, 3.75731e-01, 8.35205e-04, 1.28727e-03, 6.10305e-07
Node: 10 (pos: 0.101): 3.44136e-01, 9.79396e-06, 3.60780e-01, 7.95047e-04, 1.22752e-03, 5.72535e-07
Node: 11 (pos: 0.111): 3.42096e-01, 9.35434e-06, 3.46076e-01, 7.56882e-04, 1.17041e-03, 5.37211e-07
Node: 12 (pos: 0.121): 3.28182e-01, 6.79819e-06, 2.51855e-01, 5.37528e-04, 8.35860e-04, 3.45909e-07

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.56039e-01, 1.00000e+00, 3.96946e-01, 9.99996e-01, 9.99996e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.60284e-01, 1.00000e+00, 4.29450e-01, 9.99997e-01, 9.99997e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.64515e-01, 1.00000e+00, 4.62174e-01, 9.99997e-01, 9.99997e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.68733e-01, 1.00000e+00, 4.94860e-01, 9.99997e-01, 9.99997e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.72935e-01, 1.00000e+00, 5.27259e-01, 9.99997e-01, 9.99998e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.25984e-01, 1.00000e+00, 1.95799e-01, 9.99993e-01, 9.99993e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.30310e-01, 1.00000e+00, 2.20295e-01, 9.99993e-01, 9.99994e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.34626e-01, 1.00000e+00, 2.46471e-01, 9.99994e-01, 9.99994e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.38932e-01, 1.00000e+00, 2.74199e-01, 9.99995e-01, 9.99995e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.43227e-01, 1.00000e+00, 3.03318e-01, 9.99995e-01, 9.99995e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.47510e-01, 1.00000e+00, 3.33632e-01, 9.99995e-01, 9.99996e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.77123e-01, 1.00000e+00, 5.59136e-01, 9.99998e-01, 9.99998e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.3855117093775755), ('0.bias', 0.335862273333856), ('2.weight', 3.1124350863933), ('2.bias', 0.46362289033383186)] 

GNN_Layer 1 gradients:
[('0.weight', 0.02955507135925102), ('0.bias', 0.614880833216433), ('2.weight', 0.18158510715129328), ('2.bias', 0.45087907219902634)] 

GNN_Layer 2 gradients:
[('0.weight', 0.6202175700769004), ('0.bias', 0.26823584797072286), ('2.weight', 2.848090211159938), ('2.bias', 0.13477644970569036)] 

GNN_Layer 3 gradients:
[('0.weight', 0.02364672454542365), ('0.bias', 0.36571869609133656), ('2.weight', 0.20987014101444576), ('2.bias', 0.37306545005885766)] 

GNN_Layer 4 gradients:
[('0.weight', 1.1087412379640758e-05), ('0.bias', 0.00015873170169675393), ('2.weight', 0.00016914244527767034), ('2.bias', 0.00010218423732098792)] 

GNN_Layer 5 gradients:
[('0.weight', 2.776750766057954e-07), ('0.bias', 5.216282831685894e-07), ('2.weight', 8.956873377739663e-07), ('2.bias', 2.5094737558857356e-07)] 

Evaluation on test dataset:
Step 25, mean loss 0.026633376207514534
Step 50, mean loss 0.012713685114259106
Step 75, mean loss 0.01963822636714891
Step 100, mean loss 0.024431516913809244
Step 125, mean loss 0.03334163016237892
Step 150, mean loss 0.046581445370161643
Step 175, mean loss 0.06378637816172407
Step 200, mean loss 0.07865309822629077
Step 225, mean loss 0.07466231491672817
Unrolled forward losses 1.3349594930189341
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.11022904936327299, [0.1455074366850466, 0.09691789753376007, 0.10914876672131564, 0.12701562818374074, 0.32154425128289504, 0.32922532345250277]
Training Loss (progress: 0.08), 0.10890405817715217, [0.14550284121567758, 0.09682539887682176, 0.10907388378314142, 0.1268806525987215, 0.3217650099845247, 0.3293475148793818]
Training Loss (progress: 0.16), 0.11622106396560643, [0.14553127063723037, 0.09675595957479322, 0.10907332680023389, 0.12706363397016077, 0.3219205947206932, 0.32938740343618084]
Training Loss (progress: 0.24), 0.12330335080589933, [0.1456614171844216, 0.0967817228657852, 0.10904777175180942, 0.12714604036862195, 0.32202329044166994, 0.3293427896574168]
Training Loss (progress: 0.32), 0.11154552498438974, [0.1455915738987272, 0.09685929407779806, 0.10902610180324486, 0.1271083931101496, 0.3221125736129071, 0.3295156673820906]
Training Loss (progress: 0.40), 0.11182343268712258, [0.14566884395654234, 0.09681372577239007, 0.10904634212650408, 0.12708399582895186, 0.32227399248653843, 0.3297355883396243]
Training Loss (progress: 0.48), 0.09512473202455479, [0.14567366506811094, 0.09687571100179324, 0.10898935333571526, 0.1271481453031904, 0.3225297209325936, 0.3296372083881246]
Training Loss (progress: 0.56), 0.11787562945535626, [0.1456220255898525, 0.09683680030320511, 0.1090019883529619, 0.12714679606433726, 0.32282267653069124, 0.33008300875676194]
Training Loss (progress: 0.64), 0.11202069715659084, [0.1456942166057198, 0.09691430361323063, 0.10892532932607835, 0.12716859422079602, 0.32312530130008293, 0.3301985843465403]
Training Loss (progress: 0.72), 0.10517480543438287, [0.1456672303625705, 0.09683518648330597, 0.10892285611842993, 0.12710469624958995, 0.3230872457298868, 0.33044206090785816]
Training Loss (progress: 0.80), 0.11519423918254987, [0.14572887629524245, 0.09683051919188929, 0.10893905479987562, 0.12710031954974385, 0.3235211003206763, 0.33051398580287267]
Training Loss (progress: 0.88), 0.10470402219731915, [0.1457084273338315, 0.09680246445965844, 0.10884155653776002, 0.127134936625519, 0.32352980244932134, 0.33081872981694144]
Training Loss (progress: 0.96), 0.11489376887891439, [0.1458381152539713, 0.09667671207190388, 0.10887740819107465, 0.12719270215994663, 0.32358025215117264, 0.3306908351256247]
Evaluation on validation dataset:
Step 25, mean loss 0.029534505116900034
Step 50, mean loss 0.016711970323860945
Step 75, mean loss 0.024975187517820154
Step 100, mean loss 0.03136403417918156
Step 125, mean loss 0.035519940089531826
Step 150, mean loss 0.03997733759285724
Step 175, mean loss 0.062345219673588394
Step 200, mean loss 0.26220816906100575
Step 225, mean loss 0.19192823163368766
Unrolled forward losses 1.7944493998605422
Unrolled forward base losses 2.565701273852575
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.10282047603283258, [0.14584936543638502, 0.09666765141336996, 0.10880692904475299, 0.12713013055866562, 0.3236611874183704, 0.33085422987590246]
Training Loss (progress: 0.08), 0.11454759858327122, [0.1458047510774427, 0.09680483673394054, 0.10881827841839518, 0.12723652472459998, 0.3240804622734031, 0.33098950554946366]
Training Loss (progress: 0.16), 0.106421773305112, [0.14582696475696322, 0.096708923097435, 0.10872559581117071, 0.12717688092461454, 0.32412674383547696, 0.3310743784649918]
Training Loss (progress: 0.24), 0.11440475046880111, [0.1457791537923629, 0.09680805655754376, 0.10870659306300384, 0.1272586445236619, 0.32434288670728856, 0.33136860671962776]
Training Loss (progress: 0.32), 0.1113560023328088, [0.14575816966330385, 0.09675839204283637, 0.10872534269315942, 0.1271935894434937, 0.3245233827715673, 0.3314933789227348]
Training Loss (progress: 0.40), 0.10845307767012775, [0.14588347769496532, 0.09679102403006995, 0.10875560004577614, 0.12722416461258856, 0.32480318847188183, 0.3316341611012484]
Training Loss (progress: 0.48), 0.10301692456820209, [0.1458184544283645, 0.09672232835073366, 0.1086839356582698, 0.12728855615513784, 0.324901073094207, 0.33200222946569125]
Training Loss (progress: 0.56), 0.11421927842989493, [0.1458748681944763, 0.09678037161000132, 0.10869318256516086, 0.12718529830420539, 0.3252041262348094, 0.33206182873039786]
Training Loss (progress: 0.64), 0.11206174412534199, [0.14595400244435236, 0.09675916681367742, 0.10864356940216433, 0.12728970176132015, 0.32541717997589853, 0.3322966270933657]
Training Loss (progress: 0.72), 0.1148411431428018, [0.14592747928210303, 0.09680932229603127, 0.10860345936794577, 0.1274253863549842, 0.3255813293509986, 0.3324789421721457]
Training Loss (progress: 0.80), 0.1080068212449141, [0.14599013257070867, 0.09678016055871888, 0.10855312196684849, 0.12726554227666353, 0.32576027046655925, 0.33268567144200534]
Training Loss (progress: 0.88), 0.10707961038446819, [0.14603648499637698, 0.09670805608973666, 0.10852562641038892, 0.12730841075104646, 0.3258569721733155, 0.33296666055451213]
Training Loss (progress: 0.96), 0.10291875901713667, [0.14610859841501395, 0.09681090000208349, 0.1085165148203033, 0.1272031580954591, 0.3259969485409975, 0.3328411314233488]
Evaluation on validation dataset:
Step 25, mean loss 0.031536141605240715
Step 50, mean loss 0.016175801519431213
Step 75, mean loss 0.023874892206993882
Step 100, mean loss 0.030711293800426257
Step 125, mean loss 0.03533510157758597
Step 150, mean loss 0.04080942074747211
Step 175, mean loss 0.061472337654361185
Step 200, mean loss 0.2697061837256806
Step 225, mean loss 0.19879489259384822
Unrolled forward losses 1.7173751826359562
Unrolled forward base losses 2.565701273852575
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00), 0.1095266038817996, [0.1461289630729272, 0.09683038542923617, 0.10855724328523449, 0.12716699081342467, 0.32604091104398014, 0.332764289120227]
Training Loss (progress: 0.08), 0.10600547696365209, [0.14604451392865778, 0.09674320050743473, 0.10853598405778339, 0.1272376880439512, 0.3263541958914785, 0.33330614568434075]
Training Loss (progress: 0.16), 0.11035538667980063, [0.14614820519436492, 0.09676901358068357, 0.10848842499358387, 0.12733833579880124, 0.3262271231034962, 0.333150128422276]
Training Loss (progress: 0.24), 0.10794073462815386, [0.14614143616801478, 0.09675920819731353, 0.10849385642902276, 0.12729881367617793, 0.3263922320252804, 0.33346461006548667]
Training Loss (progress: 0.32), 0.11542702166900011, [0.14616195325562958, 0.09678086767405555, 0.10847048021697221, 0.127374018882061, 0.3267017137620822, 0.33339169129330803]
Training Loss (progress: 0.40), 0.12005785455231006, [0.14619306390782402, 0.09668538705539798, 0.10837746363119434, 0.12736673682738925, 0.32691949225121547, 0.33364032012858097]
Training Loss (progress: 0.48), 0.11308389877508283, [0.14618172933919232, 0.09680444890630413, 0.10839090796878652, 0.12736482627171436, 0.3271904795086025, 0.3338075746682262]
Training Loss (progress: 0.56), 0.11380191642715379, [0.14613592974187026, 0.09678533236301101, 0.10837016385191653, 0.12737224696192082, 0.3274090847474882, 0.3339688940931808]
Training Loss (progress: 0.64), 0.11522386089066565, [0.14621248330319317, 0.09676552998622145, 0.10839935358887128, 0.12731273518410602, 0.3275492428396329, 0.3341511848157455]
Training Loss (progress: 0.72), 0.1126425490600313, [0.1461682381633268, 0.09671557831442301, 0.10830153277378676, 0.12739532840254386, 0.3278900762437499, 0.3343800776767675]
Training Loss (progress: 0.80), 0.10524468565635829, [0.146171513226857, 0.09678872111224934, 0.10826107821576353, 0.1274259611477314, 0.3281329383536421, 0.33464103347798074]
Training Loss (progress: 0.88), 0.10336560389054401, [0.14617613088285542, 0.0967405598954929, 0.10823377957190086, 0.12724602774652458, 0.32812198861093417, 0.3347222267999753]
Training Loss (progress: 0.96), 0.10784945864805065, [0.14624680427784575, 0.09667386536614032, 0.10822643741287599, 0.12739033435892205, 0.32833979696951177, 0.3348611900131515]
Evaluation on validation dataset:
Step 25, mean loss 0.03247697198260519
Step 50, mean loss 0.019092182605280904
Step 75, mean loss 0.025538516136794153
Step 100, mean loss 0.031245259358278835
Step 125, mean loss 0.03558228945777885
Step 150, mean loss 0.039634402567262
Step 175, mean loss 0.060751140552582295
Step 200, mean loss 0.2608764029796659
Step 225, mean loss 0.1804023095257701
Unrolled forward losses 1.892733505963975
Unrolled forward base losses 2.565701273852575
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00), 0.10802670086159188, [0.14629555289894788, 0.09675085122940238, 0.1082197627615428, 0.1272692128064768, 0.3285091147807199, 0.33492871176515737]
Training Loss (progress: 0.08), 0.11024133970879284, [0.1462791284723998, 0.09675581066800841, 0.10820429691138266, 0.12734404719091325, 0.3285398487678205, 0.33504483353632203]
Training Loss (progress: 0.16), 0.10775938912293796, [0.1462774468274602, 0.09671328507410308, 0.10817203751547974, 0.12734331315953137, 0.328622331017775, 0.3351247175408366]
Training Loss (progress: 0.24), 0.09511870001537127, [0.14630619340221665, 0.09674419871049375, 0.10818587618873846, 0.12737390509284263, 0.3287059650722546, 0.3351383902791172]
Training Loss (progress: 0.32), 0.09997468424974568, [0.14634332388267618, 0.09674017206396801, 0.10816568020326604, 0.12731888784972883, 0.32875652660195215, 0.33516161720232823]
Training Loss (progress: 0.40), 0.09852314149636852, [0.14631393258634073, 0.09675198039623042, 0.10816516523694483, 0.1273262436401229, 0.3289078367278499, 0.3353626189719082]
Training Loss (progress: 0.48), 0.08915137683045501, [0.14633451386778956, 0.09669308837103516, 0.10819316742245257, 0.12731173638720403, 0.3289200047298689, 0.3352748249029961]
Training Loss (progress: 0.56), 0.09712915565677867, [0.1463817152584671, 0.0966931912856517, 0.10821636947729024, 0.12731987921307603, 0.32909486113285497, 0.3354008214967995]
Training Loss (progress: 0.64), 0.10205614040195016, [0.1463872070841131, 0.09675152612877792, 0.1081723632338058, 0.12732914446451743, 0.3290613026263671, 0.3354003457279049]
Training Loss (progress: 0.72), 0.10858401754071949, [0.14638292701023606, 0.09673667210898172, 0.10816894913980317, 0.12735598663760772, 0.3291559577236212, 0.335456130700713]
Training Loss (progress: 0.80), 0.10031273584069465, [0.14645361752272906, 0.09677437233523803, 0.10817138173712766, 0.12732238567692797, 0.32921047893504407, 0.335501790179684]
Training Loss (progress: 0.88), 0.10113029545489784, [0.14638678659470114, 0.09671776561603498, 0.10816724851315575, 0.12731610259655043, 0.32936469962478765, 0.33567275944778996]
Training Loss (progress: 0.96), 0.10498170187362442, [0.14644926998730015, 0.09672437510298572, 0.10816002141994104, 0.1273546172108078, 0.32945023248840893, 0.33570600949978785]
Evaluation on validation dataset:
Step 25, mean loss 0.027006464346041863
Step 50, mean loss 0.015032569665879587
Step 75, mean loss 0.023568774364286116
Step 100, mean loss 0.029578535272795716
Step 125, mean loss 0.03389839196142337
Step 150, mean loss 0.0384013836540344
Step 175, mean loss 0.057684594761483785
Step 200, mean loss 0.26324017307968317
Step 225, mean loss 0.1935176402786614
Unrolled forward losses 1.6809586401115046
Unrolled forward base losses 2.565701273852575
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 4.00965e-01, 2.19459e-01, 3.89964e-01, 2.74417e-01, 7.49529e-03, 1.17354e-03
Node: 01 (pos: 0.010): 3.97822e-01, 2.11613e-01, 3.75974e-01, 2.61828e-01, 7.19658e-03, 1.09049e-03
Node: 02 (pos: 0.020): 3.94706e-01, 2.03994e-01, 3.62145e-01, 2.49665e-01, 6.90884e-03, 1.01358e-03
Node: 03 (pos: 0.030): 3.91618e-01, 1.96600e-01, 3.48499e-01, 2.37929e-01, 6.63169e-03, 9.42329e-04
Node: 04 (pos: 0.040): 3.88556e-01, 1.89429e-01, 3.35057e-01, 2.26620e-01, 6.36479e-03, 8.76306e-04
Node: 05 (pos: 0.051): 3.85521e-01, 1.82478e-01, 3.21837e-01, 2.15738e-01, 6.10779e-03, 8.15114e-04
-
Node: 07 (pos: 0.071): 3.79533e-01, 1.69223e-01, 2.96134e-01, 1.95240e-01, 5.62218e-03, 7.05782e-04
Node: 08 (pos: 0.081): 3.76579e-01, 1.62913e-01, 2.83684e-01, 1.85613e-01, 5.39293e-03, 6.56992e-04
Node: 09 (pos: 0.091): 3.73652e-01, 1.56810e-01, 2.71521e-01, 1.76391e-01, 5.17231e-03, 6.11728e-04
Node: 10 (pos: 0.101): 3.70752e-01, 1.50910e-01, 2.59655e-01, 1.67567e-01, 4.96003e-03, 5.69726e-04
Node: 11 (pos: 0.111): 3.67879e-01, 1.45208e-01, 2.48099e-01, 1.59131e-01, 4.75580e-03, 5.30741e-04
Node: 12 (pos: 0.121): 3.65032e-01, 1.39700e-01, 2.36862e-01, 1.51073e-01, 4.55933e-03, 4.94547e-04

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 3.33588e-01, 6.07738e-01, 2.44937e-01, 5.53621e-01, 9.99830e-01, 9.99996e-01
Node: 01 (pos: 0.010): 3.39356e-01, 6.29366e-01, 2.70461e-01, 5.83756e-01, 9.99843e-01, 9.99996e-01
Node: 02 (pos: 0.020): 3.45127e-01, 6.50314e-01, 2.97243e-01, 6.12981e-01, 9.99855e-01, 9.99997e-01
Node: 03 (pos: 0.030): 3.50898e-01, 6.70540e-01, 3.25140e-01, 6.41151e-01, 9.99867e-01, 9.99997e-01
Node: 04 (pos: 0.040): 3.56669e-01, 6.90013e-01, 3.53986e-01, 6.68151e-01, 9.99877e-01, 9.99998e-01
Node: 05 (pos: 0.051): 3.62436e-01, 7.08707e-01, 3.83596e-01, 6.93888e-01, 9.99887e-01, 9.99998e-01
-
Node: 07 (pos: 0.071): 3.73954e-01, 7.43705e-01, 4.44308e-01, 7.41338e-01, 9.99904e-01, 9.99999e-01
Node: 08 (pos: 0.081): 3.79701e-01, 7.59998e-01, 4.74991e-01, 7.62990e-01, 9.99912e-01, 9.99999e-01
Node: 09 (pos: 0.091): 3.85437e-01, 7.75488e-01, 5.05612e-01, 7.83253e-01, 9.99919e-01, 9.99999e-01
Node: 10 (pos: 0.101): 3.91161e-01, 7.90186e-01, 5.35966e-01, 8.02143e-01, 9.99925e-01, 9.99999e-01
Node: 11 (pos: 0.111): 3.96872e-01, 8.04103e-01, 5.65862e-01, 8.19690e-01, 9.99931e-01, 9.99999e-01
Node: 12 (pos: 0.121): 4.02566e-01, 8.17256e-01, 5.95121e-01, 8.35937e-01, 9.99937e-01, 9.99999e-01
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): 3.36660e-01, 9.17542e-06, 3.11490e-01, 6.55615e-04, 5.59635e-04, 2.93899e-07
Node: 01 (pos: 0.010): 3.34622e-01, 8.77227e-06, 2.97689e-01, 6.24177e-04, 5.30901e-04, 2.75403e-07
Node: 02 (pos: 0.020): 3.32597e-01, 8.38750e-06, 2.84233e-01, 5.94290e-04, 5.03583e-04, 2.58121e-07
Node: 03 (pos: 0.030): 3.30584e-01, 8.02024e-06, 2.71138e-01, 5.65875e-04, 4.77616e-04, 2.41971e-07
Node: 04 (pos: 0.040): 3.28586e-01, 7.66965e-06, 2.58416e-01, 5.38856e-04, 4.52935e-04, 2.26876e-07
Node: 05 (pos: 0.051): 3.51305e-01, 1.25956e-05, 4.16037e-01, 9.26854e-04, 8.06608e-04, 4.65841e-07
-
Node: 07 (pos: 0.071): 3.49172e-01, 1.20352e-05, 4.00425e-01, 8.81903e-04, 7.65854e-04, 4.35912e-07
Node: 08 (pos: 0.081): 3.47053e-01, 1.15007e-05, 3.84993e-01, 8.39206e-04, 7.27067e-04, 4.07988e-07
Node: 09 (pos: 0.091): 3.44948e-01, 1.09909e-05, 3.69769e-01, 7.98645e-04, 6.90156e-04, 3.81930e-07
Node: 10 (pos: 0.101): 3.42856e-01, 1.05046e-05, 3.54782e-01, 7.60107e-04, 6.55038e-04, 3.57608e-07
Node: 11 (pos: 0.111): 3.40777e-01, 1.00406e-05, 3.40058e-01, 7.23489e-04, 6.21630e-04, 3.34902e-07
Node: 12 (pos: 0.121): 3.26600e-01, 7.33495e-06, 2.46079e-01, 5.13163e-04, 4.29481e-04, 2.12764e-07

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 4.61187e-01, 1.00000e+00, 4.07567e-01, 9.99997e-01, 9.99999e-01, 1.00000e+00
Node: 58 (pos: 0.586): 4.65517e-01, 1.00000e+00, 4.40531e-01, 9.99997e-01, 9.99999e-01, 1.00000e+00
Node: 59 (pos: 0.596): 4.69832e-01, 1.00000e+00, 4.73625e-01, 9.99997e-01, 9.99999e-01, 1.00000e+00
Node: 60 (pos: 0.606): 4.74133e-01, 1.00000e+00, 5.06585e-01, 9.99997e-01, 9.99999e-01, 1.00000e+00
Node: 61 (pos: 0.616): 4.78418e-01, 1.00000e+00, 5.39159e-01, 9.99998e-01, 9.99999e-01, 1.00000e+00
Node: 50 (pos: 0.505): 4.30525e-01, 1.00000e+00, 2.01663e-01, 9.99993e-01, 9.99998e-01, 1.00000e+00
-
Node: 51 (pos: 0.515): 4.34939e-01, 1.00000e+00, 2.26901e-01, 9.99994e-01, 9.99998e-01, 1.00000e+00
Node: 52 (pos: 0.525): 4.39343e-01, 1.00000e+00, 2.53823e-01, 9.99994e-01, 9.99998e-01, 1.00000e+00
Node: 53 (pos: 0.535): 4.43736e-01, 1.00000e+00, 2.82288e-01, 9.99995e-01, 9.99999e-01, 1.00000e+00
Node: 54 (pos: 0.545): 4.48118e-01, 1.00000e+00, 3.12117e-01, 9.99995e-01, 9.99999e-01, 1.00000e+00
Node: 55 (pos: 0.556): 4.52487e-01, 1.00000e+00, 3.43100e-01, 9.99996e-01, 9.99999e-01, 1.00000e+00
Node: 62 (pos: 0.626): 4.82688e-01, 1.00000e+00, 5.71112e-01, 9.99998e-01, 9.99999e-01, 1.00000e+00
=========================================================================================================
GNN_Layer 0 gradients:
[('0.weight', 0.14942333178375256), ('0.bias', 0.2447290292478237), ('2.weight', 1.2234105948748266), ('2.bias', 0.3795649623543905)] 

GNN_Layer 1 gradients:
[('0.weight', 0.01695798528117985), ('0.bias', 0.34157184206857144), ('2.weight', 0.10626592228738765), ('2.bias', 0.25109856661483493)] 

GNN_Layer 2 gradients:
[('0.weight', 0.08952485606533747), ('0.bias', 0.0734553282989102), ('2.weight', 0.4569699948452701), ('2.bias', 0.056653307085802004)] 

GNN_Layer 3 gradients:
[('0.weight', 0.005707101220600191), ('0.bias', 0.054221293075523064), ('2.weight', 0.041232596922239295), ('2.bias', 0.055115631293951126)] 

GNN_Layer 4 gradients:
[('0.weight', 5.647252079014179e-07), ('0.bias', 6.455456599813909e-06), ('2.weight', 7.356469884167121e-06), ('2.bias', 4.078288179610011e-06)] 

GNN_Layer 5 gradients:
[('0.weight', 2.2293467653278015e-08), ('0.bias', 8.683468168338279e-07), ('2.weight', 7.223685314527944e-07), ('2.bias', 4.566707354532528e-07)] 

Evaluation on test dataset:
Step 25, mean loss 0.022076254482880774
Step 50, mean loss 0.010833513051065415
Step 75, mean loss 0.017653214527313825
Step 100, mean loss 0.021664520427841373
Step 125, mean loss 0.02903296532941663
Step 150, mean loss 0.04205334350421204
Step 175, mean loss 0.05838988290641504
Step 200, mean loss 0.0709809927408839
Step 225, mean loss 0.06950426955379659
Unrolled forward losses 1.3003238001649151
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_mlp+product_CE_E1_xresolution100-200_lr0.0001_n6_s0.1_tw25_unrolling2_time512329.tar

Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00), 0.10200426335059143, [0.14644524916915816, 0.09671021030826378, 0.10809557763849514, 0.12736815347049743, 0.32949139963998364, 0.3357453167369854]
Training Loss (progress: 0.08), 0.1081442242776599, [0.14648330456385505, 0.09678421718081634, 0.10812934259603142, 0.12737399489644488, 0.3296767162384492, 0.3359030226094349]
Training Loss (progress: 0.16), 0.10856242007616743, [0.14654286657002788, 0.09681132660678433, 0.10811314646027223, 0.1273695480804669, 0.32959450769239373, 0.3359169558040272]
Training Loss (progress: 0.24), 0.10132198553101057, [0.1464981306633686, 0.09673650854706886, 0.10811518217364753, 0.1273645137425834, 0.32968926142074484, 0.3360723499073979]
Training Loss (progress: 0.32), 0.09998862796772734, [0.14647201014377637, 0.0967426240385918, 0.10810432003725964, 0.12744148393885024, 0.32974548131415987, 0.3361315107467276]
Training Loss (progress: 0.40), 0.10718722556123038, [0.1465402016153739, 0.09674057442969262, 0.10812255806336032, 0.12750277724711984, 0.32988148931410877, 0.33617120923412924]
Training Loss (progress: 0.48), 0.09880807168566755, [0.14653390954169487, 0.0967227422263684, 0.10809329722484451, 0.12748734191191624, 0.3299456204320847, 0.3361622755197246]
Training Loss (progress: 0.56), 0.0950734627880413, [0.14654335326171555, 0.0967409689103142, 0.10807732613154064, 0.1274900136711091, 0.32989198931889563, 0.3363046928714555]
Training Loss (progress: 0.64), 0.10514736707975997, [0.14657367244951267, 0.09675377407086871, 0.10807376695753089, 0.12751377862598234, 0.32995726291371197, 0.33628471721658587]
Training Loss (progress: 0.72), 0.11043743508337568, [0.14656168848588416, 0.09678035054083031, 0.10804846800150504, 0.12749406728491972, 0.330131538474136, 0.33642512728064133]
Training Loss (progress: 0.80), 0.10408286481667416, [0.14660663931635567, 0.09671831002446687, 0.1080517754997463, 0.1274301883981898, 0.33006604927945543, 0.33647130412967985]
Training Loss (progress: 0.88), 0.09920631375432883, [0.1466043023352949, 0.09673004100518547, 0.1080420651011465, 0.12749282680278637, 0.33014106719801567, 0.33643964167875623]
Training Loss (progress: 0.96), 0.09253140831136367, [0.14659424257019235, 0.09674453689841178, 0.10803545352707654, 0.12749942914185697, 0.330322321205474, 0.33661715560421707]
Evaluation on validation dataset:
Step 25, mean loss 0.02837758313215833
Step 50, mean loss 0.01543387943522922
Step 75, mean loss 0.024405405053134502
Step 100, mean loss 0.030248549603158963
Step 125, mean loss 0.03625267109981839
Step 150, mean loss 0.037470768984122674
Step 175, mean loss 0.05931742928722258
Step 200, mean loss 0.2652037701374366
Step 225, mean loss 0.185194981315403
Unrolled forward losses 1.7662359132607417
Unrolled forward base losses 2.565701273852575
Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00), 0.11066527364455624, [0.14659366367142962, 0.09673926678436787, 0.10801905560592316, 0.12752669793540744, 0.33042199057291904, 0.3366467005788606]
Training Loss (progress: 0.08), 0.10152475813388312, [0.14662743609211007, 0.09679795427456904, 0.10801606648368021, 0.12752474587063878, 0.3304439180784232, 0.3366974782203802]
Training Loss (progress: 0.16), 0.10088183122510384, [0.14661031238495695, 0.09679138374513026, 0.10803274432082709, 0.12755335124653044, 0.33043438787807994, 0.33679877991417323]
Training Loss (progress: 0.24), 0.10074079104205555, [0.14663134861652907, 0.09674844249590729, 0.10799015161320356, 0.1275175376220036, 0.3305601315349877, 0.33692447776981443]
Training Loss (progress: 0.32), 0.0984141077070944, [0.14668041737707757, 0.0967813738800158, 0.1079925394718252, 0.12755632444625253, 0.3305980765651033, 0.336920531226716]
Training Loss (progress: 0.40), 0.11334028464001411, [0.14665458878720639, 0.09669940681434021, 0.1080098598204964, 0.12751371395377925, 0.33068204846831795, 0.3370129114350268]
Training Loss (progress: 0.48), 0.10548240378997911, [0.146686416770896, 0.09670971923987337, 0.1079942204773843, 0.12749327529474605, 0.3307563689051678, 0.33709520795851494]
Training Loss (progress: 0.56), 0.09994449811727937, [0.14667049547024893, 0.09669902483740006, 0.1079778485208025, 0.12748566048113583, 0.330762690204099, 0.3371508737947048]
Training Loss (progress: 0.64), 0.10981652568704227, [0.14663233050460103, 0.09667161255281898, 0.1079870103125043, 0.12749147560125806, 0.3308863461498513, 0.33715367165366966]
Training Loss (progress: 0.72), 0.10430580140045675, [0.14664111893967136, 0.09668910068351762, 0.10797293467464825, 0.12751295470382673, 0.3310084724337327, 0.3371810355798051]
Training Loss (progress: 0.80), 0.10964718240937933, [0.14663284091740772, 0.09671153368646752, 0.10797134586500037, 0.1275408053981715, 0.33110126408648455, 0.3373461938794359]
Training Loss (progress: 0.88), 0.10534195906705802, [0.14662922540491585, 0.09669827824902075, 0.10795575470408318, 0.127454064619307, 0.33108349097260714, 0.3372566986824214]
Training Loss (progress: 0.96), 0.10061246564391103, [0.14661785508039557, 0.09673510067533828, 0.1079438811444944, 0.12746449991418446, 0.33117945345654287, 0.33742345772040927]
Evaluation on validation dataset:
Step 25, mean loss 0.026547649517184146
Step 50, mean loss 0.01479427917568114
Step 75, mean loss 0.02353410008678212
Step 100, mean loss 0.02971241631759758
Step 125, mean loss 0.03260568020113892
Step 150, mean loss 0.03726356317132682
Step 175, mean loss 0.0580390258658852
Step 200, mean loss 0.25598333007006463
Step 225, mean loss 0.18702728384871453
Unrolled forward losses 1.688746325004293
Unrolled forward base losses 2.565701273852575
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00), 0.10281380447598179, [0.14664004115291782, 0.09673636186706987, 0.1079628418653481, 0.12750339420082735, 0.33131434331649157, 0.3374307961916773]
Training Loss (progress: 0.08), 0.10120513612891967, [0.14667866955347061, 0.09674709851827194, 0.10796763453372694, 0.1275050444327273, 0.33136891463930546, 0.33756204631834125]
Training Loss (progress: 0.16), 0.10129246362286935, [0.14671378936953647, 0.09668900483987737, 0.10791568487808796, 0.12755029069698082, 0.3314779677770011, 0.3375503294347703]
Training Loss (progress: 0.24), 0.10646959498017441, [0.14672479230886407, 0.09671097994549731, 0.1079274253796844, 0.12747783404395038, 0.3315047149541517, 0.3375525894053036]
Training Loss (progress: 0.32), 0.10701662644394228, [0.1467163237266976, 0.09673441939990082, 0.10790331369598091, 0.12756649513690355, 0.33154767903571375, 0.3376975774912282]
Training Loss (progress: 0.40), 0.10425693195706823, [0.14671803377291512, 0.0967367459768619, 0.10793104462762992, 0.12757906894784868, 0.33165567582693245, 0.3377020723204808]
Training Loss (progress: 0.48), 0.11298413266704631, [0.14675468332393435, 0.09669002188403851, 0.1079069083390528, 0.12754036742117078, 0.3317297568450059, 0.3378475646580354]
Training Loss (progress: 0.56), 0.10643742299087464, [0.1467812446454408, 0.09671488409698095, 0.10790900965565385, 0.12757083845208844, 0.3318675204395702, 0.33797034367781625]
Training Loss (progress: 0.64), 0.1100333215074388, [0.14679030794061326, 0.09675705266361295, 0.10789697873876283, 0.12750261235473043, 0.3319156692440055, 0.3380205299208547]
Training Loss (progress: 0.72), 0.10136695557031793, [0.14680490943085467, 0.09677954125363872, 0.1078734287626692, 0.12757907328094584, 0.3320808085693802, 0.3381667831140973]
Training Loss (progress: 0.80), 0.10495180348536201, [0.1468306879489287, 0.09673948963697672, 0.10789255551156508, 0.12756642660860396, 0.3321599718019513, 0.3382178431521813]
Training Loss (progress: 0.88), 0.10212988981950723, [0.14682903509356582, 0.09676454488957827, 0.10786515081247908, 0.12759478469788757, 0.332200900560851, 0.3382757682425396]
Training Loss (progress: 0.96), 0.10013376053987876, [0.1468377159690675, 0.09681565067422204, 0.10786685989673041, 0.12758319019387757, 0.3323013960235413, 0.33848133433835265]
Evaluation on validation dataset:
Step 25, mean loss 0.025685096967469775
Step 50, mean loss 0.014703445382163675
Step 75, mean loss 0.02303237761337365
Step 100, mean loss 0.029702988770626534
Step 125, mean loss 0.03353295539599628
Step 150, mean loss 0.03821603537787417
Step 175, mean loss 0.057705680081398765
Step 200, mean loss 0.26343503353780723
Step 225, mean loss 0.19626770312210406
Unrolled forward losses 1.6937069854347309
Unrolled forward base losses 2.565701273852575
Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00), 0.10106786986898553, [0.1468034067618178, 0.09676252428140844, 0.10790243864821603, 0.12761113628761087, 0.33238541501840824, 0.33839837853594434]
Training Loss (progress: 0.08), 0.1078781963152561, [0.14683848535855631, 0.09677155407887993, 0.10784516178577161, 0.1275872225622615, 0.3324088016163826, 0.33860289049407194]
Training Loss (progress: 0.16), 0.10187861016892635, [0.1468794186604903, 0.09677776177964038, 0.10784903036831914, 0.12760373281098833, 0.33241858224926574, 0.3384961301329742]
Training Loss (progress: 0.24), 0.11397920330313355, [0.14684615237358944, 0.09669984268356978, 0.10782857736128248, 0.1276481383167707, 0.33258909485743576, 0.3387111211834675]
Training Loss (progress: 0.32), 0.10571759305975514, [0.14685459598987816, 0.09676389829321902, 0.1078210121712987, 0.1276432634332627, 0.332628393488753, 0.338833795835616]
Training Loss (progress: 0.40), 0.11008684383563833, [0.14687109162572987, 0.09676179051876471, 0.10781919754450416, 0.12762195243265897, 0.3326833573378726, 0.33894580166680566]
Training Loss (progress: 0.48), 0.11327486655029702, [0.14685420385785658, 0.09672908822329357, 0.10781452883040449, 0.12762823624968567, 0.33283331043551145, 0.33896493334592753]
Training Loss (progress: 0.56), 0.10734628578760608, [0.1468841405638753, 0.09675886435673056, 0.10779803680918404, 0.12763844806316074, 0.3328996659342967, 0.33897370190052134]
Training Loss (progress: 0.64), 0.10391373373371278, [0.1468738256661687, 0.0967266611669868, 0.10777123054121451, 0.12762065513389237, 0.33297681692549436, 0.3389964574238252]
Training Loss (progress: 0.72), 0.10434546915538241, [0.14686358420059442, 0.09676091455743066, 0.10777396728421225, 0.12762871695088107, 0.333075224476046, 0.33910591512616217]
Training Loss (progress: 0.80), 0.1036048050123298, [0.14690439241360886, 0.09674975339605459, 0.10775250469906918, 0.12764359513054596, 0.3331040052755985, 0.3391279041569332]
Training Loss (progress: 0.88), 0.10282125443019842, [0.14689615023217953, 0.09674046539115351, 0.10772448619736363, 0.12762309808792993, 0.33321446169823055, 0.3392039292643504]
Training Loss (progress: 0.96), 0.11349534159034462, [0.146924309624813, 0.096737463625462, 0.10773008550853723, 0.1276562260796169, 0.3332852517099619, 0.33927172296826863]
Evaluation on validation dataset:
Step 25, mean loss 0.025755536559132743
Step 50, mean loss 0.014427186052335448
Step 75, mean loss 0.023003446670981892
Step 100, mean loss 0.029498388790114925
Step 125, mean loss 0.032401993556426215
Step 150, mean loss 0.03867444174700771
Step 175, mean loss 0.0571532819176784
Step 200, mean loss 0.2591225747031143
Step 225, mean loss 0.199345338646608
Unrolled forward losses 1.6824342956765035
Unrolled forward base losses 2.565701273852575
Test loss: 1.3003238001649151
