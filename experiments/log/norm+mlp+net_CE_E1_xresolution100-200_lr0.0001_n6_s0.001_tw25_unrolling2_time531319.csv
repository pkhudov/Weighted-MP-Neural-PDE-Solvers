Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_norm+mlp+net_CE_E1_xresolution100-200_lr0.0001_n6_s0.001_tw25_unrolling2_time531319.tar
Number of parameters: 1035587
Epoch 0
Starting epoch 0...
=========================================================================================================
Positions transformed for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): -5.04051e-02, -1.39617e-01, 3.36624e-02, 1.08818e-01, -5.97852e-02, -9.45424e-02
Node: 01 (pos: 0.010): -5.04288e-02, -1.39818e-01, 3.33845e-02, 1.08684e-01, -5.98825e-02, -9.34613e-02
Node: 02 (pos: 0.020): -5.04584e-02, -1.40018e-01, 3.31067e-02, 1.08542e-01, -5.99820e-02, -9.23754e-02
Node: 03 (pos: 0.030): -5.04941e-02, -1.40215e-01, 3.28290e-02, 1.08393e-01, -6.00835e-02, -9.12847e-02
Node: 04 (pos: 0.040): -5.05357e-02, -1.40412e-01, 3.25515e-02, 1.08236e-01, -6.01871e-02, -9.01892e-02
Node: 05 (pos: 0.051): -5.05833e-02, -1.40606e-01, 3.22742e-02, 1.08073e-01, -6.02928e-02, -8.90888e-02
-
Node: 07 (pos: 0.071): -5.05833e-02, -1.40606e-01, 3.22742e-02, 1.08073e-01, -6.02928e-02, -8.90888e-02
Node: 08 (pos: 0.081): -5.05357e-02, -1.40412e-01, 3.25515e-02, 1.08236e-01, -6.01871e-02, -9.01892e-02
Node: 09 (pos: 0.091): -5.04941e-02, -1.40215e-01, 3.28290e-02, 1.08393e-01, -6.00835e-02, -9.12847e-02
Node: 10 (pos: 0.101): -5.04584e-02, -1.40018e-01, 3.31067e-02, 1.08542e-01, -5.99820e-02, -9.23754e-02
Node: 11 (pos: 0.111): -5.04288e-02, -1.39818e-01, 3.33845e-02, 1.08684e-01, -5.98825e-02, -9.34613e-02
Node: 12 (pos: 0.121): -5.04051e-02, -1.39617e-01, 3.36624e-02, 1.08818e-01, -5.97852e-02, -9.45424e-02

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.88133e-02, 3.42203e-09, 3.22016e-01, 7.19980e-06, 2.80358e-02, 1.31268e-04
Node: 01 (pos: 0.010): 7.86255e-02, 3.23527e-09, 3.28072e-01, 7.41378e-06, 2.77112e-02, 1.60855e-04
Node: 02 (pos: 0.020): 7.83906e-02, 3.05978e-09, 3.34187e-01, 7.64590e-06, 2.73829e-02, 1.96822e-04
Node: 03 (pos: 0.030): 7.81089e-02, 2.89481e-09, 3.40362e-01, 7.89740e-06, 2.70512e-02, 2.40475e-04
Node: 04 (pos: 0.040): 7.77810e-02, 2.73967e-09, 3.46594e-01, 8.16962e-06, 2.67162e-02, 2.93367e-04
Node: 05 (pos: 0.051): 7.74074e-02, 2.59372e-09, 3.52882e-01, 8.46403e-06, 2.63782e-02, 3.57344e-04
-
Node: 07 (pos: 0.071): 7.74074e-02, 2.59372e-09, 3.52882e-01, 8.46403e-06, 2.63782e-02, 3.57344e-04
Node: 08 (pos: 0.081): 7.77810e-02, 2.73967e-09, 3.46594e-01, 8.16962e-06, 2.67162e-02, 2.93367e-04
Node: 09 (pos: 0.091): 7.81089e-02, 2.89481e-09, 3.40362e-01, 7.89740e-06, 2.70512e-02, 2.40475e-04
Node: 10 (pos: 0.101): 7.83906e-02, 3.05978e-09, 3.34187e-01, 7.64590e-06, 2.73829e-02, 1.96822e-04
Node: 11 (pos: 0.111): 7.86255e-02, 3.23527e-09, 3.28072e-01, 7.41378e-06, 2.77112e-02, 1.60855e-04
Node: 12 (pos: 0.121): 7.88133e-02, 3.42203e-09, 3.22016e-01, 7.19980e-06, 2.80358e-02, 1.31268e-04
---------------------------------------------------------------------------------------------------------
Positions transformed for neighbours of Node 56 (for each GNN layer):

Node: 00 (pos: 0.000): -5.05833e-02, -1.40606e-01, 3.22742e-02, 1.08073e-01, -6.02928e-02, -8.90888e-02
Node: 01 (pos: 0.010): -5.05357e-02, -1.40412e-01, 3.25515e-02, 1.08236e-01, -6.01871e-02, -9.01892e-02
Node: 02 (pos: 0.020): -5.04941e-02, -1.40215e-01, 3.28290e-02, 1.08393e-01, -6.00835e-02, -9.12847e-02
Node: 03 (pos: 0.030): -5.04584e-02, -1.40018e-01, 3.31067e-02, 1.08542e-01, -5.99820e-02, -9.23754e-02
Node: 04 (pos: 0.040): -5.04288e-02, -1.39818e-01, 3.33845e-02, 1.08684e-01, -5.98825e-02, -9.34613e-02
Node: 05 (pos: 0.051): -5.04051e-02, -1.39617e-01, 3.36624e-02, 1.08818e-01, -5.97852e-02, -9.45424e-02
-
Node: 07 (pos: 0.071): -5.04288e-02, -1.39818e-01, 3.33845e-02, 1.08684e-01, -5.98825e-02, -9.34613e-02
Node: 08 (pos: 0.081): -5.04584e-02, -1.40018e-01, 3.31067e-02, 1.08542e-01, -5.99820e-02, -9.23754e-02
Node: 09 (pos: 0.091): -5.04941e-02, -1.40215e-01, 3.28290e-02, 1.08393e-01, -6.00835e-02, -9.12847e-02
Node: 10 (pos: 0.101): -5.05357e-02, -1.40412e-01, 3.25515e-02, 1.08236e-01, -6.01871e-02, -9.01892e-02
Node: 11 (pos: 0.111): -5.05833e-02, -1.40606e-01, 3.22742e-02, 1.08073e-01, -6.02928e-02, -8.90888e-02
Node: 12 (pos: 0.121): -5.04051e-02, -1.39617e-01, 3.36624e-02, 1.08818e-01, -5.97852e-02, -9.45424e-02

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 7.74074e-02, 2.59372e-09, 3.52882e-01, 8.46403e-06, 2.63782e-02, 3.57344e-04
Node: 58 (pos: 0.586): 7.77810e-02, 2.73967e-09, 3.46594e-01, 8.16962e-06, 2.67162e-02, 2.93367e-04
Node: 59 (pos: 0.596): 7.81089e-02, 2.89481e-09, 3.40362e-01, 7.89740e-06, 2.70512e-02, 2.40475e-04
Node: 60 (pos: 0.606): 7.83906e-02, 3.05978e-09, 3.34187e-01, 7.64590e-06, 2.73829e-02, 1.96822e-04
Node: 61 (pos: 0.616): 7.86255e-02, 3.23527e-09, 3.28072e-01, 7.41378e-06, 2.77112e-02, 1.60855e-04
Node: 50 (pos: 0.505): 7.88133e-02, 3.42203e-09, 3.22016e-01, 7.19980e-06, 2.80358e-02, 1.31268e-04
-
Node: 51 (pos: 0.515): 7.86255e-02, 3.23527e-09, 3.28072e-01, 7.41378e-06, 2.77112e-02, 1.60855e-04
Node: 52 (pos: 0.525): 7.83906e-02, 3.05978e-09, 3.34187e-01, 7.64590e-06, 2.73829e-02, 1.96822e-04
Node: 53 (pos: 0.535): 7.81089e-02, 2.89481e-09, 3.40362e-01, 7.89740e-06, 2.70512e-02, 2.40475e-04
Node: 54 (pos: 0.545): 7.77810e-02, 2.73967e-09, 3.46594e-01, 8.16962e-06, 2.67162e-02, 2.93367e-04
Node: 55 (pos: 0.556): 7.74074e-02, 2.59372e-09, 3.52882e-01, 8.46403e-06, 2.63782e-02, 3.57344e-04
Node: 62 (pos: 0.626): 7.88133e-02, 3.42203e-09, 3.22016e-01, 7.19980e-06, 2.80358e-02, 1.31268e-04
=========================================================================================================
Training Loss (progress: 0.00), 1.2523448117962894, -7.494843210593958e-10, 3.535543376103882e-09, 4.608452494931583e-08, -2.9184364215310632e-08, 2.582578245370821e-09, 2.500331995871763e-08, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.08), 0.2694886690687556, -2.463391976135626e-08, 9.19260333301883e-08, -2.762589908475984e-07, 1.2126518439513738e-08, -4.3901096124849144e-08, -5.605637996146637e-08, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.16), 0.20737506353757756, 1.1591792259979209e-08, 1.3757799337043162e-08, -5.1362099321434864e-08, 8.836306325468526e-08, -5.4271106304734195e-08, 3.802329802239294e-09, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.24), 0.17184228971517396, 1.1062073691030042e-08, -1.090334135920478e-07, 6.75616400519558e-07, 5.052424645217991e-06, -1.2635188942530762e-07, 1.0579367496878513e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.32), 0.15585520740829217, 3.505388314876883e-08, -4.7478164413539785e-08, 2.0778348280197744e-07, 1.8773312923212493e-06, 1.4542343255277177e-07, -1.4813870012366056e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.40), 0.13539312565078945, -2.6347798629154266e-08, -4.604010920837797e-08, 2.7022849039099886e-07, -2.526180120966517e-06, -2.0276234598548623e-07, 5.685419740142836e-08, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.48), 0.13044381420794748, 6.70452335493338e-08, 1.4729893090381947e-08, -4.142244079060702e-07, -1.5057947878093553e-06, 1.910670855938442e-07, -2.9469096274542854e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.56), 0.13043479073126032, -1.5997711632038447e-07, 6.582441677935273e-08, -1.1952817641943585e-07, -3.1762228183886362e-06, 7.85124491404014e-08, 1.3574946974326041e-06, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.64), 0.11411893184858819, -8.089574120805957e-08, 8.020673309527932e-08, -3.616854499441391e-07, -6.699914952774623e-06, 4.2275054346727005e-08, 5.145200403726144e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.72), 0.1094702562991071, -2.023683218721319e-08, -1.0691397610622551e-08, 2.411576755470976e-07, 1.255503519697478e-07, 1.5791902139318954e-07, -2.753825615877206e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.80), 0.10242545533707374, 3.377052928774827e-08, 6.6521921617636836e-09, -3.484534396894994e-09, 5.056555856616728e-06, -1.9334240871058974e-08, -2.9789908785870105e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.88), 0.10801337104095651, -3.3685551231563165e-08, 2.4881337759761493e-08, 7.661823129501736e-08, 6.253645166482791e-06, 1.9610382031397606e-07, 3.410663088615881e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Training Loss (progress: 0.96), 0.09856184052700442, 4.4173996345489547e-08, -3.811843789253342e-08, 6.523139029215534e-08, 5.7574388527138565e-06, 2.4997730955103154e-08, -4.4582407675834687e-07, [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
Evaluation on validation dataset:
