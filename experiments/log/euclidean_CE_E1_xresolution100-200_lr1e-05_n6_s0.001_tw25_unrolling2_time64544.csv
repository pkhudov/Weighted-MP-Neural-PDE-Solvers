Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar
Number of parameters: 1031657.0
Saved initial model at models/init64544.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
Node: 01 (pos: 0.010): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 02 (pos: 0.020): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 03 (pos: 0.030): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 04 (pos: 0.040): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 05 (pos: 0.051): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
-
Node: 07 (pos: 0.071): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 08 (pos: 0.081): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 09 (pos: 0.091): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 10 (pos: 0.101): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 11 (pos: 0.111): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 12 (pos: 0.121): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 58 (pos: 0.586): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 59 (pos: 0.596): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 60 (pos: 0.606): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 61 (pos: 0.616): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 50 (pos: 0.505): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
-
Node: 51 (pos: 0.515): 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02, 7.80223e-02
Node: 52 (pos: 0.525): 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01, 1.95443e-01
Node: 53 (pos: 0.535): 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01, 3.99208e-01
Node: 54 (pos: 0.545): 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01, 6.64898e-01
Node: 55 (pos: 0.556): 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01, 9.03002e-01
Node: 62 (pos: 0.626): 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02, 2.53978e-02
=========================================================================================================
Training Loss (progress: 0.00), 0.34882536962486815, [0.0020854264646084785, 0.0018855585526212717, 0.0018128666334437658, 0.001749414236149744, 0.0017811524362875383, 0.0018711649618870189], [1.0011103184892474, 1.0009990181390855, 1.0005566294447157, 1.0005806609809933, 1.000758686383756, 1.0009935379892116]
Training Loss (progress: 0.08), 0.041449228481100046, [0.0052537658184301506, 0.004548239716651359, 0.004562158272365942, 0.003933694648664764, 0.004176172305098596, 0.00489245801265335], [1.0058645110739708, 1.0066801495435602, 1.0050430383055033, 1.0055031088242996, 1.00619355580836, 1.00572125447269]
Training Loss (progress: 0.16), 0.03604902610484128, [0.006282411106285993, 0.005778517127458369, 0.005620570336698065, 0.004995610375180386, 0.005113396032462093, 0.006067608117742908], [1.0063887348281815, 1.0080278924529935, 1.0060675979124716, 1.0073096247298179, 1.0072559018989005, 1.0067029270560568]
Training Loss (progress: 0.24), 0.035397343871358074, [0.006949263545993035, 0.006489216880957835, 0.006368326212228464, 0.005744668576877133, 0.005735633316445885, 0.0069631491110318106], [1.0063871604267258, 1.0085201871846114, 1.0066307381828974, 1.0082115184056835, 1.007670232565163, 1.0071271405808957]
Training Loss (progress: 0.32), 0.035749881205402394, [0.007538403053799803, 0.007204795039745406, 0.007017453943902063, 0.006267561379753424, 0.006317538187832438, 0.007612534742388383], [1.0062678762749888, 1.0089824532692113, 1.0069236659891296, 1.0086305720691806, 1.008061539215135, 1.0074601975445598]
Training Loss (progress: 0.40), 0.03354940570222064, [0.007886114772004174, 0.007633064221440066, 0.007423743316843153, 0.0068340787357281775, 0.006665301193874534, 0.008165610971490997], [1.0061275996567436, 1.0092635057442716, 1.007081865194392, 1.0093077102631842, 1.0083575710222423, 1.0078295956698449]
Training Loss (progress: 0.48), 0.03438546333470843, [0.008223382711569374, 0.007919175066711755, 0.007759231007899952, 0.007216093630524083, 0.007024619570357008, 0.008527183710369743], [1.005980986117243, 1.0093990855006045, 1.007369579192198, 1.0097187499150702, 1.0085758848116466, 1.008115943835972]
Training Loss (progress: 0.56), 0.033982658960904404, [0.008404453478443716, 0.008253520605131676, 0.008038433534228312, 0.007481788575918902, 0.007282267248152974, 0.008789824341269674], [1.0057729093510253, 1.0096357703129628, 1.007524972727261, 1.010040128777333, 1.0087518420146675, 1.008407666149538]
Training Loss (progress: 0.64), 0.03343902674162453, [0.008587524444717123, 0.008473243588463223, 0.008277997990368081, 0.007762010962829342, 0.0074541207892441595, 0.00907153475037374], [1.0055910976644247, 1.0100258762423437, 1.0077723478270135, 1.010490932467408, 1.0089085662259831, 1.0088055165178214]
Training Loss (progress: 0.72), 0.033614307517725574, [0.0088106745927533, 0.008683741364089474, 0.008453346507044136, 0.007929763078517447, 0.007658231737319911, 0.009315730064742933], [1.0054480503227117, 1.0103600501330827, 1.0079575339257663, 1.0107215245639563, 1.0090694058495266, 1.009114180057351]
Training Loss (progress: 0.80), 0.033694091707111, [0.008999058339789463, 0.008835367315048952, 0.008602084845800065, 0.008107048734390254, 0.007862846789484491, 0.009524329665149522], [1.0052976960052258, 1.0107265953964018, 1.008203625266232, 1.0110955186367685, 1.0092807802434394, 1.009508447130388]
Training Loss (progress: 0.88), 0.03412872138767122, [0.009153886968535552, 0.008976207350714834, 0.008692993470204219, 0.008217289156069629, 0.0079911605612977, 0.009663859968368074], [1.0051150238299142, 1.0110718966460917, 1.008331610202043, 1.0113576275294573, 1.0094092705439013, 1.0098842288242553]
Training Loss (progress: 0.96), 0.033250992719589714, [0.00924302521727818, 0.009097115787284404, 0.00888637861035039, 0.008509277746239146, 0.008175915295599877, 0.009802315891145946], [1.005012658558231, 1.0113814432653707, 1.0085885516927706, 1.0118250941543876, 1.0095626628017855, 1.010216993268959]
Evaluation on validation dataset:
Step 25, mean loss 0.017719524407479367
Step 50, mean loss 0.011863286006634215
Step 75, mean loss 0.018307717735852214
Step 100, mean loss 0.01974796542916349
Step 125, mean loss 0.02673179094314029
Step 150, mean loss 0.028120951827402964
Step 175, mean loss 0.048156183279773626
Step 200, mean loss 0.18855158738711747
Step 225, mean loss 0.15016609574012574
Unrolled forward losses 1.527463772236863
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 6.73772e-01, 6.72239e-01, 6.65812e-01, 6.59161e-01, 6.46462e-01, 6.96723e-01
Node: 01 (pos: 0.010): 7.61291e-01, 7.61629e-01, 7.55891e-01, 7.51402e-01, 7.40799e-01, 7.80532e-01
Node: 02 (pos: 0.020): 8.41288e-01, 8.43540e-01, 8.38586e-01, 8.36396e-01, 8.28137e-01, 8.56550e-01
Node: 03 (pos: 0.030): 9.09277e-01, 9.13292e-01, 9.09110e-01, 9.09094e-01, 9.03126e-01, 9.20757e-01
Node: 04 (pos: 0.040): 9.61179e-01, 9.66618e-01, 9.63088e-01, 9.64860e-01, 9.60812e-01, 9.69545e-01
Node: 05 (pos: 0.051): 9.93731e-01, 1.00010e+00, 9.97000e-01, 9.99947e-01, 9.97177e-01, 1.00005e+00
-
Node: 07 (pos: 0.071): 9.93731e-01, 1.00010e+00, 9.97000e-01, 9.99947e-01, 9.97177e-01, 1.00005e+00
Node: 08 (pos: 0.081): 9.61179e-01, 9.66618e-01, 9.63088e-01, 9.64860e-01, 9.60812e-01, 9.69545e-01
Node: 09 (pos: 0.091): 9.09277e-01, 9.13292e-01, 9.09110e-01, 9.09094e-01, 9.03126e-01, 9.20757e-01
Node: 10 (pos: 0.101): 8.41288e-01, 8.43540e-01, 8.38586e-01, 8.36396e-01, 8.28137e-01, 8.56550e-01
Node: 11 (pos: 0.111): 7.61291e-01, 7.61629e-01, 7.55891e-01, 7.51402e-01, 7.40799e-01, 7.80532e-01
Node: 12 (pos: 0.121): 6.73772e-01, 6.72239e-01, 6.65812e-01, 6.59161e-01, 6.46462e-01, 6.96723e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.93731e-01, 1.00010e+00, 9.97000e-01, 9.99947e-01, 9.97177e-01, 1.00005e+00
Node: 58 (pos: 0.586): 9.61179e-01, 9.66618e-01, 9.63088e-01, 9.64860e-01, 9.60812e-01, 9.69545e-01
Node: 59 (pos: 0.596): 9.09277e-01, 9.13292e-01, 9.09110e-01, 9.09094e-01, 9.03126e-01, 9.20757e-01
Node: 60 (pos: 0.606): 8.41288e-01, 8.43540e-01, 8.38586e-01, 8.36396e-01, 8.28137e-01, 8.56550e-01
Node: 61 (pos: 0.616): 7.61291e-01, 7.61629e-01, 7.55891e-01, 7.51402e-01, 7.40799e-01, 7.80532e-01
Node: 50 (pos: 0.505): 6.73772e-01, 6.72239e-01, 6.65812e-01, 6.59161e-01, 6.46462e-01, 6.96723e-01
-
Node: 51 (pos: 0.515): 7.61291e-01, 7.61629e-01, 7.55891e-01, 7.51402e-01, 7.40799e-01, 7.80532e-01
Node: 52 (pos: 0.525): 8.41288e-01, 8.43540e-01, 8.38586e-01, 8.36396e-01, 8.28137e-01, 8.56550e-01
Node: 53 (pos: 0.535): 9.09277e-01, 9.13292e-01, 9.09110e-01, 9.09094e-01, 9.03126e-01, 9.20757e-01
Node: 54 (pos: 0.545): 9.61179e-01, 9.66618e-01, 9.63088e-01, 9.64860e-01, 9.60812e-01, 9.69545e-01
Node: 55 (pos: 0.556): 9.93731e-01, 1.00010e+00, 9.97000e-01, 9.99947e-01, 9.97177e-01, 1.00005e+00
Node: 62 (pos: 0.626): 6.73772e-01, 6.72239e-01, 6.65812e-01, 6.59161e-01, 6.46462e-01, 6.96723e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.015810189750013106
Step 50, mean loss 0.007748196013714043
Step 75, mean loss 0.013378626445681375
Step 100, mean loss 0.01757944024985842
Step 125, mean loss 0.025491240829174814
Step 150, mean loss 0.031199050350404562
Step 175, mean loss 0.04446035848897577
Step 200, mean loss 0.11233113376863232
Step 225, mean loss 0.05970727284294302
Unrolled forward losses 1.29990851108223
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.06227323746848684, [0.009252916009797964, 0.009038606873787753, 0.008878728137781573, 0.008586221091381185, 0.008254303349022813, 0.009926222610708984], [1.0048737180347196, 1.0115447803626052, 1.00858526435564, 1.0119367449742396, 1.009628083058373, 1.0104723736163246]
Training Loss (progress: 0.08), 0.06670407724239064, [0.009445774714273483, 0.009280903853037552, 0.009084812626294845, 0.008918268353726767, 0.008466542554067825, 0.010285001101482283], [1.004646181345015, 1.0118062663168266, 1.0088269148788882, 1.0124126373191846, 1.0096548772733367, 1.0108294331189736]
Training Loss (progress: 0.16), 0.06301565692642555, [0.009536984285948723, 0.009587604036888322, 0.009390854441963295, 0.009198223989505712, 0.00866359581143564, 0.010452279705852817], [1.0044279532200713, 1.0120826569051053, 1.0089557359696613, 1.01275765397147, 1.0096062122972382, 1.010987148950862]
Training Loss (progress: 0.24), 0.06569934473310998, [0.00971248169912774, 0.009691432560912957, 0.009495927470162063, 0.009501239689871564, 0.00894277634966616, 0.010747053138706381], [1.0042548914927845, 1.0123124214545218, 1.009171397441469, 1.0131947143420132, 1.0097153393591025, 1.0113918955920114]
Training Loss (progress: 0.32), 0.0623048244824581, [0.009802685659891707, 0.009768376792026745, 0.009686696100975258, 0.009631092755491687, 0.009135383272898184, 0.010941823792785431], [1.0040101649577817, 1.012582792887359, 1.0093751238859379, 1.0134158490729943, 1.0097477970783135, 1.0116613998289752]
Training Loss (progress: 0.40), 0.061901965798249095, [0.009923803794073335, 0.009937103553074747, 0.009865290338546833, 0.009791546707227951, 0.009326815679277081, 0.01106130613295886], [1.0037635902019386, 1.0128564165015932, 1.0095148891186776, 1.01368116603631, 1.00979421518255, 1.0118383339262438]
Training Loss (progress: 0.48), 0.05797272574462312, [0.010075965939744202, 0.010102181772347165, 0.009985355054608291, 0.009993781339674485, 0.009467823961985112, 0.011187240516959795], [1.0035932819194884, 1.0132510875557317, 1.0097034259170514, 1.0139347658420397, 1.0098025484203517, 1.012031185936846]
Training Loss (progress: 0.56), 0.06563291620165765, [0.010163742359662283, 0.010206489158496177, 0.010094009081761201, 0.010172614748605795, 0.009609751980641141, 0.011386458857369603], [1.0033930977120868, 1.0135540041209745, 1.0098069182477103, 1.0142082021043117, 1.0098078610899617, 1.0122782285576912]
Training Loss (progress: 0.64), 0.06047876792950376, [0.010303963113221362, 0.010264829690827247, 0.010166971530616612, 0.010343411399104809, 0.009789937550378084, 0.011541289521958467], [1.0031720857184896, 1.0138289336606634, 1.0099412567098507, 1.0144926440878306, 1.0098459482466928, 1.0125309689055932]
Training Loss (progress: 0.72), 0.05742401627250698, [0.010365526933193344, 0.010440805927645043, 0.010291263184146037, 0.010494873422139537, 0.009926494990802582, 0.01168065214057831], [1.002950155757582, 1.0141301823413282, 1.0100366984650064, 1.014769486755993, 1.0098782939029505, 1.012755011704237]
Training Loss (progress: 0.80), 0.06778620337157411, [0.010502618554782848, 0.010508266276600335, 0.010358175411084108, 0.010647544353567583, 0.010069590471708438, 0.011697050145258785], [1.0027665757827686, 1.0143413211049812, 1.0101824992748114, 1.015077657320073, 1.009928012057955, 1.0129682259121795]
Training Loss (progress: 0.88), 0.06096462459790357, [0.010594162972700855, 0.010633539217348392, 0.010425615835911466, 0.010838330916767528, 0.010185233756772787, 0.011865511100066044], [1.0025729066945632, 1.0145410088207192, 1.010341799372039, 1.01536154431478, 1.009923231769427, 1.0132496126515795]
Training Loss (progress: 0.96), 0.05856305916049374, [0.010661662619901504, 0.010646972580394041, 0.010537878067242924, 0.010994280021263394, 0.010337446182095252, 0.011998836351971229], [1.00231939653885, 1.0148231049504663, 1.0104398974831426, 1.0156512908027235, 1.0099994985568517, 1.0135035755294708]
Evaluation on validation dataset:
Step 25, mean loss 0.02004795604477573
Step 50, mean loss 0.01430933557118095
Step 75, mean loss 0.021643661630507363
Step 100, mean loss 0.023355726485104804
Step 125, mean loss 0.028574768471049193
Step 150, mean loss 0.028831826471401593
Step 175, mean loss 0.04853875909389935
Step 200, mean loss 0.21336153937799823
Step 225, mean loss 0.15319246384506724
Unrolled forward losses 1.8446484582331357
Unrolled forward base losses 2.565701273852575
Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.10480808379145888, [0.01068649765384073, 0.010632737731686043, 0.010617808411075872, 0.011024030491009392, 0.01042364915893698, 0.012030859679922866], [1.0022312880769995, 1.0148918891583016, 1.0105857731649046, 1.0157308093411195, 1.0100109632611194, 1.013554351623118]
Training Loss (progress: 0.08), 0.09546751532354107, [0.01072865045519072, 0.010757030336646014, 0.01071434177799785, 0.011141056492963717, 0.010513756284153707, 0.0121712357695827], [1.0021420864370558, 1.0150754261611263, 1.0107100235345623, 1.0159015098391317, 1.010021545192589, 1.0137736530960244]
Training Loss (progress: 0.16), 0.09921521611512857, [0.010749821361640332, 0.010876963990704864, 0.010867353315377729, 0.01125417632325148, 0.0106109125051946, 0.012293157952539104], [1.002054609765338, 1.0152691016944837, 1.0108355240779143, 1.0161185871860308, 1.0100600200761074, 1.013950324176012]
Training Loss (progress: 0.24), 0.09221427389465288, [0.010817216258042595, 0.010989976510303586, 0.010933913318019172, 0.011330778472287917, 0.010711404771558977, 0.012365583729431242], [1.0019851362580723, 1.015413672000971, 1.0109212302569721, 1.016255909637891, 1.0100948673496197, 1.0140815825817429]
Training Loss (progress: 0.32), 0.10376700035426764, [0.01083528610222369, 0.011087104058659208, 0.011057610065085354, 0.011463664435910445, 0.010788718882082248, 0.01246101308906098], [1.001861208288451, 1.0155628221704085, 1.0109953299862984, 1.0163840145677512, 1.0101170118245562, 1.0142307925331524]
Training Loss (progress: 0.40), 0.0971659975559518, [0.010870492586026253, 0.011137279510159533, 0.01115452762598153, 0.011521977691357939, 0.010860040796497496, 0.012510714701607254], [1.0017427810781012, 1.0156696592005607, 1.0110641198826351, 1.016514768762375, 1.0101016494613912, 1.0142817886190563]
Training Loss (progress: 0.48), 0.09532769126501488, [0.010962195236071507, 0.011255126858535245, 0.011161902261909367, 0.011642663428553439, 0.010940597160594966, 0.01262421715065739], [1.0017128328268308, 1.0159519730896185, 1.011162551730492, 1.016701526256305, 1.0101494293195914, 1.0144516593958681]
Training Loss (progress: 0.56), 0.09569936826221807, [0.010994869424336737, 0.011264081391346882, 0.011242672065320784, 0.011719281232286443, 0.011008727000822303, 0.012680119583145686], [1.0015892811169385, 1.0159889681065524, 1.011250548071376, 1.0168160140927336, 1.0101662471598305, 1.014508321272208]
Training Loss (progress: 0.64), 0.10025453789926393, [0.011061523652889406, 0.011312662347097387, 0.011304014240612205, 0.011813521280114165, 0.011079681642016009, 0.012764275126766856], [1.001528470056567, 1.0160891263801113, 1.0113394901709751, 1.01691946667639, 1.0102008493187937, 1.0146318284017697]
Training Loss (progress: 0.72), 0.09232282757746088, [0.011064940181401917, 0.011378957070338227, 0.011392860309363569, 0.01192281568767004, 0.01117021265432191, 0.012777854355730376], [1.0014058198329, 1.016247546801161, 1.011424317216194, 1.0171282980400944, 1.0102639759149008, 1.0147206058139853]
Training Loss (progress: 0.80), 0.09102876926706585, [0.011086906042288108, 0.011399562252945597, 0.011484379241057847, 0.012016741085668343, 0.011230043517501707, 0.012842808004800933], [1.0013064228092123, 1.0163607953826714, 1.011541499822063, 1.0173240899300073, 1.0102658224195415, 1.0148518016729111]
Training Loss (progress: 0.88), 0.09240147596298143, [0.011161685402008346, 0.01150576923618079, 0.011571509115110934, 0.012109765281105822, 0.011286734725590003, 0.01289025916689848], [1.0012707462192982, 1.016576313816938, 1.0116564329000561, 1.0173953926724646, 1.0102570304020655, 1.0149582750088249]
Training Loss (progress: 0.96), 0.09023783667034943, [0.01121969312353538, 0.011536538749264101, 0.01158933324393603, 0.01214233638937874, 0.011378977212068032, 0.012946391288365989], [1.0011805573902752, 1.0166471705403453, 1.011718205928838, 1.0175064929619264, 1.010305816754359, 1.015063476044202]
Evaluation on validation dataset:
Step 25, mean loss 0.01729160030562743
Step 50, mean loss 0.010381183965992254
Step 75, mean loss 0.01833767743013922
Step 100, mean loss 0.02091173835106737
Step 125, mean loss 0.026770106391533738
Step 150, mean loss 0.02686109430434942
Step 175, mean loss 0.04678121745108364
Step 200, mean loss 0.22424297476135083
Step 225, mean loss 0.14873775178311882
Unrolled forward losses 1.403134138839666
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.21694e-01, 7.39339e-01, 7.37898e-01, 7.52271e-01, 7.32504e-01, 7.65051e-01
Node: 01 (pos: 0.010): 7.97598e-01, 8.14909e-01, 8.12608e-01, 8.25001e-01, 8.08134e-01, 8.34103e-01
Node: 02 (pos: 0.020): 8.65603e-01, 8.82449e-01, 8.79327e-01, 8.89707e-01, 8.75787e-01, 8.95211e-01
Node: 03 (pos: 0.030): 9.22480e-01, 9.38827e-01, 9.34984e-01, 9.43523e-01, 9.32298e-01, 9.45818e-01
Node: 04 (pos: 0.040): 9.65380e-01, 9.81289e-01, 9.76884e-01, 9.83945e-01, 9.74882e-01, 9.83708e-01
Node: 05 (pos: 0.051): 9.92072e-01, 1.00768e+00, 1.00292e+00, 1.00902e+00, 1.00136e+00, 1.00717e+00
-
Node: 07 (pos: 0.071): 9.92072e-01, 1.00768e+00, 1.00292e+00, 1.00902e+00, 1.00136e+00, 1.00717e+00
Node: 08 (pos: 0.081): 9.65380e-01, 9.81289e-01, 9.76884e-01, 9.83945e-01, 9.74882e-01, 9.83708e-01
Node: 09 (pos: 0.091): 9.22480e-01, 9.38827e-01, 9.34984e-01, 9.43523e-01, 9.32298e-01, 9.45818e-01
Node: 10 (pos: 0.101): 8.65603e-01, 8.82449e-01, 8.79327e-01, 8.89707e-01, 8.75787e-01, 8.95211e-01
Node: 11 (pos: 0.111): 7.97598e-01, 8.14909e-01, 8.12608e-01, 8.25001e-01, 8.08134e-01, 8.34103e-01
Node: 12 (pos: 0.121): 7.21694e-01, 7.39339e-01, 7.37898e-01, 7.52271e-01, 7.32504e-01, 7.65051e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.92072e-01, 1.00768e+00, 1.00292e+00, 1.00902e+00, 1.00136e+00, 1.00717e+00
Node: 58 (pos: 0.586): 9.65380e-01, 9.81289e-01, 9.76884e-01, 9.83945e-01, 9.74882e-01, 9.83708e-01
Node: 59 (pos: 0.596): 9.22480e-01, 9.38827e-01, 9.34984e-01, 9.43523e-01, 9.32298e-01, 9.45818e-01
Node: 60 (pos: 0.606): 8.65603e-01, 8.82449e-01, 8.79327e-01, 8.89707e-01, 8.75787e-01, 8.95211e-01
Node: 61 (pos: 0.616): 7.97598e-01, 8.14909e-01, 8.12608e-01, 8.25001e-01, 8.08134e-01, 8.34103e-01
Node: 50 (pos: 0.505): 7.21694e-01, 7.39339e-01, 7.37898e-01, 7.52271e-01, 7.32504e-01, 7.65051e-01
-
Node: 51 (pos: 0.515): 7.97598e-01, 8.14909e-01, 8.12608e-01, 8.25001e-01, 8.08134e-01, 8.34103e-01
Node: 52 (pos: 0.525): 8.65603e-01, 8.82449e-01, 8.79327e-01, 8.89707e-01, 8.75787e-01, 8.95211e-01
Node: 53 (pos: 0.535): 9.22480e-01, 9.38827e-01, 9.34984e-01, 9.43523e-01, 9.32298e-01, 9.45818e-01
Node: 54 (pos: 0.545): 9.65380e-01, 9.81289e-01, 9.76884e-01, 9.83945e-01, 9.74882e-01, 9.83708e-01
Node: 55 (pos: 0.556): 9.92072e-01, 1.00768e+00, 1.00292e+00, 1.00902e+00, 1.00136e+00, 1.00717e+00
Node: 62 (pos: 0.626): 7.21694e-01, 7.39339e-01, 7.37898e-01, 7.52271e-01, 7.32504e-01, 7.65051e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.01405969954020192
Step 50, mean loss 0.007369705494242877
Step 75, mean loss 0.01275082264732403
Step 100, mean loss 0.015772789492378127
Step 125, mean loss 0.02227135770064593
Step 150, mean loss 0.0296328627163402
Step 175, mean loss 0.044186063897779355
Step 200, mean loss 0.08855774253279705
Step 225, mean loss 0.06102648576920959
Unrolled forward losses 1.1073199775296232
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.10353410300736506, [0.011217216554250484, 0.011546426149345469, 0.011632748374497355, 0.01217683395288652, 0.011424283050931301, 0.013009357011946318], [1.0011139637861788, 1.0166599334460824, 1.0117509032261536, 1.0175379042871717, 1.010339090747177, 1.015123553053079]
Training Loss (progress: 0.08), 0.10051890797652319, [0.011254763024291781, 0.011611179337905978, 0.01167539993203242, 0.012280645040576781, 0.011510401954054852, 0.013078423252366356], [1.0010532544344068, 1.016778511251752, 1.0118102427063338, 1.0176918577075242, 1.0103623796104735, 1.0152675858697424]
Training Loss (progress: 0.16), 0.09373787537554998, [0.01126818315564334, 0.01167731323402523, 0.011730248542946145, 0.012369799517987268, 0.011584253329866125, 0.013145649692612551], [1.000943476988384, 1.0168978345095299, 1.0118827974403493, 1.017777896511101, 1.0103970657363726, 1.015333968307916]
Training Loss (progress: 0.24), 0.10089818402852399, [0.011324034152210206, 0.011718004143245914, 0.011784937297766463, 0.012415419615490026, 0.011652856734841132, 0.01318516985464338], [1.000879084352013, 1.0170498200217941, 1.0119934426634611, 1.0178886617700946, 1.0104097053158672, 1.0154412976304552]
Training Loss (progress: 0.32), 0.09805897348910253, [0.011355645908189787, 0.011770033206832667, 0.011842743954571782, 0.01249112033205154, 0.011736597982505526, 0.01328245023340415], [1.0007909944398894, 1.0171679918121828, 1.0120839918471582, 1.0179745747867759, 1.0104404332975114, 1.0155239161675982]
Training Loss (progress: 0.40), 0.10479247727666696, [0.011407509255564252, 0.011810902845778896, 0.011911543200180987, 0.012560312900639923, 0.011767645296051147, 0.01333335575150783], [1.0007643238288244, 1.017246636652116, 1.0121873198255054, 1.0180813990873785, 1.0104517149441115, 1.0156373203369795]
Training Loss (progress: 0.48), 0.09709504403209923, [0.011433713133686064, 0.011842342746054788, 0.011958716494782291, 0.012700210409601514, 0.011843099178681973, 0.013412104752156396], [1.0006600256355318, 1.0174117432141971, 1.0122739575626667, 1.01834815894473, 1.0104742244372251, 1.0157656655556782]
Training Loss (progress: 0.56), 0.1026001419309402, [0.011451520642995833, 0.011914596702681557, 0.012022510238725675, 0.012754481332677455, 0.011908158455652539, 0.013442422444044814], [1.000536511063455, 1.0175697199273042, 1.0123224894271154, 1.0184765975172654, 1.0105012279054635, 1.0158136652134413]
Training Loss (progress: 0.64), 0.09775217167249929, [0.011472627603749203, 0.011970390164404241, 0.012078972086851162, 0.0128095050338197, 0.011938611735357324, 0.013519650673711046], [1.0004468664623745, 1.0176841716796081, 1.012380514180148, 1.0186021018700264, 1.0104940731093814, 1.0159420786548585]
Training Loss (progress: 0.72), 0.09603967578107013, [0.011528231666388422, 0.011992164486297807, 0.01217775361307273, 0.012868168312087822, 0.012023491733084812, 0.01354768688437072], [1.0003931037536753, 1.0178035135754337, 1.0124841694819153, 1.0186898563941973, 1.0105255901477181, 1.0160350260952071]
Training Loss (progress: 0.80), 0.09747121409246257, [0.011577638834170708, 0.012076975580250995, 0.012223583204249483, 0.012957759086747647, 0.012085868640196723, 0.013567662224302768], [1.0003234935349086, 1.0179710555753962, 1.012602438056149, 1.018844422057327, 1.0105453139171097, 1.0160991108498842]
Training Loss (progress: 0.88), 0.09681346223350337, [0.011588611920760402, 0.01211321265284501, 0.012270538458213028, 0.012988281601951927, 0.012145367972853318, 0.013650328383120044], [1.000189587787907, 1.018046164139069, 1.0126370575650818, 1.0189238738122255, 1.0105688771864962, 1.0162473282216158]
Training Loss (progress: 0.96), 0.09526487582531679, [0.01161281172748709, 0.012109813269722913, 0.012352048664855413, 0.01305991752233257, 0.01220213071744239, 0.013705939563270425], [1.0001035420202513, 1.018122450313248, 1.0126847229765579, 1.0190327680090532, 1.010577978005562, 1.0163494081082656]
Evaluation on validation dataset:
Step 25, mean loss 0.017477138479427497
Step 50, mean loss 0.010235311779811983
Step 75, mean loss 0.01765108246676548
Step 100, mean loss 0.020394284591704073
Step 125, mean loss 0.0263666376476052
Step 150, mean loss 0.02698213082041653
Step 175, mean loss 0.047076621408389
Step 200, mean loss 0.22262912278202224
Step 225, mean loss 0.15263073990720777
Unrolled forward losses 1.3961928185050556
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.30082e-01, 7.52289e-01, 7.51824e-01, 7.69902e-01, 7.48349e-01, 7.77139e-01
Node: 01 (pos: 0.010): 8.03773e-01, 8.25184e-01, 8.23460e-01, 8.38785e-01, 8.20294e-01, 8.43550e-01
Node: 02 (pos: 0.020): 8.69566e-01, 8.90048e-01, 8.87120e-01, 8.99703e-01, 8.84275e-01, 9.02085e-01
Node: 03 (pos: 0.030): 9.24439e-01, 9.44004e-01, 9.40017e-01, 9.50126e-01, 9.37468e-01, 9.50407e-01
Node: 04 (pos: 0.040): 9.65741e-01, 9.84534e-01, 9.79720e-01, 9.87864e-01, 9.77411e-01, 9.86500e-01
Node: 05 (pos: 0.051): 9.91404e-01, 1.00968e+00, 1.00434e+00, 1.01122e+00, 1.00219e+00, 1.00881e+00
-
Node: 07 (pos: 0.071): 9.91404e-01, 1.00968e+00, 1.00434e+00, 1.01122e+00, 1.00219e+00, 1.00881e+00
Node: 08 (pos: 0.081): 9.65741e-01, 9.84534e-01, 9.79720e-01, 9.87864e-01, 9.77411e-01, 9.86500e-01
Node: 09 (pos: 0.091): 9.24439e-01, 9.44004e-01, 9.40017e-01, 9.50126e-01, 9.37468e-01, 9.50407e-01
Node: 10 (pos: 0.101): 8.69566e-01, 8.90048e-01, 8.87120e-01, 8.99703e-01, 8.84275e-01, 9.02085e-01
Node: 11 (pos: 0.111): 8.03773e-01, 8.25184e-01, 8.23460e-01, 8.38785e-01, 8.20294e-01, 8.43550e-01
Node: 12 (pos: 0.121): 7.30082e-01, 7.52289e-01, 7.51824e-01, 7.69902e-01, 7.48349e-01, 7.77139e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.91404e-01, 1.00968e+00, 1.00434e+00, 1.01122e+00, 1.00219e+00, 1.00881e+00
Node: 58 (pos: 0.586): 9.65741e-01, 9.84534e-01, 9.79720e-01, 9.87864e-01, 9.77411e-01, 9.86500e-01
Node: 59 (pos: 0.596): 9.24439e-01, 9.44004e-01, 9.40017e-01, 9.50126e-01, 9.37468e-01, 9.50407e-01
Node: 60 (pos: 0.606): 8.69566e-01, 8.90048e-01, 8.87120e-01, 8.99703e-01, 8.84275e-01, 9.02085e-01
Node: 61 (pos: 0.616): 8.03773e-01, 8.25184e-01, 8.23460e-01, 8.38785e-01, 8.20294e-01, 8.43550e-01
Node: 50 (pos: 0.505): 7.30082e-01, 7.52289e-01, 7.51824e-01, 7.69902e-01, 7.48349e-01, 7.77139e-01
-
Node: 51 (pos: 0.515): 8.03773e-01, 8.25184e-01, 8.23460e-01, 8.38785e-01, 8.20294e-01, 8.43550e-01
Node: 52 (pos: 0.525): 8.69566e-01, 8.90048e-01, 8.87120e-01, 8.99703e-01, 8.84275e-01, 9.02085e-01
Node: 53 (pos: 0.535): 9.24439e-01, 9.44004e-01, 9.40017e-01, 9.50126e-01, 9.37468e-01, 9.50407e-01
Node: 54 (pos: 0.545): 9.65741e-01, 9.84534e-01, 9.79720e-01, 9.87864e-01, 9.77411e-01, 9.86500e-01
Node: 55 (pos: 0.556): 9.91404e-01, 1.00968e+00, 1.00434e+00, 1.01122e+00, 1.00219e+00, 1.00881e+00
Node: 62 (pos: 0.626): 7.30082e-01, 7.52289e-01, 7.51824e-01, 7.69902e-01, 7.48349e-01, 7.77139e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.014586782083768565
Step 50, mean loss 0.007843558669907023
Step 75, mean loss 0.013015427985362413
Step 100, mean loss 0.016070259593669905
Step 125, mean loss 0.022249603960291305
Step 150, mean loss 0.029640076034300723
Step 175, mean loss 0.04614197551198987
Step 200, mean loss 0.08746608822156951
Step 225, mean loss 0.05961208758499678
Unrolled forward losses 1.1320608627607083
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.09056914327413179, [0.011669321883681038, 0.012127089633966224, 0.012334141182758298, 0.013110191591242104, 0.012229816385880577, 0.013720570853323967], [1.0000962808066374, 1.018186980159152, 1.0126715971921962, 1.0191234489272705, 1.010589178775185, 1.016405376466342]
Training Loss (progress: 0.08), 0.09280795527253519, [0.011679413472434178, 0.012164354357022759, 0.012332796746764881, 0.013108275739127833, 0.01228386268070569, 0.013748352891254959], [0.9999658317539318, 1.0183102745599517, 1.0127459748745309, 1.0191519095143773, 1.0105949994499142, 1.016476856174603]
Training Loss (progress: 0.16), 0.09691486223392198, [0.011723478174722494, 0.012190037560513282, 0.012344879122300343, 0.013228443241571977, 0.012378428945343146, 0.013797718565544714], [0.9999023708049238, 1.0183675331574202, 1.0127808535065308, 1.0193492404959563, 1.0106446134016758, 1.0165853098532023]
Training Loss (progress: 0.24), 0.09647544719234195, [0.011749576682241535, 0.012231009732631502, 0.012467308467897679, 0.01326463005773858, 0.012414645878316762, 0.013846858983494196], [0.9998414534564568, 1.0185557015137967, 1.0129717553210227, 1.0194701149192704, 1.0106787092310219, 1.0167028932936275]
Training Loss (progress: 0.32), 0.09635397760418811, [0.011805828591258023, 0.01228603434441888, 0.012456578191741148, 0.013312290296503543, 0.012485379512162165, 0.013878641288250181], [0.9997620332093006, 1.0187234773717049, 1.0130221919812799, 1.0195462482352116, 1.0106872219358791, 1.0167391573997924]
Training Loss (progress: 0.40), 0.09504615767996857, [0.011793568716947337, 0.012292055189497247, 0.012537395288188027, 0.013408218780003343, 0.012559105358652258, 0.013950882094892838], [0.9996538980336216, 1.0188205878779035, 1.0131505191325405, 1.019678262034773, 1.010722680727002, 1.0168597485832893]
Training Loss (progress: 0.48), 0.09708483318922591, [0.011830222815967755, 0.01233648576484424, 0.012558938861256192, 0.013490352607735945, 0.012603818689966316, 0.01395307270749595], [0.9995908155247814, 1.0189369158354502, 1.0131784332042646, 1.0197684237420221, 1.0107351574435404, 1.0169227131906502]
Training Loss (progress: 0.56), 0.10242127217063272, [0.011893914206337682, 0.012343413005120495, 0.012615954412033748, 0.013567501806178498, 0.012667159710202668, 0.01401953810243248], [0.9995049211842448, 1.019030842590255, 1.0132530635560166, 1.0198664009343883, 1.0107686318183717, 1.0170379828785807]
Training Loss (progress: 0.64), 0.0912985816378122, [0.011903426357634419, 0.012377314462202537, 0.012665642771817809, 0.01364765053715478, 0.012714592897176039, 0.014005789207337306], [0.999421495885574, 1.0191830519663643, 1.0133462062018321, 1.0200611679644616, 1.010781203223587, 1.0171083464839257]
Training Loss (progress: 0.72), 0.09518334264619079, [0.011934006192334692, 0.012405245857431111, 0.012735321897755718, 0.01364888195350408, 0.012757256301057661, 0.01408428324108363], [0.9993378295989473, 1.0192956892218137, 1.0134081333460252, 1.020091323807459, 1.0107893040448965, 1.0172218065312781]
Training Loss (progress: 0.80), 0.09919810839589246, [0.011965171227684446, 0.01250424092914463, 0.012808912920767917, 0.013761106170963301, 0.012819520922051218, 0.014087611512016384], [0.9992967634373207, 1.0194987516134986, 1.013502160610507, 1.0202337885140402, 1.0108281928203071, 1.0172453478855075]
Training Loss (progress: 0.88), 0.0994088332489246, [0.011969607874886304, 0.012517881257349215, 0.012854980350384444, 0.01381000259612021, 0.012890013080979234, 0.014152337349870078], [0.999185699334945, 1.0195732409034535, 1.013567256434832, 1.020334667386839, 1.01087316431501, 1.0173771489904502]
Training Loss (progress: 0.96), 0.10003108794457573, [0.012060761360350086, 0.012546603022216787, 0.012891437920849935, 0.013871658464584852, 0.012932779581779179, 0.014146946524378172], [0.9991365816765636, 1.019633761693209, 1.013648533271156, 1.0204591216139123, 1.0109043061264753, 1.0174196225161594]
Evaluation on validation dataset:
Step 25, mean loss 0.01718456681248654
Step 50, mean loss 0.010236784752739213
Step 75, mean loss 0.0181032276451173
Step 100, mean loss 0.019938273407603832
Step 125, mean loss 0.026781883075558913
Step 150, mean loss 0.02694573573098657
Step 175, mean loss 0.047954469563325205
Step 200, mean loss 0.21414035651026803
Step 225, mean loss 0.1588229657546833
Unrolled forward losses 1.428392422619535
Unrolled forward base losses 2.565701273852575
Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.09504462922151087, [0.012062796846289149, 0.01251220504724225, 0.012867992646180349, 0.013847644470553871, 0.012954423047275253, 0.014185070368400873], [0.9990741172772954, 1.0196266927812179, 1.0136341877858417, 1.0204921402379277, 1.0109082068199784, 1.0174981557950222]
Training Loss (progress: 0.08), 0.0950426828814091, [0.012072967673520462, 0.012574696150536059, 0.012900407012082683, 0.013909128511479579, 0.012986130979952796, 0.014204882753355332], [0.9990296404246397, 1.019730843781218, 1.0136698428306967, 1.0205573896231033, 1.0109321068474215, 1.017535907925592]
Training Loss (progress: 0.16), 0.08919298985445369, [0.012078430447941164, 0.01257190546496414, 0.012916106210077936, 0.013952519277564007, 0.013015228363565808, 0.014253214159503544], [0.9990043720377226, 1.0197794686126918, 1.0137375933731563, 1.0206418417622483, 1.0109513331155504, 1.017605224788507]
Training Loss (progress: 0.24), 0.0924133504167038, [0.01209611464146496, 0.012583054474459288, 0.01293365405937752, 0.013974884511320864, 0.013044493073336444, 0.014257590905889886], [0.998978994313583, 1.0198188946622186, 1.0137928157830958, 1.0206940558248645, 1.0109668187149758, 1.017647177505321]
Training Loss (progress: 0.32), 0.08757624063610317, [0.012129165173257177, 0.012597131502057917, 0.01293935996023069, 0.014000776984103508, 0.013048951209046035, 0.014270933016738175], [0.9989484561204259, 1.0198718897001748, 1.0138388352558414, 1.020772593787847, 1.0109772326873774, 1.017706841442448]
Training Loss (progress: 0.40), 0.0988222269747814, [0.012144413036008795, 0.012607948838199382, 0.012957125240008867, 0.014014470996021102, 0.01306842537023405, 0.014305645754651507], [0.9989081621565891, 1.0199487868539434, 1.013854272357647, 1.020808936739234, 1.0109834210159008, 1.0177682592313537]
Training Loss (progress: 0.48), 0.09974903687449256, [0.012160711738267108, 0.012600641916434403, 0.012969388926515436, 0.014039093364440801, 0.01309734058244147, 0.014308542913900173], [0.998898072987998, 1.019982930147783, 1.0138699837137621, 1.0208710905131033, 1.0110195478429542, 1.017794817211267]
Training Loss (progress: 0.56), 0.09366794046927654, [0.012178397158202481, 0.012607388679764561, 0.012984032098330971, 0.014045790504294334, 0.013121862233555515, 0.01432853115554897], [0.998867986211935, 1.0200172017637794, 1.0139168479416605, 1.0209297847809828, 1.011018839518556, 1.0178189077802244]
Training Loss (progress: 0.64), 0.08504331174623685, [0.012178728011959074, 0.012622697471309827, 0.013013474227054626, 0.014109211848764323, 0.013150124246222517, 0.014347552286967455], [0.998809746289104, 1.0200682177737708, 1.0139830336497524, 1.0210277507358785, 1.011048182153641, 1.0178892957958383]
Training Loss (progress: 0.72), 0.08359092322339232, [0.012182011367688605, 0.012642837057431753, 0.013012819974058213, 0.014131963283629895, 0.013164081957341166, 0.014375859192411201], [0.9987894249106708, 1.020113579422249, 1.0140016367845168, 1.0210771166964026, 1.0110659479311055, 1.0179547417939205]
Training Loss (progress: 0.80), 0.08438727701370122, [0.012210465708996811, 0.01265396959275107, 0.013009539853384627, 0.014156707219147507, 0.013192256071868325, 0.014410065058732576], [0.9987730072330615, 1.020189534883586, 1.0140320450130234, 1.0211249584821216, 1.0110875171904605, 1.0180253213587194]
Training Loss (progress: 0.88), 0.0838307441365453, [0.012229970290466671, 0.01267164088493726, 0.013041461283974138, 0.014189520856897089, 0.01321786120588451, 0.014421735211701777], [0.9987567918147578, 1.020278385423792, 1.014084327903056, 1.02118037232459, 1.0111079725485907, 1.0180544988087243]
Training Loss (progress: 0.96), 0.08245529370829328, [0.012232283641771187, 0.012695244387055637, 0.013072448466476242, 0.01421164872010274, 0.013231065065221015, 0.014429102992381905], [0.9987174487689969, 1.0203130521933166, 1.0141393444994495, 1.0212163972339972, 1.0111059435755945, 1.0180967904225708]
Evaluation on validation dataset:
Step 25, mean loss 0.018331606703084674
Step 50, mean loss 0.010703181801071184
Step 75, mean loss 0.017960733243030977
Step 100, mean loss 0.020093457682564035
Step 125, mean loss 0.026898881332526166
Step 150, mean loss 0.027838379982550697
Step 175, mean loss 0.0469059098353283
Step 200, mean loss 0.2221674547499507
Step 225, mean loss 0.1510562909746809
Unrolled forward losses 1.4273621888034518
Unrolled forward base losses 2.565701273852575
Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.09305376237024368, [0.012228383218269152, 0.012704267217400795, 0.013096464214985537, 0.014235756634251404, 0.013255594396152332, 0.014475833061564119], [0.9986981185333041, 1.020339386461723, 1.014149359753096, 1.0212471408449686, 1.0111220496990123, 1.0181407696994542]
Training Loss (progress: 0.08), 0.08613229333398538, [0.012242688655554564, 0.012713941134445562, 0.013097166068517402, 0.014260065479302622, 0.013281317281596093, 0.014454488367100386], [0.9986855492270615, 1.0203900656979583, 1.0142052882330603, 1.0213152482407686, 1.0111371707806305, 1.0181543354616909]
Training Loss (progress: 0.16), 0.09116759670954827, [0.012265914889365665, 0.012717645991301563, 0.013083518385524441, 0.014292014769666525, 0.013307079375981837, 0.014461212527668759], [0.9986525025088372, 1.02043278356235, 1.014221229624215, 1.0213705556545305, 1.0111513464391664, 1.0181765844578405]
Training Loss (progress: 0.24), 0.09993037888419488, [0.012276419401598336, 0.01271714277786339, 0.013105580149995731, 0.01430205859942381, 0.013324417443101326, 0.014468133937272882], [0.9986310321539889, 1.0204974810354897, 1.0142697538729497, 1.0213966538438044, 1.0111672968972862, 1.0182253522958926]
Training Loss (progress: 0.32), 0.0980565401387334, [0.012287810000574174, 0.01272352437026188, 0.013139251097595587, 0.014323149953122144, 0.01334749040689821, 0.01450215644176602], [0.9985907386453577, 1.0205272837372723, 1.0143253175163656, 1.0214344087453084, 1.0111757728796646, 1.0182820469284755]
Training Loss (progress: 0.40), 0.09558582171125489, [0.012310364339201468, 0.012751329439398594, 0.013158891235498025, 0.014352404894248027, 0.013379996260295611, 0.014504630130507977], [0.9985693988292055, 1.0205667034537584, 1.014361939690571, 1.02147691910136, 1.0112111754677298, 1.0183590203663162]
Training Loss (progress: 0.48), 0.09705899834846327, [0.012311774703101888, 0.012749925617333041, 0.013164059937050639, 0.014397741213772014, 0.013383172579569466, 0.014517665145978804], [0.998531275719927, 1.0206125695943142, 1.0143875531598392, 1.0215722833448517, 1.0112127842513683, 1.0183823650155963]
Training Loss (progress: 0.56), 0.0987240092446133, [0.01230400120468464, 0.01275668622448496, 0.01317622281497439, 0.014393080207553317, 0.013413627594513453, 0.014538608017362415], [0.9984708090370294, 1.02066013767486, 1.0144437678573501, 1.0215959506771137, 1.011228475254556, 1.018420011834379]
Training Loss (progress: 0.64), 0.09726278569598514, [0.012326754432466874, 0.012769557706059054, 0.013160594556936625, 0.014418601823312857, 0.013436788436145116, 0.014571006174136538], [0.9984599012199814, 1.0207485694648735, 1.0144396264263826, 1.0216501333747583, 1.0112372656403168, 1.0185061866510923]
Training Loss (progress: 0.72), 0.08681168949511273, [0.012340320232221062, 0.012801765779625843, 0.013205232059892961, 0.01444284031850964, 0.013457765129957187, 0.014585649501680176], [0.9984281069219978, 1.0208197702793902, 1.0144937201717723, 1.0216827803142172, 1.0112599202100787, 1.0185317202659905]
Training Loss (progress: 0.80), 0.10079429388028009, [0.012328890901372428, 0.012797187943827663, 0.013235177688431814, 0.014496525403437916, 0.013485006689939516, 0.014604848637587653], [0.9983868665942434, 1.0208276228103808, 1.0145177198428068, 1.0217673600696633, 1.011272504460481, 1.0185841283103922]
Training Loss (progress: 0.88), 0.09110566221360071, [0.01233506904989131, 0.01281007422792061, 0.0132619471412822, 0.014512340511574353, 0.013506456367669074, 0.01460679377578355], [0.9983454723874763, 1.0208995546572932, 1.0145802500298788, 1.021817587238162, 1.0112854623912784, 1.018620737601179]
Training Loss (progress: 0.96), 0.08792323790987863, [0.01234783753046497, 0.012815142438509243, 0.013273331092043246, 0.014545457247321694, 0.013531000748465633, 0.014630844236456742], [0.9983142654191068, 1.0209398581158924, 1.0146413408439212, 1.0218718747507411, 1.0113109926077268, 1.0186735643549123]
Evaluation on validation dataset:
Step 25, mean loss 0.017279457724097065
Step 50, mean loss 0.010201879340284974
Step 75, mean loss 0.018070100040854194
Step 100, mean loss 0.019774071571157625
Step 125, mean loss 0.026186360308100072
Step 150, mean loss 0.027180521964374458
Step 175, mean loss 0.04685381812741192
Step 200, mean loss 0.21863440060559042
Step 225, mean loss 0.15419733834024385
Unrolled forward losses 1.4084142620195421
Unrolled forward base losses 2.565701273852575
Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.0870879218362237, [0.01235649032243477, 0.012838802490950449, 0.013266879538612282, 0.01453764098260598, 0.013549412743324434, 0.014628780181485895], [0.9982936893381191, 1.0209944854822586, 1.0146354585858057, 1.0218516447633648, 1.011322535169349, 1.0186952326182446]
Training Loss (progress: 0.08), 0.0983604736101352, [0.012390898848923082, 0.012847338702384772, 0.013306394582876333, 0.014582437091646258, 0.013552131076080796, 0.01465793443164138], [0.9982921293320649, 1.0210102289673342, 1.0146941294569014, 1.02193524880288, 1.011328195381409, 1.0187545088076448]
Training Loss (progress: 0.16), 0.0897019652894972, [0.012389509664009802, 0.012830295278693156, 0.013299124583619999, 0.014609776221091681, 0.013566150128220942, 0.014658682728551389], [0.9982610956792867, 1.0210761353650135, 1.0147133998714752, 1.0219711029594725, 1.0113344985696588, 1.0187625790913952]
Training Loss (progress: 0.24), 0.08248946275771862, [0.012399662606066463, 0.012858338817686861, 0.013318415120627635, 0.014636518599138632, 0.01360959869449411, 0.014699433639061203], [0.9982335229900042, 1.02114612231357, 1.014745586067706, 1.022018618582206, 1.0113613140096476, 1.0188212671531118]
Training Loss (progress: 0.32), 0.08533394231974714, [0.012409103900938767, 0.01287812114922107, 0.013334783164113535, 0.014638468990742714, 0.013626743250403186, 0.014707110487189432], [0.9982072444703578, 1.021195478854721, 1.014794211407506, 1.0220669394378217, 1.0113793982398123, 1.0188765516236011]
Training Loss (progress: 0.40), 0.10113403915196881, [0.012405559794903745, 0.012887363017295218, 0.013382402879158647, 0.014686584926550018, 0.013650003122323355, 0.014719436206482283], [0.9981661278997884, 1.0212343837116322, 1.0148499006870793, 1.0221241999906088, 1.0113930573964607, 1.0189105367676095]
Training Loss (progress: 0.48), 0.08521362757922743, [0.012426409929026579, 0.012929043056020472, 0.01339717320828347, 0.014717128251771024, 0.013662896423219108, 0.0147383621699456], [0.998143520241703, 1.02128959940361, 1.0148710659370004, 1.0222032675488586, 1.0113950296732104, 1.0189560897399388]
Training Loss (progress: 0.56), 0.0977644103167081, [0.012446442369696067, 0.012920108770816603, 0.013399981191153895, 0.014736100025772638, 0.013708698451753529, 0.014774608789706292], [0.9981114314175039, 1.0213277803136973, 1.014913744766886, 1.0222627293928508, 1.011420152853935, 1.0190151999941053]
Training Loss (progress: 0.64), 0.09037093439730044, [0.012459912580585103, 0.012919938480788118, 0.01340004180978432, 0.014755249720397315, 0.0137355214759597, 0.014783474436510699], [0.9980924456269137, 1.0213399841501662, 1.0149419107474216, 1.0223035543433387, 1.0114389168829192, 1.0190463168804562]
Training Loss (progress: 0.72), 0.09079603878830064, [0.012469218983493211, 0.012935720947521854, 0.013436098096454428, 0.014761154238014008, 0.01372660746809045, 0.014783268597063451], [0.9980470734228023, 1.0214474467157997, 1.0150062251993661, 1.0223634146531067, 1.0114334077990097, 1.0190893545086301]
Training Loss (progress: 0.80), 0.08680122550097093, [0.012481618642306543, 0.012925849471037039, 0.01344051259834338, 0.014786896934307419, 0.01376977681135586, 0.014801643841131754], [0.9980228747469839, 1.0214848589199925, 1.01503216828008, 1.022396431350243, 1.011466720810296, 1.0191398145843025]
Training Loss (progress: 0.88), 0.08539935470774937, [0.01249759263349861, 0.012945965982322262, 0.013458474272135944, 0.014819521625105561, 0.013777434202249684, 0.014806907737414594], [0.9979960370808704, 1.0215447233139938, 1.0150720302424237, 1.0224285499808365, 1.011455618401602, 1.0191762247502192]
Training Loss (progress: 0.96), 0.08832527923508864, [0.012496143539019958, 0.012939896969048476, 0.01347094978640839, 0.014856932247256177, 0.013788623730736076, 0.014844155120009642], [0.9979580114968611, 1.0215744205360864, 1.0151080241833594, 1.0224797351293329, 1.011472089341167, 1.0192508754923115]
Evaluation on validation dataset:
Step 25, mean loss 0.017103902220482792
Step 50, mean loss 0.010504431870185783
Step 75, mean loss 0.01817155844810197
Step 100, mean loss 0.019800885471585122
Step 125, mean loss 0.02617815075745457
Step 150, mean loss 0.027360429387181213
Step 175, mean loss 0.04739090129623501
Step 200, mean loss 0.21955828380381665
Step 225, mean loss 0.15320691423910077
Unrolled forward losses 1.413135540164882
Unrolled forward base losses 2.565701273852575
Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.08788740550296543, [0.012504535019098468, 0.01293552398825182, 0.013461764648851014, 0.014867704499886588, 0.01381394816628335, 0.014854344345039905], [0.9979573343580468, 1.0216024106185848, 1.0151323365849787, 1.0225010427319086, 1.0114960764337801, 1.0192705377874935]
Training Loss (progress: 0.08), 0.10064541806231461, [0.012523270891814544, 0.012966779137038172, 0.013487241906169737, 0.014912441393296057, 0.01382638243328363, 0.01484043768699865], [0.997938810828018, 1.021633877626138, 1.0151798025364722, 1.022588795414224, 1.0114976387677654, 1.0192827465940433]
Training Loss (progress: 0.16), 0.08946298198388669, [0.01252184294983325, 0.012983314981593639, 0.01348838791213603, 0.014910481561890098, 0.013854552316520281, 0.014846929143519481], [0.9978978351298728, 1.0217279470357177, 1.015213426875712, 1.022642605339118, 1.0115124897962218, 1.0193499644447843]
Training Loss (progress: 0.24), 0.08918139686404317, [0.012530321760522555, 0.013016541021465128, 0.013506146561603776, 0.01494944730885906, 0.013875119015123633, 0.014874579389354868], [0.997862447596276, 1.0217942237824602, 1.0152372799967386, 1.022715404225529, 1.0115331925876745, 1.0193979641352557]
Training Loss (progress: 0.32), 0.08745435394601944, [0.012542301073021834, 0.013012944058042247, 0.013534255871377897, 0.015005447313434056, 0.0139096363883898, 0.014909128606394033], [0.9978437769823377, 1.0217965370451543, 1.0152557809413636, 1.0227998328320265, 1.0115753307706254, 1.0194272661815826]
Training Loss (progress: 0.40), 0.09704347228734239, [0.012562133260899166, 0.01302400666499416, 0.013573915101229041, 0.015026284711219645, 0.013929819296073719, 0.01492870842468468], [0.9978205778358872, 1.021865224095689, 1.0153437197843442, 1.0228383509269092, 1.0115835740901262, 1.0194908543489725]
Training Loss (progress: 0.48), 0.09408184872160551, [0.01257966496479359, 0.013035989697661489, 0.013585853968123732, 0.015039936759185272, 0.013932337251182626, 0.014925537448757551], [0.9977903692025766, 1.0219187372975018, 1.0153660173993937, 1.0228837156994348, 1.011593037668755, 1.0195068642554297]
Training Loss (progress: 0.56), 0.09000234217985238, [0.012580520395375291, 0.01302416364535407, 0.013602826986814306, 0.015052390320036368, 0.013962493490623415, 0.014961708747745677], [0.997751640588398, 1.0219298135914563, 1.0153936557826229, 1.022941619013359, 1.011596417493986, 1.0195733878306388]
Training Loss (progress: 0.64), 0.09244322824828033, [0.012577888034086526, 0.013037986003001958, 0.013614256550579048, 0.015088387236206782, 0.013987385002090397, 0.014960698595020194], [0.9976998243068089, 1.0219965306645633, 1.0154412367798145, 1.0229795978740421, 1.0116152358909363, 1.019616285850566]
Training Loss (progress: 0.72), 0.09448026010646846, [0.012583676400897997, 0.013056874864729673, 0.013606059430507347, 0.015096714860900518, 0.01401003119640079, 0.01498889216699852], [0.9976684031229084, 1.0220356146432632, 1.0154610813891405, 1.0230239562802637, 1.011625406867484, 1.019682179268468]
Training Loss (progress: 0.80), 0.09423922112899329, [0.012605566314454145, 0.013063311393565402, 0.01360491204628997, 0.015112992916165061, 0.01402787304297177, 0.014987475835427873], [0.9976544879393328, 1.022088406959929, 1.0154862525627095, 1.0230614649594456, 1.011639997096466, 1.0197136960084443]
Training Loss (progress: 0.88), 0.08981125584440947, [0.012632705527611875, 0.013069008536922269, 0.013610331564504315, 0.015117074417252995, 0.014044902172351523, 0.015008582194634273], [0.9976333082272993, 1.0221221454743312, 1.0155300510023717, 1.0231014097369464, 1.011656835899899, 1.0198005864465587]
Training Loss (progress: 0.96), 0.09408766775378463, [0.012642289766084534, 0.013097447768160868, 0.013658858504194299, 0.015148033978270754, 0.014037644563215986, 0.01501225859744633], [0.9976136766064071, 1.0221895114959532, 1.0155813896674797, 1.023169729254966, 1.011636709893508, 1.0198240090171695]
Evaluation on validation dataset:
Step 25, mean loss 0.01743022483893344
Step 50, mean loss 0.010214668785893008
Step 75, mean loss 0.017863070317172776
Step 100, mean loss 0.020096410889805717
Step 125, mean loss 0.026538898063038482
Step 150, mean loss 0.02748859004604587
Step 175, mean loss 0.04656235050103178
Step 200, mean loss 0.21683927855233975
Step 225, mean loss 0.15257976629067962
Unrolled forward losses 1.412607022068268
Unrolled forward base losses 2.565701273852575
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.09828877636542627, [0.012653131810017504, 0.013106334900964555, 0.013656661524226702, 0.015154155316533403, 0.014056079323626337, 0.015006695572982343], [0.9976056904364093, 1.0222096313607485, 1.0155879307240692, 1.0231910949098026, 1.0116640170833906, 1.019848273242974]
Training Loss (progress: 0.08), 0.08806365827092859, [0.012669649716006204, 0.013110731083854877, 0.013629771196843093, 0.015152366450975709, 0.014068517046870574, 0.015024351307641423], [0.9975658026338415, 1.0222715760815553, 1.0156025188194473, 1.0232046263905632, 1.0116697678293243, 1.0198971598815894]
Training Loss (progress: 0.16), 0.09357524966830559, [0.012679361927828248, 0.01312725408608906, 0.013685770264252328, 0.015213786285954153, 0.014087176071527673, 0.015034919844665328], [0.997535596142405, 1.022318946739348, 1.015667156654682, 1.0232989907224312, 1.011688215549153, 1.0199250803868243]
Training Loss (progress: 0.24), 0.09430032327815357, [0.012674720511920722, 0.013135906421175556, 0.013699342127176531, 0.015227927162132561, 0.014131057658045114, 0.015032905064591243], [0.9974936401025362, 1.0223369883832218, 1.0157129797537567, 1.0233255005174473, 1.0117190531194988, 1.019966529462696]
Training Loss (progress: 0.32), 0.08386224078731949, [0.01269933257158128, 0.013141369908183955, 0.013725149890453776, 0.015250510138844212, 0.01415460948617883, 0.015058526661531464], [0.9974811899413143, 1.0223817322780193, 1.0157747898735503, 1.0233827299643385, 1.0117397733995301, 1.0200117812595118]
Training Loss (progress: 0.40), 0.08706621278477324, [0.012707869617405455, 0.013159944156113169, 0.013729485097661384, 0.015267562741178357, 0.014179546157922924, 0.015044561411741882], [0.9974580629135139, 1.0224645504467436, 1.0158059551413607, 1.0234497720269708, 1.011751824712974, 1.020042509013639]
Training Loss (progress: 0.48), 0.09996030300049222, [0.012695290841959188, 0.013182112229564932, 0.013760377192026838, 0.015306256718981458, 0.014190463215644728, 0.015080088470561536], [0.9974072367568539, 1.022523119395753, 1.0158509420196449, 1.023523536641026, 1.0117530451891978, 1.0200971311847866]
Training Loss (progress: 0.56), 0.09112343346988663, [0.012712019330193607, 0.01316559965519334, 0.013726885170008286, 0.015335027354969725, 0.014198977874316286, 0.015096005379253551], [0.9973847213166002, 1.022546322085475, 1.0158566816121501, 1.023600106448061, 1.0117674359755053, 1.0201396000824363]
Training Loss (progress: 0.64), 0.09111477275320377, [0.012724093277789, 0.013188220866101451, 0.013724307478231796, 0.015353238221612981, 0.014237301093843947, 0.015097185027083631], [0.9973559534629515, 1.0226225840628655, 1.0159039685502256, 1.0236281161669953, 1.0117766781443303, 1.0201886046654152]
Training Loss (progress: 0.72), 0.08417114721996824, [0.012745499036365153, 0.013191142899626447, 0.01372861780332379, 0.015335145986440549, 0.014246562700790761, 0.015097902213752776], [0.9973322420154085, 1.0226436044899998, 1.0159384873958028, 1.0236282058454835, 1.0117825714761144, 1.020243218669157]
Training Loss (progress: 0.80), 0.09346467570869936, [0.012747331835465671, 0.013199706317007351, 0.013744368675522334, 0.01537542437407146, 0.014259024252871259, 0.01510558822744831], [0.9972909125713967, 1.0226869310923472, 1.015988719719333, 1.0237049292911686, 1.0118088000621106, 1.0202861774786633]
Training Loss (progress: 0.88), 0.08644562344228102, [0.012757042015769396, 0.013208619038509915, 0.013770865909307971, 0.01538061224456539, 0.014296095855814688, 0.015110302777053263], [0.9972543806447118, 1.0227514136240465, 1.0160433362088632, 1.0237440858105917, 1.0118355218225956, 1.0203288590397472]
Training Loss (progress: 0.96), 0.10145328420324523, [0.012761685262645737, 0.01321765560932351, 0.013794692918738078, 0.015409847140815737, 0.014336338319220012, 0.015141409444888647], [0.9972218382773745, 1.0228189109916987, 1.0160766209430205, 1.0238041985007296, 1.011854511083079, 1.0203697934229563]
Evaluation on validation dataset:
Step 25, mean loss 0.01673478741118583
Step 50, mean loss 0.010119595046867371
Step 75, mean loss 0.017797966658715962
Step 100, mean loss 0.02004952240557277
Step 125, mean loss 0.0265692966541951
Step 150, mean loss 0.02769761754838007
Step 175, mean loss 0.04715670142006234
Step 200, mean loss 0.2082529845081022
Step 225, mean loss 0.15462030030072818
Unrolled forward losses 1.4157575602611958
Unrolled forward base losses 2.565701273852575
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.09117232323350219, [0.012755711541689613, 0.013228236084677214, 0.013815031616939385, 0.015417816461322387, 0.014351204031403653, 0.015149624263877158], [0.997205497322841, 1.022846300080303, 1.016090728755381, 1.0238064124325497, 1.0118586178432984, 1.0204007242428739]
Training Loss (progress: 0.08), 0.09573667213697488, [0.012766004585026594, 0.013232992736022828, 0.013815128077720826, 0.01540682473233244, 0.014343443676625552, 0.015140964788820191], [0.9971979686633105, 1.022885888948389, 1.0161094922811498, 1.0238232676762988, 1.0118472992226082, 1.0203984809017506]
Training Loss (progress: 0.16), 0.08350455576904327, [0.01276057930950745, 0.013223955646872678, 0.013827236833172217, 0.01542648511229273, 0.014358336273896093, 0.015166571071907725], [0.9971792079450853, 1.0228778077698153, 1.0161334436748612, 1.0238419448078238, 1.0118614328864062, 1.0204354320874978]
Training Loss (progress: 0.24), 0.08970783428031721, [0.012770139642696212, 0.013233757909895368, 0.013832065895182032, 0.015444442182961593, 0.014365821249505726, 0.015165366306536606], [0.9971719098617677, 1.0228987156748344, 1.0161492369010876, 1.0238620832122078, 1.0118670209611849, 1.0204381954122506]
Training Loss (progress: 0.32), 0.09868106194038365, [0.012780983751042918, 0.013236022372276222, 0.013829960033468687, 0.015453792851919484, 0.014369917723558716, 0.015179715466046306], [0.9971704055600108, 1.0229298426051123, 1.0161573925759693, 1.0238984815054994, 1.0118776246375527, 1.0204666350981544]
Training Loss (progress: 0.40), 0.09935997163975076, [0.012776984446301742, 0.013239181069508115, 0.013844274018044379, 0.01546415903386911, 0.014379638608468191, 0.015177476053162357], [0.997153193678027, 1.0229534910927724, 1.01616737010596, 1.0239159621242115, 1.0118936989442768, 1.0204833633030717]
Training Loss (progress: 0.48), 0.08605549112894098, [0.01278250774083393, 0.013234025946416067, 0.013831280647176456, 0.015470937414649381, 0.014385093615353813, 0.015197927197172374], [0.9971413562612536, 1.0229759312900888, 1.0161764627230436, 1.0239338367281203, 1.0118965790668812, 1.0205152982689352]
Training Loss (progress: 0.56), 0.09156589880032526, [0.012794683915372427, 0.013234606955753528, 0.013844067569233743, 0.015482294755106879, 0.014387779638553592, 0.01520502081131674], [0.9971355380764054, 1.0229977966226855, 1.0161974072240347, 1.0239482521605332, 1.0119017520749751, 1.0205358336496997]
Training Loss (progress: 0.64), 0.08685793716644918, [0.012800378633526288, 0.013232617868657157, 0.013850067024524846, 0.015483931905540949, 0.014397125943724234, 0.015209980354317944], [0.9971207377140032, 1.0230241714653492, 1.0162313135391594, 1.023975649578294, 1.0119029278517506, 1.0205582495705612]
Training Loss (progress: 0.72), 0.0873353969671258, [0.01280156113558774, 0.013244791143274382, 0.01385285428241758, 0.015512021755986285, 0.014406160833808176, 0.015223182702945598], [0.9971103914595085, 1.023045484691844, 1.0162345101543364, 1.0240074132958328, 1.011916487733369, 1.0205899811240147]
Training Loss (progress: 0.80), 0.08445549224768338, [0.012802909239886891, 0.013241968083556339, 0.013879060776408837, 0.015512514452049597, 0.014413087046103999, 0.015224799268622672], [0.997095924553778, 1.0230590953964445, 1.016266193863505, 1.0240094101682593, 1.0119280185146533, 1.0206014842411761]
Training Loss (progress: 0.88), 0.08469385210809788, [0.012812364919998775, 0.013253145165599335, 0.01386024136560984, 0.015521118827787235, 0.01441954734736169, 0.015228161777684079], [0.9970877380356121, 1.0230845869802005, 1.0162772021085411, 1.0240434161961696, 1.0119426507936737, 1.0206243884048862]
Training Loss (progress: 0.96), 0.0919241735790248, [0.012818249120086495, 0.013272398419026497, 0.013862716801916287, 0.015524997974241755, 0.014427230571149492, 0.015221266348955115], [0.99707772895386, 1.02311557149315, 1.0162945999278936, 1.02405261761355, 1.0119460657909798, 1.0206334827025292]
Evaluation on validation dataset:
Step 25, mean loss 0.017004079778095493
Step 50, mean loss 0.009991034778009536
Step 75, mean loss 0.017767957708644637
Step 100, mean loss 0.019501402497266673
Step 125, mean loss 0.025936822419495326
Step 150, mean loss 0.027280524859634017
Step 175, mean loss 0.04731816829657916
Step 200, mean loss 0.21429348235024812
Step 225, mean loss 0.1531240578763449
Unrolled forward losses 1.4235299428227113
Unrolled forward base losses 2.565701273852575
Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.08531668644246243, [0.01282039545489005, 0.013261474959557379, 0.013863640418723388, 0.015542043265967316, 0.01443604702970192, 0.01522427295257548], [0.9970697810269878, 1.0231136179019045, 1.0163071457335688, 1.0240864371463405, 1.011948679970778, 1.0206434494759247]
Training Loss (progress: 0.08), 0.08827860652620018, [0.012816068808091973, 0.013273086995146482, 0.01387568938130518, 0.015544325292358952, 0.01444842342781638, 0.015232705911153463], [0.9970543659515645, 1.0231407751009836, 1.0163229648815453, 1.0240862871721987, 1.0119597336809785, 1.0206635039913996]
Training Loss (progress: 0.16), 0.09799037552626481, [0.012820624011604699, 0.013270017516263598, 0.013871323315553168, 0.015558208175453498, 0.014451675079822283, 0.015232533753680201], [0.9970486395137245, 1.0231558591727157, 1.0163386272263724, 1.0241097332713516, 1.0119698857826358, 1.0206764545027072]
Training Loss (progress: 0.24), 0.08088205257639364, [0.012824819015374168, 0.013275172275205853, 0.013879012879427234, 0.015564513779845164, 0.014462899421356472, 0.01524744291185155], [0.9970348796372558, 1.0231810373289254, 1.0163615478090613, 1.024133386997132, 1.0119826670141059, 1.020710528214292]
Training Loss (progress: 0.32), 0.09030169313026402, [0.012827476364368501, 0.013284169673799725, 0.013889381193097265, 0.01558056774570574, 0.01447180384669338, 0.015252142472070514], [0.9970233032522926, 1.0232082905313418, 1.0163831199031808, 1.0241553668607646, 1.0119858932268486, 1.0207221174802705]
Training Loss (progress: 0.40), 0.08863090886050415, [0.012844175028648113, 0.01329522943494537, 0.013900752586188793, 0.015602048375716531, 0.014477082915968735, 0.015248510407364423], [0.9970248752372185, 1.023250315726117, 1.0163981553238979, 1.0242078297342319, 1.011995133758189, 1.0207415848659163]
Training Loss (progress: 0.48), 0.08371854447784506, [0.012841449656457527, 0.013303885873605073, 0.013906375760095785, 0.015612313646040087, 0.014494551190815046, 0.01526748608487373], [0.9970022708618259, 1.023257597049688, 1.0164134765080315, 1.0242171397548527, 1.0120144362260515, 1.0207788379088556]
Training Loss (progress: 0.56), 0.09411283976605558, [0.012844840111198767, 0.013304336661137636, 0.013920424559548528, 0.01561407078129382, 0.014502770903353949, 0.015281096416907457], [0.9969935643106957, 1.0232830480593735, 1.01643217473776, 1.0242234908320125, 1.0120145034481733, 1.0208042346818944]
Training Loss (progress: 0.64), 0.0872876483039132, [0.012841604379565453, 0.013308954818698588, 0.013925400240992076, 0.015628727252150518, 0.014503753893606452, 0.01528112023135693], [0.99697268610417, 1.023311061603398, 1.016450291178765, 1.024261133050707, 1.0120259373996183, 1.020832493048106]
Training Loss (progress: 0.72), 0.09554205611157006, [0.01285266941493587, 0.013297615848138863, 0.013920621383623252, 0.01563481613280524, 0.014504875811167887, 0.015275766180796652], [0.996965785751237, 1.0233159317196807, 1.0164588200824753, 1.0242969660213117, 1.0120282355026973, 1.0208488285547532]
Training Loss (progress: 0.80), 0.08873206224663631, [0.012859371350263937, 0.013305790179072009, 0.013929443838974811, 0.015637194969888617, 0.014519042000524233, 0.01527944249166307], [0.9969593645179665, 1.0233345207465587, 1.0164671074160747, 1.0243114017615274, 1.0120308511377796, 1.0208650634275667]
Training Loss (progress: 0.88), 0.10228151080054418, [0.012847707951150112, 0.013299988454482888, 0.013922827109627483, 0.01565384357427334, 0.01453037249333357, 0.015290601626738495], [0.9969338529699309, 1.0233486987753688, 1.0164765320686064, 1.0243308939322797, 1.0120433989165596, 1.0208860709473015]
Training Loss (progress: 0.96), 0.08930215168147783, [0.012872942407989597, 0.013312502999516335, 0.01393417258018217, 0.015654494213024804, 0.014536000415674372, 0.015292147558868222], [0.9969423176638509, 1.0233789664180986, 1.016505792387584, 1.0243514545305414, 1.012050492684448, 1.020896525264333]
Evaluation on validation dataset:
Step 25, mean loss 0.016794926692037228
Step 50, mean loss 0.009949217785032487
Step 75, mean loss 0.017890381479824368
Step 100, mean loss 0.019746318918639053
Step 125, mean loss 0.026139629614899074
Step 150, mean loss 0.026865878729933207
Step 175, mean loss 0.0469128673513215
Step 200, mean loss 0.21867737758626987
Step 225, mean loss 0.15469522326402765
Unrolled forward losses 1.426834154893726
Unrolled forward base losses 2.565701273852575
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.08610620823486743, [0.012870386901653557, 0.013314920799335798, 0.013934923820827635, 0.01565987056882722, 0.014534014961980656, 0.015290601261803384], [0.9969275958859045, 1.0233839995596823, 1.0165147522011049, 1.0243639037704453, 1.0120454002771953, 1.0209064251097508]
Training Loss (progress: 0.08), 0.09474435362952431, [0.012871237207178128, 0.01331423954738477, 0.013936766279610132, 0.015671205843889497, 0.01454514926520412, 0.01530061222171343], [0.9969068609511773, 1.023392433938552, 1.0165226363293953, 1.0243896479967574, 1.0120565002862865, 1.020937461809694]
Training Loss (progress: 0.16), 0.08815048790810695, [0.012877213478435437, 0.013317272966834088, 0.013939803040880563, 0.01567681699827039, 0.014568304904368208, 0.015303596430696535], [0.9968950842859244, 1.0233985886449768, 1.0165335124478292, 1.0243911304383686, 1.0120759284230016, 1.0209595697204616]
Training Loss (progress: 0.24), 0.08800914960065544, [0.012881120404762641, 0.013333840745674123, 0.013954452473195008, 0.01569271424869388, 0.014567442967829088, 0.015307912192705013], [0.9968905872389797, 1.0234481566487101, 1.0165634052902979, 1.0244249763006539, 1.0120857715517038, 1.0209813541571688]
Training Loss (progress: 0.32), 0.09169710420928684, [0.012879920396856304, 0.013334565519717387, 0.013948946069750827, 0.015701415198237777, 0.014568319669114822, 0.015319001417053871], [0.9968820191157967, 1.023470355402196, 1.0165591714473958, 1.0244467031282272, 1.0120793177405534, 1.020996929435446]
Training Loss (progress: 0.40), 0.08418776149447481, [0.012895066519370009, 0.013335001057553017, 0.01396680556628153, 0.01571731541725753, 0.014582301641529703, 0.015322233081889331], [0.9968774084258171, 1.0234882238950662, 1.0165919449847103, 1.0244663356687724, 1.0120937667902277, 1.0210274312177878]
Training Loss (progress: 0.48), 0.08943359952849068, [0.012900616789657543, 0.013329314766700488, 0.01396272560146, 0.015708583814454315, 0.014577774058241863, 0.015324009791062734], [0.9968651106600561, 1.0234967682031846, 1.0166057856313024, 1.0244887154755697, 1.0120947354728134, 1.0210449412003872]
Training Loss (progress: 0.56), 0.09569223820848723, [0.012895035069750347, 0.01333680641581363, 0.013973156766488581, 0.015719053619019888, 0.014590144311452146, 0.015319042050267075], [0.9968485629967268, 1.023530765267098, 1.0166292269119495, 1.0245016049443094, 1.0121108238175116, 1.0210540894673372]
Training Loss (progress: 0.64), 0.08938050039926444, [0.012906778590881046, 0.013344961177147096, 0.013990107213532171, 0.01573316162849537, 0.014601616869655963, 0.015335419720366278], [0.9968411095273467, 1.0235499617492627, 1.0166562457856017, 1.0245345088857802, 1.012123482086677, 1.021079561456394]
Training Loss (progress: 0.72), 0.08716787722527498, [0.012906259297133871, 0.013349433662970001, 0.013984894719252954, 0.015732833941744507, 0.014606891701972952, 0.015340527020712941], [0.9968241163554772, 1.0235675547989285, 1.016665696434924, 1.0245551755479836, 1.0121294425913576, 1.0211075334654989]
Training Loss (progress: 0.80), 0.09504356256185113, [0.012912928609483447, 0.0133631836295816, 0.013995461727693139, 0.01573028567964266, 0.014609488997116357, 0.015346573613095925], [0.9968149967905438, 1.0236010154938386, 1.0166803291087858, 1.0245656190102372, 1.0121291565731063, 1.0211242741094348]
Training Loss (progress: 0.88), 0.09352146113131922, [0.012923359934380206, 0.013366477671288757, 0.014001740702611494, 0.01574469182578885, 0.014633283349116863, 0.01535554128825375], [0.9968111714029645, 1.0236315067555177, 1.016689700144965, 1.024579691963079, 1.0121527673482063, 1.0211445908393526]
Training Loss (progress: 0.96), 0.09009193588193608, [0.012932130186217838, 0.013366210768243575, 0.01399920254630255, 0.015759838892966827, 0.014632812624653943, 0.015363918831938678], [0.9968063849141732, 1.0236525981125428, 1.0167066742493305, 1.0246232411498364, 1.0121543983300114, 1.0211655039258838]
Evaluation on validation dataset:
Step 25, mean loss 0.01660734768491826
Step 50, mean loss 0.009847188224888753
Step 75, mean loss 0.017716344719561794
Step 100, mean loss 0.019778943173810706
Step 125, mean loss 0.02593511076758905
Step 150, mean loss 0.026971875613969395
Step 175, mean loss 0.046657616604637214
Step 200, mean loss 0.21970107948445325
Step 225, mean loss 0.15365304130114665
Unrolled forward losses 1.4184243672561465
Unrolled forward base losses 2.565701273852575
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.09642839864789106, [0.012931298838945147, 0.01335661281893534, 0.01399902664649, 0.01577042310396326, 0.014640670342024556, 0.015366207147614927], [0.9967976915576278, 1.0236435953805076, 1.0167061133849962, 1.0246380032646436, 1.0121548568570435, 1.0211731127645198]
Training Loss (progress: 0.08), 0.09008004573613548, [0.012925921177570899, 0.013367305882862963, 0.014006409608659327, 0.01578191065898544, 0.014648127647654535, 0.015376313408614735], [0.9967750380686581, 1.0236709594724842, 1.0167248539499911, 1.024653155237219, 1.0121623925596699, 1.021207162869889]
Training Loss (progress: 0.16), 0.08811635877497011, [0.01292572381957765, 0.0133689009586303, 0.014012243348543861, 0.015781323891032734, 0.01465744559919364, 0.015367778936240093], [0.9967558783334045, 1.0236758147327782, 1.016731654463819, 1.024660148471417, 1.0121704587863225, 1.0212077597392797]
Training Loss (progress: 0.24), 0.0850662299853148, [0.012938885635865632, 0.013375777766326586, 0.01401976798782263, 0.015792349692637098, 0.014658386671364453, 0.01538135430859435], [0.9967555137345967, 1.023704211957366, 1.0167563634818066, 1.0246938996984547, 1.0121767606362662, 1.0212450178363133]
Training Loss (progress: 0.32), 0.09880149545040484, [0.012934465318150382, 0.013381305428429021, 0.014041340932809005, 0.015804430383680405, 0.014677944293251005, 0.015389366470384067], [0.9967434117663685, 1.0237346400133613, 1.0167948960639366, 1.0247062693950533, 1.012185539815617, 1.0212621070261598]
Training Loss (progress: 0.40), 0.09009326249318926, [0.012946279295343649, 0.013384169235275925, 0.014033698780520994, 0.015818818072150163, 0.014687567150708358, 0.015389358740256941], [0.9967396799469666, 1.0237417779141031, 1.016798812278152, 1.0247309390110646, 1.0121892512409756, 1.0212689914088728]
Training Loss (progress: 0.48), 0.08732735098811059, [0.01294998729036123, 0.013371938528897216, 0.014036283476440526, 0.015832724544259637, 0.014691018427689283, 0.015402076628252749], [0.99673139127932, 1.0237563139358203, 1.0168074165787857, 1.024758818203012, 1.0121963207748021, 1.0212906400901398]
Training Loss (progress: 0.56), 0.0837359669470909, [0.01295015065394608, 0.013384986279990537, 0.01402385135054801, 0.015833258294337977, 0.014708893304288902, 0.015404150206091378], [0.9967092437726873, 1.0237762587806005, 1.016817014936842, 1.0247690454963978, 1.0122096131532266, 1.0213196583711988]
Training Loss (progress: 0.64), 0.08282292918947497, [0.012959680128703983, 0.013392762485842447, 0.014033455841566367, 0.015850767032956185, 0.014705703315985886, 0.015411592100384224], [0.9967077865864407, 1.023817274673172, 1.0168410310882325, 1.0248035189258062, 1.0122084603639847, 1.0213317323641171]
Training Loss (progress: 0.72), 0.0978275888831605, [0.012958019703771608, 0.013400027440161435, 0.014054226812908074, 0.01586153445927806, 0.014722420816108332, 0.01543092961350838], [0.9966958262222537, 1.0238368004880427, 1.0168641258874516, 1.0248318013310613, 1.0122233803822362, 1.021359313664471]
Training Loss (progress: 0.80), 0.092814602722026, [0.012971532817259223, 0.013393203726452167, 0.01405277401957263, 0.01587199472721636, 0.014725318852786919, 0.015428867837980226], [0.9966885300481584, 1.0238341072268982, 1.0168801072814975, 1.0248536992275585, 1.0122337843465925, 1.0213878301780244]
Training Loss (progress: 0.88), 0.0825719025713533, [0.012969744734560616, 0.013402453530283511, 0.014038877131181033, 0.015879147307503613, 0.014718863505989116, 0.015423963841963666], [0.9966716746507142, 1.0238598224453255, 1.016882362664324, 1.0248820268226884, 1.0122222610730476, 1.0213999540664667]
Training Loss (progress: 0.96), 0.09038995317534297, [0.012975306008770539, 0.013418275189407275, 0.014056860403270966, 0.015893830270149505, 0.014732485332018026, 0.015429239915698622], [0.9966613394867229, 1.023903498722242, 1.0169036436432684, 1.0249052905125433, 1.0122360902300485, 1.0214215268123361]
Evaluation on validation dataset:
Step 25, mean loss 0.01679266173689242
Step 50, mean loss 0.01015614318343883
Step 75, mean loss 0.01772263394738361
Step 100, mean loss 0.01987790327311396
Step 125, mean loss 0.026229751970607992
Step 150, mean loss 0.02742547412641134
Step 175, mean loss 0.04781536121004935
Step 200, mean loss 0.21510965442498722
Step 225, mean loss 0.15606073275546486
Unrolled forward losses 1.4239240745052497
Unrolled forward base losses 2.565701273852575
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00), 0.09162566348587933, [0.012981285135911612, 0.013417575921365025, 0.01406448543374277, 0.015910948031228837, 0.014735070350801575, 0.015431152951128946], [0.99666137801192, 1.0239172655124185, 1.0169187452036934, 1.0249275227459542, 1.0122338739026517, 1.021435904375429]
Training Loss (progress: 0.08), 0.08632462938592664, [0.012988074430594021, 0.013429380432849759, 0.014061198656567922, 0.01589784787615919, 0.01474564403621661, 0.015428857056365328], [0.9966520861431011, 1.0239344076171346, 1.0169267179377577, 1.0249354107184752, 1.0122501981550687, 1.0214448656682718]
Training Loss (progress: 0.16), 0.08658458481160707, [0.012986873512153616, 0.01343154436739272, 0.014093706815138362, 0.01590588214062075, 0.014749754147451154, 0.0154307942438641], [0.9966425368292797, 1.0239646547205783, 1.0169617569458667, 1.0249501811471817, 1.0122565594346324, 1.0214569760999324]
Training Loss (progress: 0.24), 0.09162456653224633, [0.012989702651878886, 0.013439201467952841, 0.014095432446002329, 0.015920327871215223, 0.014762151871885561, 0.015440049490177923], [0.996630372954933, 1.0239841521271138, 1.0169826794517935, 1.024989913301716, 1.0122725916359478, 1.0214830866080735]
Training Loss (progress: 0.32), 0.09269644109323785, [0.012992216811108672, 0.013423716279518019, 0.01409828004061398, 0.01594009801619072, 0.014773505093497053, 0.015444388585046855], [0.9966169751025494, 1.0239872708480877, 1.0169887815889707, 1.0250040367049431, 1.0122779908495283, 1.021501749074739]
Training Loss (progress: 0.40), 0.08955667377217559, [0.012989875945856868, 0.013420505994831283, 0.014099105095248534, 0.015943662676635446, 0.014787474321579045, 0.015459190538472337], [0.9966019468658078, 1.0239916733624854, 1.0170007902657923, 1.0250123089405108, 1.0122798565233475, 1.021523519914113]
Training Loss (progress: 0.48), 0.08663100370270255, [0.012987352783920398, 0.013429799479315793, 0.014096767453497121, 0.015951464675881627, 0.014787258788042434, 0.015461903830936619], [0.9965878282205923, 1.024030488521561, 1.0170143131342315, 1.0250395691341385, 1.0122720261137248, 1.0215420235061408]
Training Loss (progress: 0.56), 0.09119357043482015, [0.01298707768766591, 0.013433917696971982, 0.01411229470213234, 0.015967516102914168, 0.014790243768953453, 0.015469282867680718], [0.9965704298989819, 1.0240495607723086, 1.017038965559021, 1.0250594661621517, 1.0122772613938325, 1.0215621612766939]
Training Loss (progress: 0.64), 0.0853494258616431, [0.012995613715689032, 0.013440299193169247, 0.014108897149196904, 0.015970509632933828, 0.014802020876926886, 0.015465490898679844], [0.996557983038211, 1.0240674345454581, 1.0170386206025865, 1.0250735897120307, 1.0122858712176268, 1.0215751940053017]
Training Loss (progress: 0.72), 0.08918959587402091, [0.01299746382256959, 0.01344293658758632, 0.014120851402438463, 0.015990495757011725, 0.014817322966765575, 0.01547962738489686], [0.996550596059263, 1.024092818411796, 1.0170520187595051, 1.0251058397547939, 1.0123013746513008, 1.0216012368090823]
Training Loss (progress: 0.80), 0.08954634197753847, [0.013011626911698618, 0.01343949350339526, 0.014111078574231042, 0.01598113300932709, 0.014820362180293789, 0.015483303724644449], [0.9965449930592399, 1.0241258072895296, 1.0170602188830546, 1.025128974525966, 1.012302902378436, 1.0216248432818156]
Training Loss (progress: 0.88), 0.09327399511318445, [0.013009282936128288, 0.013447220153011521, 0.014115716805581074, 0.01599607898863499, 0.014826907683538186, 0.015479205691515603], [0.9965261733877653, 1.024153272287576, 1.0170738643964188, 1.025164531308888, 1.012314636708343, 1.0216343440061098]
Training Loss (progress: 0.96), 0.09332445007341182, [0.013013973587683353, 0.013443636840039113, 0.01412641369537995, 0.01601169809609739, 0.014836639684790574, 0.015490843874131403], [0.9965184309012648, 1.0241819634230327, 1.0171014785991461, 1.0251836853973741, 1.0123194443719934, 1.0216567339267144]
Evaluation on validation dataset:
Step 25, mean loss 0.01645336750573017
Step 50, mean loss 0.00995571621292684
Step 75, mean loss 0.017578000390072176
Step 100, mean loss 0.019820001543545455
Step 125, mean loss 0.0257628190650843
Step 150, mean loss 0.02698815180087134
Step 175, mean loss 0.04716110348175769
Step 200, mean loss 0.21913618691225728
Step 225, mean loss 0.15161999563204098
Unrolled forward losses 1.3959267351428468
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.51418e-01, 7.79326e-01, 7.84309e-01, 8.14966e-01, 7.90253e-01, 8.05994e-01
Node: 01 (pos: 0.010): 8.19110e-01, 8.47182e-01, 8.49137e-01, 8.74164e-01, 8.52371e-01, 8.66557e-01
Node: 02 (pos: 0.020): 8.79007e-01, 9.07072e-01, 9.06145e-01, 9.25784e-01, 9.06810e-01, 9.19479e-01
Node: 03 (pos: 0.030): 9.28605e-01, 9.56566e-01, 9.53118e-01, 9.68031e-01, 9.51544e-01, 9.62865e-01
Node: 04 (pos: 0.040): 9.65737e-01, 9.93563e-01, 9.88153e-01, 9.99382e-01, 9.84842e-01, 9.95102e-01
Node: 05 (pos: 0.051): 9.88725e-01, 1.01645e+00, 1.00979e+00, 1.01868e+00, 1.00538e+00, 1.01496e+00
-
Node: 07 (pos: 0.071): 9.88725e-01, 1.01645e+00, 1.00979e+00, 1.01868e+00, 1.00538e+00, 1.01496e+00
Node: 08 (pos: 0.081): 9.65737e-01, 9.93563e-01, 9.88153e-01, 9.99382e-01, 9.84842e-01, 9.95102e-01
Node: 09 (pos: 0.091): 9.28605e-01, 9.56566e-01, 9.53118e-01, 9.68031e-01, 9.51544e-01, 9.62865e-01
Node: 10 (pos: 0.101): 8.79007e-01, 9.07072e-01, 9.06145e-01, 9.25784e-01, 9.06810e-01, 9.19479e-01
Node: 11 (pos: 0.111): 8.19110e-01, 8.47182e-01, 8.49137e-01, 8.74164e-01, 8.52371e-01, 8.66557e-01
Node: 12 (pos: 0.121): 7.51418e-01, 7.79326e-01, 7.84309e-01, 8.14966e-01, 7.90253e-01, 8.05994e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.88725e-01, 1.01645e+00, 1.00979e+00, 1.01868e+00, 1.00538e+00, 1.01496e+00
Node: 58 (pos: 0.586): 9.65737e-01, 9.93563e-01, 9.88153e-01, 9.99382e-01, 9.84842e-01, 9.95102e-01
Node: 59 (pos: 0.596): 9.28605e-01, 9.56566e-01, 9.53118e-01, 9.68031e-01, 9.51544e-01, 9.62865e-01
Node: 60 (pos: 0.606): 8.79007e-01, 9.07072e-01, 9.06145e-01, 9.25784e-01, 9.06810e-01, 9.19479e-01
Node: 61 (pos: 0.616): 8.19110e-01, 8.47182e-01, 8.49137e-01, 8.74164e-01, 8.52371e-01, 8.66557e-01
Node: 50 (pos: 0.505): 7.51418e-01, 7.79326e-01, 7.84309e-01, 8.14966e-01, 7.90253e-01, 8.05994e-01
-
Node: 51 (pos: 0.515): 8.19110e-01, 8.47182e-01, 8.49137e-01, 8.74164e-01, 8.52371e-01, 8.66557e-01
Node: 52 (pos: 0.525): 8.79007e-01, 9.07072e-01, 9.06145e-01, 9.25784e-01, 9.06810e-01, 9.19479e-01
Node: 53 (pos: 0.535): 9.28605e-01, 9.56566e-01, 9.53118e-01, 9.68031e-01, 9.51544e-01, 9.62865e-01
Node: 54 (pos: 0.545): 9.65737e-01, 9.93563e-01, 9.88153e-01, 9.99382e-01, 9.84842e-01, 9.95102e-01
Node: 55 (pos: 0.556): 9.88725e-01, 1.01645e+00, 1.00979e+00, 1.01868e+00, 1.00538e+00, 1.01496e+00
Node: 62 (pos: 0.626): 7.51418e-01, 7.79326e-01, 7.84309e-01, 8.14966e-01, 7.90253e-01, 8.05994e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.013748356007105726
Step 50, mean loss 0.007198017648143564
Step 75, mean loss 0.012726983534051214
Step 100, mean loss 0.015501011772496324
Step 125, mean loss 0.021373843535763947
Step 150, mean loss 0.029967088065578216
Step 175, mean loss 0.045871431075783495
Step 200, mean loss 0.0849659551337838
Step 225, mean loss 0.061567859673360044
Unrolled forward losses 1.1101118770866925
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar

Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00), 0.09698886966733407, [0.013009482953361861, 0.01344412629239967, 0.014134200381886989, 0.016009278735676806, 0.014834752867635722, 0.015493306236299865], [0.9965068437714044, 1.0241906117248376, 1.017108224383457, 1.0251972721688347, 1.012318523107727, 1.0216714620309622]
Training Loss (progress: 0.08), 0.09116896197407619, [0.013011795052252516, 0.01344585010626568, 0.014134744012034088, 0.016004574221298307, 0.014845187890231955, 0.015497682486626875], [0.9965023434897649, 1.0242015135302922, 1.0171194825560492, 1.0251964873910546, 1.0123273077877175, 1.021684633626836]
Training Loss (progress: 0.16), 0.08787847143638991, [0.013015124875589885, 0.013452305980355236, 0.014138653959309388, 0.016007448361613406, 0.014853235763775154, 0.015502561372226484], [0.9964992916623172, 1.0242098879872334, 1.0171322934419262, 1.025207273413833, 1.0123353998166074, 1.0216975765667056]
Training Loss (progress: 0.24), 0.09316399512727465, [0.013012830287073316, 0.013452935769870706, 0.014137597503883655, 0.01601273107660899, 0.014853285169603986, 0.015502980067332578], [0.9964925124761812, 1.024210564717565, 1.017132511850094, 1.0252087373542922, 1.0123355050595886, 1.0217022315429511]
Training Loss (progress: 0.32), 0.09549430320846046, [0.013012219478913128, 0.013451701721818492, 0.01413380087929855, 0.016015409004847575, 0.014857329790869626, 0.015506768870786463], [0.9964864752698741, 1.0242186917599085, 1.017138434927756, 1.0252197379833978, 1.012338125079538, 1.0217092683406828]
Training Loss (progress: 0.40), 0.09226940524315688, [0.01300950778566431, 0.01345161942949006, 0.014135342040288627, 0.016019200029526334, 0.01485755613446741, 0.015506624963847837], [0.9964777869601056, 1.0242116383808677, 1.0171427291027133, 1.025223587303621, 1.012336143219355, 1.021718099795843]
Training Loss (progress: 0.48), 0.08839732255793183, [0.013015927403055117, 0.01345629703514515, 0.014135100708731464, 0.01602512022877097, 0.01486253666489913, 0.015510241266448071], [0.9964775038348593, 1.0242277628374021, 1.0171459979775468, 1.0252388558791246, 1.0123414029612332, 1.0217270547243045]
Training Loss (progress: 0.56), 0.09856620061752719, [0.013017697118587381, 0.013456886360567513, 0.014145943767920912, 0.01602942132552036, 0.014864046848490005, 0.015514537760998013], [0.9964700709427734, 1.024238418664308, 1.0171595860766531, 1.025249486943147, 1.0123460287670307, 1.0217359190926976]
Training Loss (progress: 0.64), 0.08278223798005023, [0.01301828246504992, 0.013463344853845047, 0.014143957200003464, 0.016032888589796936, 0.01486562381397194, 0.015509966608509758], [0.9964647994744891, 1.0242510938881477, 1.0171617652075875, 1.0252571235123862, 1.0123486837135214, 1.0217412224198024]
Training Loss (progress: 0.72), 0.0856529740378894, [0.01302165641064213, 0.013457677261174117, 0.014149071534299413, 0.01603056283595426, 0.014867441906025262, 0.0155108877661937], [0.9964654419678128, 1.0242578106246207, 1.0171743870221648, 1.0252623907211684, 1.0123470454265904, 1.0217484519129894]
Training Loss (progress: 0.80), 0.08882610798326074, [0.013021051690312476, 0.013466796461870729, 0.014154586433080197, 0.01603428301185847, 0.01487425256094582, 0.015513564866808636], [0.9964582280935287, 1.0242648667593188, 1.017180836225095, 1.0252634665460387, 1.0123554298631188, 1.0217571281156248]
Training Loss (progress: 0.88), 0.09079127025632738, [0.013026206291772055, 0.013470920159770678, 0.01415872216322455, 0.016033670565348792, 0.014881040380856714, 0.015518688638562293], [0.9964552080433932, 1.0242695059153157, 1.0171864279981042, 1.0252705596358713, 1.012363103596376, 1.0217673110794259]
Training Loss (progress: 0.96), 0.08715079708828102, [0.01302823679839767, 0.013466379600012478, 0.014152590347327668, 0.01603863465988477, 0.014878025070847001, 0.01552027583428735], [0.9964494709053314, 1.024276328425002, 1.01719536100551, 1.02528372616016, 1.0123627520461564, 1.021776220904848]
Evaluation on validation dataset:
Step 25, mean loss 0.01640679558567498
Step 50, mean loss 0.009920011566139874
Step 75, mean loss 0.01776343750472552
Step 100, mean loss 0.01966517003096773
Step 125, mean loss 0.025716334981486923
Step 150, mean loss 0.026832607861260564
Step 175, mean loss 0.04680315565405728
Step 200, mean loss 0.21655917104989852
Step 225, mean loss 0.15400744463240545
Unrolled forward losses 1.420134996818385
Unrolled forward base losses 2.565701273852575
Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00), 0.08979456847554694, [0.013026356666523842, 0.013468171338501554, 0.014153819640708542, 0.016038171388671894, 0.014880172289081745, 0.015521355199429794], [0.9964479713628486, 1.0242766170054385, 1.0171967526647212, 1.0252863092978386, 1.012362971853727, 1.0217769531930085]
Training Loss (progress: 0.08), 0.09280955409350944, [0.013028695245706986, 0.013470632286696161, 0.014161376255299396, 0.016047593668754575, 0.014885085159225231, 0.015520122052372666], [0.9964422315145062, 1.0242911968072967, 1.0172112204041628, 1.0253002633278616, 1.012369749576689, 1.021787945220738]
Training Loss (progress: 0.16), 0.09426931973505066, [0.0130305728037485, 0.013471076342250839, 0.014156363988738176, 0.016052060323157768, 0.014888346191072504, 0.015521370572908574], [0.9964380704232185, 1.024302896144966, 1.0172137880475909, 1.0253137703369262, 1.0123724878402036, 1.021793003576828]
Training Loss (progress: 0.24), 0.08769351806644303, [0.013031949567680343, 0.013470297522708237, 0.014158445131574542, 0.016055326518307114, 0.01489220546512912, 0.015525562173930755], [0.9964337771899091, 1.0243082078081995, 1.0172193315734344, 1.025317486078186, 1.0123801668148242, 1.021803214070526]
Training Loss (progress: 0.32), 0.08491729013729597, [0.013033968620286959, 0.013462366153687962, 0.0141544383078045, 0.016067029735210386, 0.014898155070767869, 0.015522996492965431], [0.9964276575909258, 1.0243054164636918, 1.0172243774343148, 1.0253277255911943, 1.0123870282101926, 1.021808038858518]
Training Loss (progress: 0.40), 0.09153054166714536, [0.013042378262700695, 0.013463485132760232, 0.014164076551545888, 0.01606542544731196, 0.014901506726025759, 0.015526558899455313], [0.9964279078824413, 1.024304140316989, 1.0172382910848812, 1.025344391259187, 1.0123905716764352, 1.0218210125211815]
Training Loss (progress: 0.48), 0.09059933904928262, [0.013036141697201974, 0.01346234631277676, 0.014165747886671293, 0.0160637885034208, 0.014902271460437632, 0.01552666298065021], [0.9964157708201018, 1.0243119903625377, 1.0172431877753962, 1.0253439627426424, 1.012391237320128, 1.0218267379020787]
Training Loss (progress: 0.56), 0.0848253678303137, [0.013038504231970413, 0.013471811233791691, 0.014166562335148774, 0.01607275481009279, 0.014907919061326439, 0.01553020927831188], [0.9964141110534537, 1.0243286645460767, 1.017247632043675, 1.025359537142455, 1.012398913002524, 1.021838495742225]
Training Loss (progress: 0.64), 0.0967816003747406, [0.013038345987192786, 0.01347608804007581, 0.014174447888524263, 0.016083716878464303, 0.014906895127455381, 0.015533183743584386], [0.9964041088937463, 1.0243397439163366, 1.0172566297053545, 1.0253721487666059, 1.0123965460529631, 1.0218494399010674]
Training Loss (progress: 0.72), 0.08524095477427421, [0.013037257734620708, 0.01347308116639882, 0.014170036148518184, 0.016076067999137075, 0.014913168774874095, 0.015535880551554337], [0.9963990505714843, 1.0243446713705109, 1.0172557021232866, 1.0253743363363927, 1.0124015383816078, 1.0218549975544662]
Training Loss (progress: 0.80), 0.09134069123075357, [0.013045280687666453, 0.013472993884199529, 0.014177076645289367, 0.01608396472838509, 0.01490746753861945, 0.015536805580414563], [0.9964024822328088, 1.0243587625884563, 1.0172690654673189, 1.0253905719419771, 1.0124024219430752, 1.0218628117535977]
Training Loss (progress: 0.88), 0.08223959306765051, [0.013044961625752152, 0.01347918525504169, 0.014176493321188222, 0.016087123592340248, 0.01491827600246585, 0.015540200720678977], [0.9963982254940942, 1.0243708756789582, 1.0172726227287847, 1.0253954295549639, 1.0124132735351279, 1.021866919686006]
Training Loss (progress: 0.96), 0.09079858381373211, [0.013044340556805551, 0.013479971662280781, 0.014177401611320872, 0.016092897900826325, 0.014921504384803127, 0.015541816000002259], [0.9963902472188831, 1.0243771771982275, 1.0172774902522643, 1.0254041761297101, 1.0124164523697077, 1.0218729172662366]
Evaluation on validation dataset:
Step 25, mean loss 0.01643369163295719
Step 50, mean loss 0.009909190289594378
Step 75, mean loss 0.01758831235622377
Step 100, mean loss 0.019524325164102678
Step 125, mean loss 0.025708188316578028
Step 150, mean loss 0.026913883938396773
Step 175, mean loss 0.046723675445149625
Step 200, mean loss 0.21702980902109395
Step 225, mean loss 0.15459752089259632
Unrolled forward losses 1.3980896971782237
Unrolled forward base losses 2.565701273852575
Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00), 0.09977538436362993, [0.013043860620233803, 0.013483500776921151, 0.01418159632209523, 0.016088807246386085, 0.014920784522190932, 0.015539489551806451], [0.9963854107170417, 1.0243822450484295, 1.017282782702463, 1.0254087787503738, 1.0124179896968557, 1.02187823891714]
Training Loss (progress: 0.08), 0.09247285585425184, [0.013044823613143528, 0.013483549915574431, 0.014184523156591846, 0.016102136134418615, 0.014927250959232927, 0.015545373454577014], [0.9963818124399818, 1.024395088114245, 1.0172919671041496, 1.0254247953958553, 1.0124240372503632, 1.0218934284549197]
Training Loss (progress: 0.16), 0.08923899147203541, [0.013046032036400333, 0.013479178596013974, 0.014194940712604312, 0.016103842971332667, 0.01493331144206768, 0.01555121568078619], [0.9963807248561569, 1.0243942029095443, 1.0173032358697225, 1.0254324105264452, 1.0124284499018565, 1.0218984691377453]
Training Loss (progress: 0.24), 0.08444978722682532, [0.013049455484218793, 0.013483173768024992, 0.01419707060866704, 0.01611020136165207, 0.01493316832002738, 0.015547055661994758], [0.9963758138087692, 1.0244120498659979, 1.0173134040567793, 1.0254452193606098, 1.0124285655776555, 1.0219051846243146]
Training Loss (progress: 0.32), 0.09649555062153041, [0.013047338112262908, 0.01348301368885402, 0.01420767084336538, 0.0161166505354542, 0.014939456525730048, 0.01554922372746885], [0.9963687066415611, 1.0244141956603803, 1.017324434121806, 1.0254507452777382, 1.0124368889924156, 1.0219120864037627]
Training Loss (progress: 0.40), 0.0909160997688422, [0.013048362814422553, 0.013482393494699287, 0.0142080348670961, 0.016123886580808572, 0.014942112615276634, 0.015551328801601863], [0.9963646241280967, 1.0244189625689561, 1.0173337082790224, 1.02546291250067, 1.0124361481711779, 1.0219155349881108]
Training Loss (progress: 0.48), 0.09128629464128139, [0.013052476484978608, 0.013488628994438109, 0.0142074511497157, 0.016128741399720533, 0.01494630258634135, 0.015553337475263988], [0.9963646640033613, 1.0244284425738557, 1.017339231179791, 1.0254733471155468, 1.0124414205678893, 1.021928031108035]
Training Loss (progress: 0.56), 0.08188900188374923, [0.013053460009903418, 0.013487456277073533, 0.014213555349032845, 0.016126579922820673, 0.014950288180195986, 0.015555848453568919], [0.9963579568529016, 1.024439823203936, 1.017354396722285, 1.0254858394085395, 1.012445415148616, 1.0219398236231028]
Training Loss (progress: 0.64), 0.09289658031155185, [0.01305588782535774, 0.013484603608835863, 0.014211336523608406, 0.016132902178677893, 0.014949457058784772, 0.015560668470752428], [0.9963549374410984, 1.0244493841883446, 1.017361142948228, 1.0254962134412957, 1.0124431203946402, 1.021948460169507]
Training Loss (progress: 0.72), 0.09483234746544511, [0.01305791429719457, 0.013491696959730641, 0.014212811994145228, 0.016132079440848202, 0.014954833044479856, 0.015565209829112005], [0.9963476002322271, 1.0244569637460368, 1.0173642312384354, 1.0254980054302427, 1.0124484310476973, 1.021957385953994]
Training Loss (progress: 0.80), 0.09007808800100409, [0.01306191754480028, 0.013487101666319012, 0.014206883441238806, 0.01614088996079529, 0.014958606904850994, 0.015560507978330097], [0.9963446313356678, 1.0244550420059961, 1.0173661218764825, 1.025510014495193, 1.0124571970356502, 1.0219596008995477]
Training Loss (progress: 0.88), 0.09171399905288004, [0.013063176121873082, 0.013487063315455548, 0.014209725866352484, 0.016142617046771616, 0.014958081691668733, 0.01556485823328017], [0.9963402078830308, 1.02446626081091, 1.0173765335403229, 1.025517952800984, 1.0124522153024886, 1.021972098697336]
Training Loss (progress: 0.96), 0.08705187178729121, [0.013063538079878804, 0.013494422228115007, 0.014211471487857882, 0.01614638438538689, 0.014963818499985395, 0.015565345577870376], [0.9963330575453649, 1.0244856735828323, 1.0173811551588996, 1.025520295465731, 1.0124550733741067, 1.0219776475700635]
Evaluation on validation dataset:
Step 25, mean loss 0.01640978315135358
Step 50, mean loss 0.00990629867319573
Step 75, mean loss 0.017735178943551366
Step 100, mean loss 0.019664237061036684
Step 125, mean loss 0.025888632910921576
Step 150, mean loss 0.02692939323647343
Step 175, mean loss 0.04675953014680022
Step 200, mean loss 0.21778015008529986
Step 225, mean loss 0.15378332832817998
Unrolled forward losses 1.4241393517445782
Unrolled forward base losses 2.565701273852575
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00), 0.09078354004622362, [0.013070514724989496, 0.013494505314900224, 0.014207998218642257, 0.01613949149050589, 0.014964259195285736, 0.01556339356320205], [0.9963370671093241, 1.024490840674507, 1.0173850860276918, 1.0255283976853353, 1.0124578497236376, 1.0219806464253718]
Training Loss (progress: 0.08), 0.09405930359962085, [0.013071868691687175, 0.013497122921519835, 0.01422052777842085, 0.01614890414918158, 0.014972859569785701, 0.015564156369195545], [0.9963343811451607, 1.024495560843515, 1.0174037419664643, 1.0255364462930983, 1.0124691461415516, 1.0219925277875166]
Training Loss (progress: 0.16), 0.09164149981779955, [0.013066202242403773, 0.013505929016494255, 0.01422987107538931, 0.016145764076893485, 0.01497951739426666, 0.015569453074188356], [0.9963250302485344, 1.0245096581155073, 1.017413372266667, 1.0255357259641862, 1.0124763781638435, 1.0220015874918789]
Training Loss (progress: 0.24), 0.09137692438041688, [0.013073095257541793, 0.013502398291131501, 0.014226541304878194, 0.0161530924661177, 0.01498446371395479, 0.015572293088577632], [0.9963264852743801, 1.0245162851954202, 1.0174156504498024, 1.0255503787857574, 1.0124788476890394, 1.022012049547469]
Training Loss (progress: 0.32), 0.09845773650212913, [0.01307106568137479, 0.013499566334119735, 0.01422238025258893, 0.016158659581400697, 0.014985514897935766, 0.015570468627694687], [0.9963189028246505, 1.0245239110888633, 1.0174126812800097, 1.0255499659501348, 1.012480235524908, 1.0220211221450648]
Training Loss (progress: 0.40), 0.09258158886935963, [0.013074624342439993, 0.013506256236984466, 0.01422748252879422, 0.01615621007980471, 0.014984573322119608, 0.015578862222402828], [0.9963190546516755, 1.0245348272004455, 1.017421462174271, 1.0255552616364443, 1.0124768700571083, 1.022030418512193]
Training Loss (progress: 0.48), 0.09033965978329085, [0.013081217003321861, 0.013497199647618398, 0.014215605194497756, 0.016158708381531148, 0.01498731915896439, 0.015576886891842757], [0.996317570811446, 1.0245340278667114, 1.017419540988679, 1.0255665455895142, 1.0124840390071694, 1.022040447907747]
Training Loss (progress: 0.56), 0.08260280365542512, [0.01307492751818582, 0.013502629583833917, 0.014224897860081107, 0.016160544580062615, 0.014995125931940226, 0.015579377430549513], [0.9963060499298781, 1.024543657555915, 1.0174314941697469, 1.025569015571008, 1.0124913745814494, 1.0220486241795081]
Training Loss (progress: 0.64), 0.09435209060420963, [0.013077601678699249, 0.013506690927857395, 0.01422620050705278, 0.016165305399614468, 0.014997316209955865, 0.015583494520363573], [0.996302746360335, 1.024557204662209, 1.0174419476838072, 1.0255861143137979, 1.0124965564119348, 1.0220612131667328]
Training Loss (progress: 0.72), 0.08934930507433848, [0.013082522753521036, 0.013508677728881376, 0.014226245550773673, 0.016166976006707683, 0.015001689383040548, 0.015586795029032478], [0.9963008865215357, 1.02456998666772, 1.0174499706990427, 1.0255963230418135, 1.0124974746036242, 1.0220698726598612]
Training Loss (progress: 0.80), 0.09158525216984657, [0.013081321191615522, 0.013510642381227666, 0.01422868332204002, 0.016170178234420598, 0.015002751106890236, 0.015585998434134964], [0.9962948691993547, 1.0245817171648215, 1.0174579126768928, 1.025600126024374, 1.0125002787089061, 1.02207481496459]
Training Loss (progress: 0.88), 0.08704467027724773, [0.013079051024592076, 0.013508890111892312, 0.014231397533900719, 0.016178048497173466, 0.015014318949808848, 0.015592736114162583], [0.9962880369115776, 1.0245812340136677, 1.0174617592793938, 1.0256079504679223, 1.0125109055208141, 1.0220830791321336]
Training Loss (progress: 0.96), 0.08350695023683129, [0.01307904338552181, 0.013514460590001135, 0.01423460770422225, 0.016176882149006637, 0.01501593751217395, 0.015592549930445567], [0.996280738497264, 1.0245871643947824, 1.0174700537760757, 1.0256222669237829, 1.0125126254088086, 1.022092684373033]
Evaluation on validation dataset:
Step 25, mean loss 0.016357130674339818
Step 50, mean loss 0.009894602056942536
Step 75, mean loss 0.01777324855096929
Step 100, mean loss 0.0195673079329708
Step 125, mean loss 0.02565027750414041
Step 150, mean loss 0.02686847398943114
Step 175, mean loss 0.046824773640105975
Step 200, mean loss 0.2159259709978947
Step 225, mean loss 0.15459874207097735
Unrolled forward losses 1.3952191862940837
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 7.52444e-01, 7.80746e-01, 7.86076e-01, 8.17306e-01, 7.92812e-01, 8.07572e-01
Node: 01 (pos: 0.010): 8.19830e-01, 8.48356e-01, 8.50559e-01, 8.76020e-01, 8.54338e-01, 8.67847e-01
Node: 02 (pos: 0.020): 8.79430e-01, 9.08006e-01, 9.07232e-01, 9.27182e-01, 9.08213e-01, 9.20494e-01
Node: 03 (pos: 0.030): 9.28764e-01, 9.57285e-01, 9.53910e-01, 9.69032e-01, 9.52454e-01, 9.63640e-01
Node: 04 (pos: 0.040): 9.65688e-01, 9.94114e-01, 9.88715e-01, 1.00008e+00, 9.85368e-01, 9.95692e-01
Node: 05 (pos: 0.051): 9.88544e-01, 1.01689e+00, 1.01020e+00, 1.01918e+00, 1.00566e+00, 1.01543e+00
-
Node: 07 (pos: 0.071): 9.88544e-01, 1.01689e+00, 1.01020e+00, 1.01918e+00, 1.00566e+00, 1.01543e+00
Node: 08 (pos: 0.081): 9.65688e-01, 9.94114e-01, 9.88715e-01, 1.00008e+00, 9.85368e-01, 9.95692e-01
Node: 09 (pos: 0.091): 9.28764e-01, 9.57285e-01, 9.53910e-01, 9.69032e-01, 9.52454e-01, 9.63640e-01
Node: 10 (pos: 0.101): 8.79430e-01, 9.08006e-01, 9.07232e-01, 9.27182e-01, 9.08213e-01, 9.20494e-01
Node: 11 (pos: 0.111): 8.19830e-01, 8.48356e-01, 8.50559e-01, 8.76020e-01, 8.54338e-01, 8.67847e-01
Node: 12 (pos: 0.121): 7.52444e-01, 7.80746e-01, 7.86076e-01, 8.17306e-01, 7.92812e-01, 8.07572e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.88544e-01, 1.01689e+00, 1.01020e+00, 1.01918e+00, 1.00566e+00, 1.01543e+00
Node: 58 (pos: 0.586): 9.65688e-01, 9.94114e-01, 9.88715e-01, 1.00008e+00, 9.85368e-01, 9.95692e-01
Node: 59 (pos: 0.596): 9.28764e-01, 9.57285e-01, 9.53910e-01, 9.69032e-01, 9.52454e-01, 9.63640e-01
Node: 60 (pos: 0.606): 8.79430e-01, 9.08006e-01, 9.07232e-01, 9.27182e-01, 9.08213e-01, 9.20494e-01
Node: 61 (pos: 0.616): 8.19830e-01, 8.48356e-01, 8.50559e-01, 8.76020e-01, 8.54338e-01, 8.67847e-01
Node: 50 (pos: 0.505): 7.52444e-01, 7.80746e-01, 7.86076e-01, 8.17306e-01, 7.92812e-01, 8.07572e-01
-
Node: 51 (pos: 0.515): 8.19830e-01, 8.48356e-01, 8.50559e-01, 8.76020e-01, 8.54338e-01, 8.67847e-01
Node: 52 (pos: 0.525): 8.79430e-01, 9.08006e-01, 9.07232e-01, 9.27182e-01, 9.08213e-01, 9.20494e-01
Node: 53 (pos: 0.535): 9.28764e-01, 9.57285e-01, 9.53910e-01, 9.69032e-01, 9.52454e-01, 9.63640e-01
Node: 54 (pos: 0.545): 9.65688e-01, 9.94114e-01, 9.88715e-01, 1.00008e+00, 9.85368e-01, 9.95692e-01
Node: 55 (pos: 0.556): 9.88544e-01, 1.01689e+00, 1.01020e+00, 1.01918e+00, 1.00566e+00, 1.01543e+00
Node: 62 (pos: 0.626): 7.52444e-01, 7.80746e-01, 7.86076e-01, 8.17306e-01, 7.92812e-01, 8.07572e-01
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.013725438791813295
Step 50, mean loss 0.007198329118934114
Step 75, mean loss 0.012540402350377265
Step 100, mean loss 0.015561014967650832
Step 125, mean loss 0.021628227140119652
Step 150, mean loss 0.029996377199161917
Step 175, mean loss 0.04507193448544852
Step 200, mean loss 0.0829984358650209
Step 225, mean loss 0.062016077850703115
Unrolled forward losses 1.1080069660254188
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr1e-05_n6_s0.001_tw25_unrolling2_time64544.tar

Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00), 0.09279903929269775, [0.013084408392776633, 0.013515677074958125, 0.014234491031693768, 0.016180397320781505, 0.015019093880303277, 0.015592026244038869], [0.9962805433903937, 1.0245966431657774, 1.017469125736141, 1.0256258891471342, 1.0125160569262859, 1.0220997330212978]
Training Loss (progress: 0.08), 0.08953974573006358, [0.01308245474283124, 0.01351461753712615, 0.014235579578868396, 0.016179907484853474, 0.015026443726352505, 0.01559831221911142], [0.9962767008494307, 1.0246139198060977, 1.0174770364433308, 1.0256309522355307, 1.0125233538459921, 1.0221098066012786]
Training Loss (progress: 0.16), 0.08687510179315797, [0.01308737601998269, 0.01351598809629915, 0.014235400187970352, 0.016191604078142324, 0.015019825560019328, 0.015596242127414277], [0.9962770207906413, 1.0246187467566195, 1.0174843159430853, 1.0256528060522616, 1.0125208117877875, 1.0221137424279187]
Training Loss (progress: 0.24), 0.08594192763743189, [0.013084752011108543, 0.013516973522811409, 0.014240233188146606, 0.016202267284716344, 0.015025797820844764, 0.01559835857613185], [0.9962686840829906, 1.0246298058525227, 1.0174906856003383, 1.0256584669419864, 1.012529136170039, 1.0221222740155316]
Training Loss (progress: 0.32), 0.09450493920061705, [0.01308556598282384, 0.01351658867960147, 0.014243450013604412, 0.016202085806878114, 0.015030938861182456, 0.01559734402423185], [0.9962622792199223, 1.0246334090846922, 1.0174991809247185, 1.025667419537379, 1.0125294993919616, 1.0221277824414075]
Training Loss (progress: 0.40), 0.09267817154957073, [0.013090108131784559, 0.01352254246853245, 0.014249938886346857, 0.016203186537387226, 0.01503288721771218, 0.015604825119849987], [0.996258056562315, 1.0246556357358354, 1.0175138612415315, 1.0256777734336477, 1.0125331321136155, 1.0221415669644238]
Training Loss (progress: 0.48), 0.09440128490661943, [0.013099039806296382, 0.013524164432958855, 0.01425352680848967, 0.016206936669895342, 0.015037304472418248, 0.015601452836143082], [0.9962615647963765, 1.0246671402363912, 1.0175225653481212, 1.0256954858503793, 1.01254355077943, 1.022146719684256]
Training Loss (progress: 0.56), 0.09628693204026885, [0.013099075824052836, 0.013524435138461327, 0.014256052000311326, 0.016209685400555212, 0.015043751457301453, 0.015609863693392716], [0.996256273109855, 1.0246689327395184, 1.0175313399454515, 1.025703296544815, 1.0125450949078278, 1.0221616198699113]
Training Loss (progress: 0.64), 0.08941888821953041, [0.013099987317923414, 0.013523785992595677, 0.01425975945261124, 0.016210654564569486, 0.015046508186393978, 0.015612962257736477], [0.9962538202620334, 1.0246820765437992, 1.0175436050211888, 1.0257061657717486, 1.0125472070713997, 1.0221686944005874]
Training Loss (progress: 0.72), 0.08987557619753714, [0.013105064731275083, 0.013520102732586876, 0.014251680221071797, 0.016217115775907405, 0.01504986916168252, 0.015608697559205183], [0.9962510785547751, 1.024682048685591, 1.017541362686504, 1.0257195041738858, 1.0125491980880068, 1.0221748830239041]
Training Loss (progress: 0.80), 0.09168135165493954, [0.01310370734642597, 0.013528158470945408, 0.014249369912441513, 0.016221242855218768, 0.015049648688002766, 0.015610548091546562], [0.9962457445767225, 1.0247020347084812, 1.017544914562791, 1.0257270676251222, 1.012553238008822, 1.0221861618955996]
Training Loss (progress: 0.88), 0.09244699592712215, [0.013104046144403152, 0.013528588558619447, 0.014261360403367944, 0.016224925976326392, 0.01505143699333206, 0.015605816599716944], [0.9962378321015843, 1.024714525521852, 1.017557071741772, 1.0257396849253824, 1.0125576162121586, 1.0221864452376193]
Training Loss (progress: 0.96), 0.08652624893786934, [0.01310607411600643, 0.013529562448385563, 0.014259759384493812, 0.016228372246853966, 0.015055705149162523, 0.015612042914365805], [0.9962350251855344, 1.0247115199030505, 1.0175601372427925, 1.025747022798346, 1.0125614263073028, 1.0221992119683194]
Evaluation on validation dataset:
Step 25, mean loss 0.01630084946748183
Step 50, mean loss 0.009880263412383
Step 75, mean loss 0.017729209935621264
Step 100, mean loss 0.01960469450442727
Step 125, mean loss 0.025698310787230474
Step 150, mean loss 0.027174307678954707
Step 175, mean loss 0.046888704469214074
Step 200, mean loss 0.2175161489854705
Step 225, mean loss 0.1552922592129556
Unrolled forward losses 1.4108963633375036
Unrolled forward base losses 2.565701273852575
Test loss: 1.1080069660254188
