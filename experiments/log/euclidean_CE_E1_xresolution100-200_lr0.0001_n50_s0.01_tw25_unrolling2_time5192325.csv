Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar
Number of parameters: 1031657.0
Saved initial model at models/init5192325.pt
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02
Node: 21 (pos: 0.212): 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02
Node: 22 (pos: 0.222): 1.11141e-02, 1.11141e-02, 1.11141e-02, 1.11141e-02, 1.11141e-02, 1.11141e-02
Node: 23 (pos: 0.232): 7.16698e-03, 7.16698e-03, 7.16698e-03, 7.16698e-03, 7.16698e-03, 7.16698e-03
Node: 24 (pos: 0.242): 4.52830e-03, 4.52830e-03, 4.52830e-03, 4.52830e-03, 4.52830e-03, 4.52830e-03
Node: 25 (pos: 0.253): 2.80332e-03, 2.80332e-03, 2.80332e-03, 2.80332e-03, 2.80332e-03, 2.80332e-03
-
Node: 26 (pos: 0.263): 1.70039e-03, 1.70039e-03, 1.70039e-03, 1.70039e-03, 1.70039e-03, 1.70039e-03
Node: 27 (pos: 0.273): 1.01056e-03, 1.01056e-03, 1.01056e-03, 1.01056e-03, 1.01056e-03, 1.01056e-03
Node: 28 (pos: 0.283): 5.88451e-04, 5.88451e-04, 5.88451e-04, 5.88451e-04, 5.88451e-04, 5.88451e-04
Node: 29 (pos: 0.293): 3.35737e-04, 3.35737e-04, 3.35737e-04, 3.35737e-04, 3.35737e-04, 3.35737e-04
Node: 30 (pos: 0.303): 1.87683e-04, 1.87683e-04, 1.87683e-04, 1.87683e-04, 1.87683e-04, 1.87683e-04
Node: 31 (pos: 0.313): 1.02799e-04, 1.02799e-04, 1.02799e-04, 1.02799e-04, 1.02799e-04, 1.02799e-04
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 1.78295e-01, 1.78295e-01, 1.78295e-01, 1.78295e-01, 1.78295e-01, 1.78295e-01
Node: 08 (pos: 0.081): 2.30101e-01, 2.30101e-01, 2.30101e-01, 2.30101e-01, 2.30101e-01, 2.30101e-01
Node: 09 (pos: 0.091): 2.90960e-01, 2.90960e-01, 2.90960e-01, 2.90960e-01, 2.90960e-01, 2.90960e-01
Node: 10 (pos: 0.101): 3.60485e-01, 3.60485e-01, 3.60485e-01, 3.60485e-01, 3.60485e-01, 3.60485e-01
Node: 11 (pos: 0.111): 4.37602e-01, 4.37602e-01, 4.37602e-01, 4.37602e-01, 4.37602e-01, 4.37602e-01
Node: 12 (pos: 0.121): 5.20485e-01, 5.20485e-01, 5.20485e-01, 5.20485e-01, 5.20485e-01, 5.20485e-01
-
Node: 00 (pos: 0.000): 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02, 1.68869e-02
Node: 01 (pos: 0.010): 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02, 2.51399e-02
Node: 02 (pos: 0.020): 3.66704e-02, 3.66704e-02, 3.66704e-02, 3.66704e-02, 3.66704e-02, 3.66704e-02
Node: 03 (pos: 0.030): 5.24089e-02, 5.24089e-02, 5.24089e-02, 5.24089e-02, 5.24089e-02, 5.24089e-02
Node: 04 (pos: 0.040): 7.33892e-02, 7.33892e-02, 7.33892e-02, 7.33892e-02, 7.33892e-02, 7.33892e-02
Node: 05 (pos: 0.051): 1.00692e-01, 1.00692e-01, 1.00692e-01, 1.00692e-01, 1.00692e-01, 1.00692e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.3782797286657313, [0.006253643873948688, 0.014405589916981149, 0.014909309497804316, 0.015753333932043787, 0.017706121206911493, 0.017460136207105095], [1.0075630538047218, 1.0147849108009421, 1.0136767331834862, 1.0141993598800123, 1.0141569973621736, 1.0132138247183247]
Training Loss (progress: 0.08), 0.25769540783941663, [0.00035896320557765024, 0.0012587145526124237, 0.0009730786005889123, 0.0028781452617120287, 0.012878769074820318, 0.018285561048876523], [1.0429049807161022, 1.0365409385556597, 1.036425963098662, 1.0442261685246421, 1.0468097217291668, 1.0375374334946161]
Training Loss (progress: 0.16), 0.2070377195107737, [0.00032045685385053183, 0.0014316612138754272, 0.0009740095212771613, 0.0023505658972279775, 0.011032839246719963, 0.016810507057202377], [1.0480004240562248, 1.0366199367924884, 1.040823122147648, 1.0585500625913158, 1.0617787389448654, 1.0403552357898767]
Training Loss (progress: 0.24), 0.20019012577545747, [0.00031122787475021147, 0.0014415236508820044, 0.0009338135930123811, 0.0015873765356419368, 0.010104327168414003, 0.014862448053182587], [1.048471905492797, 1.0381496382750894, 1.0445271334159016, 1.072850119521328, 1.0757016791510732, 1.0425839898128115]
Training Loss (progress: 0.32), 0.16486668321353318, [0.0002952348881047162, 0.0013692432570911928, 0.0008321566836363942, 0.0018341987403430147, 0.009209556868960604, 0.013280633115187146], [1.048884720560581, 1.0396032076913206, 1.0481465937776775, 1.0833158597541899, 1.088577484710057, 1.0452657013766484]
Training Loss (progress: 0.40), 0.15102998402636367, [0.0002896378410985678, 0.0014751636432136497, 0.0007987862665725532, 0.0015322936833672522, 0.008496098068375275, 0.012550198870302431], [1.0477468198101185, 1.0412527631621185, 1.0519390405996196, 1.0927305894437462, 1.1014005064828472, 1.047907135574161]
Training Loss (progress: 0.48), 0.15380348425063098, [0.0002788597034076202, 0.0014483652127272803, 0.0008843418773492282, 0.0019250327731374, 0.007600009035304797, 0.011500596236027808], [1.046454641478826, 1.04172739078227, 1.054970955225033, 1.1007721544353393, 1.1136513771318506, 1.0505028443555946]
Training Loss (progress: 0.56), 0.14497201655119613, [0.00029941142374361216, 0.0014917717288069048, 0.0008261048647845507, 0.0018215221668769, 0.008128206054433885, 0.011074194174419833], [1.0465075299674051, 1.0427834416792834, 1.0584787760803176, 1.1058345644914722, 1.1269912839322815, 1.0533210627720475]
Training Loss (progress: 0.64), 0.13500724107118697, [0.0003042047848203862, 0.0014956543948145805, 0.000793172192083952, 0.0017398978275676355, 0.007323456530741939, 0.010222978779379287], [1.0447307988461156, 1.04379424987131, 1.061291311870681, 1.1122267360039935, 1.1382838638792463, 1.0556799944751942]
Training Loss (progress: 0.72), 0.12873011788836033, [0.0003068566131758212, 0.0015552029615787434, 0.000775401908391314, 0.0016111424300637117, 0.00771361642406142, 0.009944939548420288], [1.0446271752262932, 1.0446772066692405, 1.0647848769508534, 1.1169605399450167, 1.1501928649841262, 1.0584924162544105]
Training Loss (progress: 0.80), 0.14672120114848616, [0.0002669209943242697, 0.0013944221908125558, 0.000766261029490955, 0.0016875239988273652, 0.0077055265670643875, 0.01003134110892713], [1.0428152324916613, 1.0447705738796522, 1.067813667371493, 1.1217973962955097, 1.1598875084694122, 1.060983185989198]
Training Loss (progress: 0.88), 0.12855908324124732, [0.00025459406750466513, 0.0014015309548970158, 0.0009467009929066873, 0.0017871020268206816, 0.00760069947358261, 0.009880094346579524], [1.0409198487910643, 1.0454424586335207, 1.0720303328766607, 1.1250652426230048, 1.1703985368237966, 1.0637119061287765]
Training Loss (progress: 0.96), 0.11711983530585296, [0.0002898789801533948, 0.0014950942447549507, 0.0008164005776770687, 0.0018445765247443984, 0.007252985514901079, 0.009459059734219132], [1.0371428619710685, 1.0463607451119143, 1.0745209137366665, 1.1295893142694655, 1.178188034527331, 1.066639123031532]
Evaluation on validation dataset:
Step 25, mean loss 0.09017212154595906
Step 50, mean loss 0.08818712826352072
Step 75, mean loss 0.15503279775128653
Step 100, mean loss 0.2715554770565619
Step 125, mean loss 0.1968624243020658
Step 150, mean loss 0.28079823310987617
Step 175, mean loss 0.300959325233926
Step 200, mean loss 0.3296691389650171
Step 225, mean loss 0.49606403487497286
Unrolled forward losses 18.20846770780411
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 1.21734e-51, 6.63305e-11, 1.06810e-19, 5.76660e-09, 7.66736e-03, 1.98752e-02
Node: 21 (pos: 0.212): 3.83070e-57, 5.24808e-12, 9.45492e-22, 7.32884e-10, 4.44904e-03, 1.29239e-02
Node: 22 (pos: 0.222): 6.29483e-63, 3.64577e-13, 6.56789e-24, 8.37930e-11, 2.51052e-03, 8.22035e-03
Node: 23 (pos: 0.232): 5.40169e-69, 2.22372e-14, 3.58026e-26, 8.61862e-12, 1.37765e-03, 5.11448e-03
Node: 24 (pos: 0.242): 2.42055e-75, 1.19089e-15, 1.53152e-28, 7.97489e-13, 7.35179e-04, 3.11263e-03
Node: 25 (pos: 0.253): 5.66420e-82, 5.59974e-17, 5.14107e-31, 6.63848e-14, 3.81527e-04, 1.85296e-03
-
Node: 26 (pos: 0.263): 6.92152e-89, 2.31187e-18, 1.35426e-33, 4.97130e-15, 1.92546e-04, 1.07900e-03
Node: 27 (pos: 0.273): 4.41677e-96, 8.38036e-20, 2.79945e-36, 3.34910e-16, 9.44979e-05, 6.14596e-04
Node: 28 (pos: 0.283): 1.47179e-103, 2.66724e-21, 4.54113e-39, 2.02976e-17, 4.51011e-05, 3.42431e-04
Node: 29 (pos: 0.293): 2.56111e-111, 7.45358e-23, 5.78063e-42, 1.10667e-18, 2.09330e-05, 1.86626e-04
Node: 30 (pos: 0.303): 2.32728e-119, 1.82881e-24, 5.77440e-45, 5.42809e-20, 9.44827e-06, 9.94909e-05
Node: 31 (pos: 0.313): 1.10436e-127, 3.93981e-26, 4.52646e-48, 2.39516e-21, 4.14717e-06, 5.18812e-05
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 1.48880e-24, 1.76007e-05, 1.36600e-09, 1.48399e-04, 1.11780e-01, 1.65395e-01
Node: 08 (pos: 0.081): 5.01008e-21, 8.94865e-05, 2.82779e-08, 5.56820e-04, 1.58449e-01, 2.17943e-01
Node: 09 (pos: 0.091): 8.80424e-18, 3.99471e-04, 4.59369e-07, 1.87956e-03, 2.18420e-01, 2.80916e-01
Node: 10 (pos: 0.101): 8.07940e-15, 1.56573e-03, 5.85595e-06, 5.70764e-03, 2.92802e-01, 3.54180e-01
Node: 11 (pos: 0.111): 3.87174e-12, 5.38825e-03, 5.85806e-05, 1.55924e-02, 3.81711e-01, 4.36804e-01
Node: 12 (pos: 0.121): 9.68883e-10, 1.62810e-02, 4.59865e-04, 3.83201e-02, 4.83918e-01, 5.26943e-01
-
Node: 00 (pos: 0.000): 3.83070e-57, 5.24808e-12, 9.45492e-22, 7.32884e-10, 4.44904e-03, 1.29239e-02
Node: 01 (pos: 0.010): 1.21734e-51, 6.63305e-11, 1.06810e-19, 5.76660e-09, 7.66736e-03, 1.98752e-02
Node: 02 (pos: 0.020): 2.02015e-46, 7.36086e-10, 9.46853e-18, 4.08190e-08, 1.28500e-02, 2.98981e-02
Node: 03 (pos: 0.030): 1.75063e-41, 7.17208e-09, 6.58682e-16, 2.59933e-07, 2.09431e-02, 4.39938e-02
Node: 04 (pos: 0.040): 7.92221e-37, 6.13569e-08, 3.59575e-14, 1.48908e-06, 3.31936e-02, 6.33217e-02
Node: 05 (pos: 0.051): 1.87213e-32, 4.60876e-07, 1.54036e-12, 7.67416e-06, 5.11618e-02, 8.91514e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.07158780517731317
Step 50, mean loss 0.08056346826367991
Step 75, mean loss 0.11330032122700032
Step 100, mean loss 0.14884080537358876
Step 125, mean loss 0.29564455579703364
Step 150, mean loss 0.7738936418311091
Step 175, mean loss 0.6066207335411953
Step 200, mean loss 0.5195900619866398
Step 225, mean loss 0.29755177531570076
Unrolled forward losses 17.75929430846509
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00), 0.3048484115417423, [0.000279605745560582, 0.0015847481493389817, 0.0008427380895800229, 0.0020158955165182906, 0.007926229283570802, 0.009450690634266382], [1.0357524170190486, 1.0463363577707832, 1.0758127287080728, 1.1308172231450473, 1.1820137600245129, 1.06787456382489]
Training Loss (progress: 0.08), 0.29490218719742656, [0.00034876729090652445, 0.0018104499238880889, 0.0008691662563174396, 0.001984730769542061, 0.009126391295149226, 0.011177477297045093], [1.029502334447553, 1.0476760857197027, 1.0784524071047175, 1.1309751148931382, 1.1877619241370512, 1.0725311820471788]
Training Loss (progress: 0.16), 0.2548278320478573, [0.00034366713349883056, 0.0018718417678959336, 0.0008704317295899968, 0.0022230407068488302, 0.010277645269884278, 0.011193388092635563], [1.0274114301642912, 1.0486571713697321, 1.0821004297519157, 1.133168307105536, 1.1949174398899312, 1.0777860188899062]
Training Loss (progress: 0.24), 0.2775531040960773, [0.00042011739826179525, 0.0021808382042019106, 0.0009952369137073043, 0.0023321017908144756, 0.011412868984655065, 0.011234820291982812], [1.0255841195105329, 1.0498930185547162, 1.0863403950775445, 1.1355371428800753, 1.2016851076942687, 1.0831593733883849]
Training Loss (progress: 0.32), 0.2556074989390678, [0.000408740205901347, 0.002000739996707576, 0.0009785999614025339, 0.0029096975526615755, 0.01206884108015918, 0.010787766656421846], [1.0242438446360722, 1.0500392440414938, 1.0915387979588613, 1.1370667000490196, 1.208627565976335, 1.0873778688885878]
Training Loss (progress: 0.40), 0.22522136923857952, [0.0003841024051178698, 0.002277289709302947, 0.0010506518983264348, 0.002460690637211627, 0.010754561592877366, 0.010478508849861868], [1.0231743672161377, 1.0527020315493358, 1.0957242508773009, 1.1395711114767364, 1.2117538383127946, 1.091388419692578]
Training Loss (progress: 0.48), 0.21520581300761504, [0.00045246532659770216, 0.0021176378531578247, 0.0010228812068892655, 0.002230194576341448, 0.012143863846063085, 0.010208748787135326], [1.022020516186768, 1.0535141265154553, 1.0996637787165047, 1.1409983421713648, 1.216103333165996, 1.09563517351894]
Training Loss (progress: 0.56), 0.2078977136845531, [0.00037552606058610736, 0.002176710303283267, 0.000941223451991618, 0.0021972628946760626, 0.010908612119784427, 0.009317877108050456], [1.0211576971208742, 1.0546414229161007, 1.1040781388390921, 1.1428759092812093, 1.2200508432431494, 1.0995484887838727]
Training Loss (progress: 0.64), 0.24434644738591904, [0.00043053252021886437, 0.002117122350787942, 0.0011983003872991919, 0.002694143305230501, 0.011558884517662642, 0.009680199362151438], [1.019536242326204, 1.0555119716173835, 1.1081833174774232, 1.1447328461629778, 1.223200192737229, 1.1033348708941255]
Training Loss (progress: 0.72), 0.2043588557344137, [0.00040950885032123904, 0.0023307259674135677, 0.0011428757968849443, 0.002448809598074198, 0.010574861295453706, 0.009132768823394904], [1.0183143697101884, 1.056339993568651, 1.1109933715658726, 1.1463699524187454, 1.2262255890873432, 1.1067257072436494]
Training Loss (progress: 0.80), 0.22462530192651065, [0.00039605963096297386, 0.00213952492987753, 0.0010962215198317784, 0.0023858662771114457, 0.010971754699746407, 0.009231559009285855], [1.0181694456954513, 1.0570022297797825, 1.1151427466360304, 1.1485987571234293, 1.2299641176104987, 1.1100280338682498]
Training Loss (progress: 0.88), 0.20421892067094774, [0.0004934394815050562, 0.0024387299791207174, 0.0011364385874792066, 0.0024936505077571254, 0.010984451789396485, 0.0090120293920458], [1.0162969139863611, 1.0563172816235484, 1.1174415191844294, 1.149880391394524, 1.2318009615837207, 1.1134480665235884]
Training Loss (progress: 0.96), 0.1950428657586158, [0.0004300506215607121, 0.0023917101411111957, 0.0011878187401118789, 0.0024153574024892914, 0.0099767503800324, 0.008180299665562734], [1.0145591509892298, 1.0564871055625005, 1.120765821833641, 1.151668175602965, 1.2342372111460493, 1.115975215883181]
Evaluation on validation dataset:
Step 25, mean loss 0.13675617745882654
Step 50, mean loss 0.08342225741295026
Step 75, mean loss 0.14460177677601055
Step 100, mean loss 0.23405714016861107
Step 125, mean loss 0.14179643523321217
Step 150, mean loss 0.18206450009294334
Step 175, mean loss 0.22095442530327547
Step 200, mean loss 0.33832632435151877
Step 225, mean loss 0.37267631884595176
Unrolled forward losses 5.593176633802594
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 2.05365e-38, 1.72111e-07, 3.73939e-14, 4.92405e-07, 3.52070e-02, 1.38469e-02
Node: 21 (pos: 0.212): 1.73929e-42, 3.18033e-08, 1.30858e-15, 1.00977e-07, 2.39715e-02, 8.61695e-03
Node: 22 (pos: 0.222): 9.10729e-47, 5.38928e-09, 3.85595e-17, 1.90915e-08, 1.60030e-02, 5.23347e-03
Node: 23 (pos: 0.232): 2.94834e-51, 8.37494e-10, 9.56744e-19, 3.32787e-09, 1.04748e-02, 3.10214e-03
Node: 24 (pos: 0.242): 5.90117e-56, 1.19351e-10, 1.99891e-20, 5.34820e-10, 6.72249e-03, 1.79461e-03
Node: 25 (pos: 0.253): 7.30247e-61, 1.55978e-11, 3.51662e-22, 7.92430e-11, 4.23013e-03, 1.01324e-03
-
Node: 26 (pos: 0.263): 5.58693e-66, 1.86936e-12, 5.20942e-24, 1.08250e-11, 2.60985e-03, 5.58332e-04
Node: 27 (pos: 0.273): 2.64270e-71, 2.05455e-13, 6.49812e-26, 1.36335e-12, 1.57877e-03, 3.00267e-04
Node: 28 (pos: 0.283): 7.72848e-77, 2.07078e-14, 6.82525e-28, 1.58307e-13, 9.36396e-04, 1.57601e-04
Node: 29 (pos: 0.293): 1.39737e-82, 1.91401e-15, 6.03647e-30, 1.69475e-14, 5.44553e-04, 8.07322e-05
Node: 30 (pos: 0.303): 1.56207e-88, 1.62236e-16, 4.49552e-32, 1.67272e-15, 3.10499e-04, 4.03618e-05
Node: 31 (pos: 0.313): 1.07960e-94, 1.26108e-17, 2.81910e-34, 1.52214e-16, 1.73588e-04, 1.96938e-05
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 2.29117e-18, 7.01571e-04, 5.50566e-07, 1.20194e-03, 2.33592e-01, 1.43056e-01
Node: 08 (pos: 0.081): 9.34141e-16, 2.07090e-03, 4.72219e-06, 3.31872e-03, 2.98859e-01, 1.93891e-01
Node: 09 (pos: 0.091): 2.35472e-13, 5.60580e-03, 3.41045e-05, 8.44835e-03, 3.74899e-01, 2.56475e-01
Node: 10 (pos: 0.101): 3.66976e-11, 1.39159e-02, 2.07402e-04, 1.98283e-02, 4.61107e-01, 3.31107e-01
Node: 11 (pos: 0.111): 3.53597e-09, 3.16794e-02, 1.06205e-03, 4.29056e-02, 5.56069e-01, 4.17183e-01
Node: 12 (pos: 0.121): 2.10644e-07, 6.61356e-02, 4.57946e-03, 8.55961e-02, 6.57498e-01, 5.13005e-01
-
Node: 00 (pos: 0.000): 1.73929e-42, 3.18033e-08, 1.30858e-15, 1.00977e-07, 2.39715e-02, 8.61695e-03
Node: 01 (pos: 0.010): 2.05365e-38, 1.72111e-07, 3.73939e-14, 4.92405e-07, 3.52070e-02, 1.38469e-02
Node: 02 (pos: 0.020): 1.49918e-34, 8.54153e-07, 8.99779e-13, 2.21377e-06, 5.06994e-02, 2.17165e-02
Node: 03 (pos: 0.030): 6.76629e-31, 3.88738e-06, 1.82307e-11, 9.17608e-06, 7.15838e-02, 3.32400e-02
Node: 04 (pos: 0.040): 1.88808e-27, 1.62245e-05, 3.11032e-10, 3.50666e-05, 9.90984e-02, 4.96556e-02
Node: 05 (pos: 0.051): 3.25732e-24, 6.20981e-05, 4.46828e-09, 1.23550e-04, 1.34511e-01, 7.23956e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.11105037317561622
Step 50, mean loss 0.0684964735938158
Step 75, mean loss 0.09638088844227102
Step 100, mean loss 0.10437540121657399
Step 125, mean loss 0.1505362946487855
Step 150, mean loss 0.28149923811948185
Step 175, mean loss 0.3157023954867605
Step 200, mean loss 0.3476731012103178
Step 225, mean loss 0.31697080185599413
Unrolled forward losses 5.393148836621157
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00), 0.2560412011197388, [0.0004635531526511682, 0.0022051643948637293, 0.0010859494806591964, 0.0023678565560413542, 0.010406047518572582, 0.008553844724695298], [1.0139643471547968, 1.0565641023974162, 1.1224397909658048, 1.1522719874376472, 1.2354875948165396, 1.1175426205475798]
Training Loss (progress: 0.08), 0.27012370684387704, [0.000481516221165194, 0.002131734040095283, 0.001056275201207005, 0.0023827999519748927, 0.01081295388193274, 0.008255714786949713], [1.01318900248857, 1.057035054673507, 1.1227195399792909, 1.1528994805327426, 1.2368386824902724, 1.1193432072210174]
Training Loss (progress: 0.16), 0.27744370802130186, [0.00045692577469921456, 0.0019835997358366394, 0.001087510174648023, 0.0022175613975797246, 0.01122642390719432, 0.008503125793945533], [1.0120857453668002, 1.0570496787007044, 1.1231620393666886, 1.1539362534537976, 1.2383104907337859, 1.1212892842937896]
Training Loss (progress: 0.24), 0.25799811505148273, [0.0005046348099356817, 0.0020092845254978038, 0.0010878977788932721, 0.0022675619762469734, 0.010837995456519606, 0.00824142271265381], [1.0115869567800613, 1.0571419549507055, 1.1237322574559236, 1.1543103717002985, 1.239432630621746, 1.1229464891578753]
Training Loss (progress: 0.32), 0.2289236754807908, [0.00046271846938321143, 0.0021320193528673735, 0.0010331804005776837, 0.0023287511300990136, 0.010791375430668342, 0.007934532979442433], [1.0112570989211986, 1.057095628112884, 1.1242920045664837, 1.155199195134094, 1.2415558871237797, 1.1246048802549014]
Training Loss (progress: 0.40), 0.24145805141561555, [0.000466223232546248, 0.002169474381438721, 0.0010098759635067016, 0.00227021951040211, 0.01054993466644256, 0.007806805946172588], [1.0104805316377534, 1.057145090680205, 1.1250067336087117, 1.1558004270733442, 1.2426153018824364, 1.1258633636001907]
Training Loss (progress: 0.48), 0.24468757431498012, [0.00045842553707184743, 0.0020990083002735225, 0.0010367326896629375, 0.0023191186981917286, 0.010690927102026933, 0.00775649843684552], [1.0102398281333316, 1.0572655159197557, 1.1250054375778007, 1.1558499184848552, 1.2438855946229677, 1.12707997965303]
Training Loss (progress: 0.56), 0.2726458187715981, [0.0005098963300637283, 0.002231907620901104, 0.001046485564152342, 0.002372185001849756, 0.0108542403262817, 0.007782289586544501], [1.0098069037405317, 1.0572322079463734, 1.1258188595271994, 1.1560674177423305, 1.2458073685789433, 1.128167956495175]
Training Loss (progress: 0.64), 0.2558982377322728, [0.0004743880428937313, 0.0021805968179345932, 0.001051692156946567, 0.0023701360672697754, 0.011070569979666729, 0.007826804622308376], [1.0088501112977901, 1.0574552714412346, 1.1258403762307352, 1.1566812735065137, 1.247326958223256, 1.1295553487679362]
Training Loss (progress: 0.72), 0.23373236509111514, [0.0004917316429441389, 0.0020541197270580102, 0.0010298903565061303, 0.002374410519063968, 0.0104387958043303, 0.007657065791768436], [1.0085086172660795, 1.0570382199625556, 1.1263255023113767, 1.1566589178636169, 1.2484418648652953, 1.1307290815333604]
Training Loss (progress: 0.80), 0.23574660047401255, [0.00043619224294728985, 0.0022296301106120536, 0.0010548392944350598, 0.00224972599223078, 0.010749948484382807, 0.0077174324947245155], [1.0075706090122327, 1.0571815454154687, 1.1264002434308378, 1.157244628043912, 1.2503742834835638, 1.1316446768034283]
Training Loss (progress: 0.88), 0.2261639464030902, [0.00046240056944681844, 0.0021139526039051534, 0.0010720251363680324, 0.0021355975616327866, 0.010711986735488375, 0.0076329021687871965], [1.0068874811169914, 1.0573743893162346, 1.1273754908404139, 1.1577391754131763, 1.2522415366761708, 1.132712933197876]
Training Loss (progress: 0.96), 0.22471196132404558, [0.00047629427684141973, 0.002262906721320575, 0.0010197933866075398, 0.002293928838273427, 0.010729268015718793, 0.007633947697013374], [1.0062607943361341, 1.0574481793646506, 1.1277921619090077, 1.1581536515882802, 1.253659408510525, 1.133658182024985]
Evaluation on validation dataset:
Step 25, mean loss 0.09494475606014058
Step 50, mean loss 0.0544627147842212
Step 75, mean loss 0.10860274243976431
Step 100, mean loss 0.1097343704461303
Step 125, mean loss 0.10709380117124874
Step 150, mean loss 0.09728603146490054
Step 175, mean loss 0.13883830497384353
Step 200, mean loss 0.19546136721283291
Step 225, mean loss 0.2609835871136928
Unrolled forward losses 3.3585896714532737
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 1.06041e-35, 4.62847e-08, 5.66660e-15, 1.78971e-07, 4.07437e-02, 9.02786e-03
Node: 21 (pos: 0.212): 1.76515e-39, 7.42082e-09, 1.61641e-16, 3.28823e-08, 2.81373e-02, 5.35558e-03
Node: 22 (pos: 0.222): 1.88066e-43, 1.08317e-09, 3.84203e-18, 5.53871e-09, 1.90660e-02, 3.09314e-03
Node: 23 (pos: 0.232): 1.28250e-47, 1.43938e-10, 7.60941e-20, 8.55302e-10, 1.26763e-02, 1.73925e-03
Node: 24 (pos: 0.242): 5.59794e-52, 1.74134e-11, 1.25581e-21, 1.21087e-10, 8.26947e-03, 9.52127e-04
Node: 25 (pos: 0.253): 1.56394e-56, 1.91789e-12, 1.72693e-23, 1.57159e-11, 5.29321e-03, 5.07456e-04
-
Node: 26 (pos: 0.263): 2.79660e-61, 1.92308e-13, 1.97883e-25, 1.87002e-12, 3.32442e-03, 2.63313e-04
Node: 27 (pos: 0.273): 3.20083e-66, 1.75550e-14, 1.88939e-27, 2.03995e-13, 2.04865e-03, 1.33020e-04
Node: 28 (pos: 0.283): 2.34486e-71, 1.45893e-15, 1.50320e-29, 2.04014e-14, 1.23872e-03, 6.54231e-05
Node: 29 (pos: 0.293): 1.09949e-76, 1.10383e-16, 9.96538e-32, 1.87053e-15, 7.34913e-04, 3.13268e-05
Node: 30 (pos: 0.303): 3.29978e-82, 7.60325e-18, 5.50492e-34, 1.57230e-16, 4.27811e-04, 1.46040e-05
Node: 31 (pos: 0.313): 6.33872e-88, 4.76792e-19, 2.53390e-36, 1.21163e-17, 2.44356e-04, 6.62822e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 4.24883e-17, 3.79501e-04, 2.28218e-07, 7.50381e-04, 2.52107e-01, 1.18040e-01
Node: 08 (pos: 0.081): 1.12333e-14, 1.22693e-03, 2.23146e-06, 2.22311e-03, 3.19630e-01, 1.64968e-01
Node: 09 (pos: 0.091): 1.90094e-12, 3.61124e-03, 1.81806e-05, 6.03819e-03, 3.97617e-01, 2.24461e-01
Node: 10 (pos: 0.101): 2.05897e-10, 9.67666e-03, 1.23426e-04, 1.50355e-02, 4.85331e-01, 2.97339e-01
Node: 11 (pos: 0.111): 1.42742e-08, 2.36062e-02, 6.98214e-04, 3.43237e-02, 5.81254e-01, 3.83472e-01
Node: 12 (pos: 0.121): 6.33393e-07, 5.24274e-02, 3.29117e-03, 7.18350e-02, 6.83044e-01, 4.81489e-01
-
Node: 00 (pos: 0.000): 1.76515e-39, 7.42082e-09, 1.61641e-16, 3.28823e-08, 2.81373e-02, 5.35558e-03
Node: 01 (pos: 0.010): 1.06041e-35, 4.62847e-08, 5.66660e-15, 1.78971e-07, 4.07437e-02, 9.02786e-03
Node: 02 (pos: 0.020): 4.07744e-32, 2.62818e-07, 1.65529e-13, 8.93035e-07, 5.78887e-02, 1.48161e-02
Node: 03 (pos: 0.030): 1.00351e-28, 1.35864e-06, 4.02910e-12, 4.08526e-06, 8.07016e-02, 2.36729e-02
Node: 04 (pos: 0.040): 1.58080e-25, 6.39415e-06, 8.17188e-11, 1.71331e-05, 1.10389e-01, 3.68248e-02
Node: 05 (pos: 0.051): 1.59387e-22, 2.73965e-05, 1.38108e-09, 6.58749e-05, 1.48157e-01, 5.57699e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.073414140316594
Step 50, mean loss 0.04197604230185243
Step 75, mean loss 0.06167225586814432
Step 100, mean loss 0.08818595235216092
Step 125, mean loss 0.0826418686101957
Step 150, mean loss 0.21765890234643695
Step 175, mean loss 0.15569582136140364
Step 200, mean loss 0.37068635505772807
Step 225, mean loss 0.2164532835266346
Unrolled forward losses 2.765215907793404
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00), 0.22254559200447072, [0.00043575797213061564, 0.0020879986542681403, 0.001074831209398917, 0.0023949758639934563, 0.01076547280094887, 0.007667046584388233], [1.005943951599935, 1.0570712380164171, 1.1281351736818088, 1.1582141667904073, 1.2540076857982858, 1.134403418645116]
Training Loss (progress: 0.08), 0.22947353051977482, [0.00045719359376324744, 0.00202050975417982, 0.0010989504514256402, 0.0023043428093793878, 0.010612708540180574, 0.007637977139226655], [1.0054800788885034, 1.0574004457862922, 1.1283326647785843, 1.1586226620941995, 1.255660725432197, 1.1355290227642807]
Training Loss (progress: 0.16), 0.2108572081020074, [0.0004955698427656984, 0.001985716808685212, 0.001074110752786834, 0.0023114505017347944, 0.010735684993365486, 0.007376756922473123], [1.0054992799419624, 1.0571568658953916, 1.1281445221126414, 1.1588219098933104, 1.2565285916820657, 1.1364504637730293]
Training Loss (progress: 0.24), 0.2026109185701576, [0.00048213957731651, 0.002200504120383031, 0.001018341102073338, 0.0023051371525368426, 0.010412348018766851, 0.007392436393225458], [1.005036380104727, 1.0570816120285218, 1.1284379476475366, 1.1592662793595145, 1.2574391322693097, 1.1373690145696638]
Training Loss (progress: 0.32), 0.2210838046829017, [0.0005166328996206593, 0.0021660534157164762, 0.0010567999672344102, 0.002439014719593614, 0.010878415868770148, 0.007426122137921762], [1.0043998754004333, 1.056567823641884, 1.1286960619361788, 1.1594076404732734, 1.258998257066663, 1.1383884623138405]
Training Loss (progress: 0.40), 0.22582165297233578, [0.00048611990511831214, 0.002171015154350517, 0.0011164534267808504, 0.0025087272180423347, 0.01071529300019462, 0.007416906287228405], [1.003486307757929, 1.0570762027865837, 1.1291364190608946, 1.1597061145711627, 1.2596915925962455, 1.1394561076302103]
Training Loss (progress: 0.48), 0.22732376444075836, [0.0004706258464877999, 0.0021413924874148765, 0.0011205402025812967, 0.0023663250662191203, 0.010728257381332814, 0.007350921323385008], [1.002408820535165, 1.0568100709267512, 1.1299149358110654, 1.1600787037471458, 1.2610128750991876, 1.1402868197902698]
Training Loss (progress: 0.56), 0.24441369428137824, [0.000468896773487306, 0.002127843530700321, 0.0011089065157904169, 0.002401977592527366, 0.010669920102483526, 0.007411581924595325], [1.002023915496586, 1.0570398529779805, 1.1302727138093096, 1.1605108361210477, 1.261773224551839, 1.1410278673132992]
Training Loss (progress: 0.64), 0.1992692678224924, [0.0004661988179724158, 0.0021221050730229818, 0.0011000266390469224, 0.002403083971268599, 0.010717309283326041, 0.007362836824596237], [1.0013789592894196, 1.056937587427748, 1.1302016918834592, 1.1600209699767916, 1.262869519209433, 1.1418107455041444]
Training Loss (progress: 0.72), 0.2062681011783655, [0.0004503448809170663, 0.0021287557248441938, 0.001125545798017599, 0.002413979364331882, 0.010875951477367234, 0.007426153970138851], [1.0010836307116655, 1.0566859153049482, 1.1302868068472454, 1.1605554068917454, 1.2647008577425696, 1.1427470413893899]
Training Loss (progress: 0.80), 0.19209701003354837, [0.0004834367314517341, 0.002219991566222309, 0.0010414959159428699, 0.002371240510232504, 0.010622006470663869, 0.007230766153321983], [1.0004736966638086, 1.056904513058683, 1.1304403643333474, 1.1610664510604183, 1.2656182320680078, 1.1435346894990372]
Training Loss (progress: 0.88), 0.22998137624943413, [0.0004920346640811002, 0.002123541566652846, 0.0010613981592284942, 0.002423586066750927, 0.010892177734149206, 0.007194372853422542], [0.9998012204012257, 1.0570095376844004, 1.130846934707804, 1.1613404969295622, 1.2672428571210963, 1.144492391874583]
Training Loss (progress: 0.96), 0.22680661848137346, [0.00042526901622524997, 0.002271064455533126, 0.0010243208309262549, 0.0023312520253646095, 0.011043767464756053, 0.007465171018866354], [0.9993779680028568, 1.0569415373702244, 1.1308990852467873, 1.1612601351944265, 1.2679192881520576, 1.1452983200918876]
Evaluation on validation dataset:
Step 25, mean loss 0.08834716872432496
Step 50, mean loss 0.047708950139815076
Step 75, mean loss 0.10917437986383874
Step 100, mean loss 0.10731469140034432
Step 125, mean loss 0.10881227017599311
Step 150, mean loss 0.08499338588483146
Step 175, mean loss 0.1375961366184694
Step 200, mean loss 0.23592361206372098
Step 225, mean loss 0.2590360457127645
Unrolled forward losses 3.3644236712066355
Unrolled forward base losses 2.565701273852575
Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00), 0.2030512378233823, [0.000483586523659991, 0.0021114356338553493, 0.0010901826642720593, 0.0023629200380307627, 0.010555828325938163, 0.007212816646072705], [0.9988594396551338, 1.056819025663393, 1.131399458343758, 1.1621337965433975, 1.268297140260346, 1.1455708841731405]
Training Loss (progress: 0.08), 0.20844632067501484, [0.00044249141812128205, 0.002184957304531565, 0.0010764962200326296, 0.0024788357762913363, 0.010800977059197471, 0.007369126027378653], [0.998038699586566, 1.056510713408534, 1.1316979319122384, 1.1618557120524735, 1.2696868207959051, 1.146653783599769]
Training Loss (progress: 0.16), 0.2159826453697455, [0.0004958592966430675, 0.0021324472123048777, 0.0011395934869807666, 0.002325492039967035, 0.011014929524923101, 0.007240166892995689], [0.9977994649720751, 1.056766499039035, 1.1322736415761376, 1.1624904617850125, 1.2705996147949172, 1.146971224266598]
Training Loss (progress: 0.24), 0.19351466576271775, [0.0004808077574150889, 0.0022692709609073787, 0.0010710078440737802, 0.0023931132590560822, 0.011032675149491475, 0.007306563970535421], [0.9967978510218318, 1.0562029340655716, 1.1319578404068662, 1.1623423938342183, 1.2714343436869706, 1.1475935193572646]
Training Loss (progress: 0.32), 0.22492480661643965, [0.0004827305759857198, 0.00225646103083879, 0.0011562685433971992, 0.0024368318935227918, 0.010618871184768746, 0.007233152442863823], [0.9964898442565805, 1.0561947007142254, 1.132328303715059, 1.1627989119037467, 1.2722892012336944, 1.1484455116786467]
Training Loss (progress: 0.40), 0.19316066910913088, [0.0005019560644249349, 0.002215332301059634, 0.0010915166252356094, 0.0023315169945785806, 0.011109965133284102, 0.007171110427323545], [0.9960526994368362, 1.0562323243445442, 1.132468668423958, 1.163019826135156, 1.273946960230198, 1.1493102321404773]
Training Loss (progress: 0.48), 0.22348287936961758, [0.00044176685965013885, 0.0020521413412086346, 0.0010333319140688603, 0.002334666924109679, 0.010853411646418181, 0.007170467081232236], [0.9953284060505355, 1.056296498547044, 1.132605019051196, 1.163617262495845, 1.2742276027902986, 1.1499032490111938]
Training Loss (progress: 0.56), 0.2040616357097984, [0.00044145238200178285, 0.002082744482040879, 0.001016543582436958, 0.0024133600756613004, 0.010882166331414556, 0.007115279673514907], [0.9951701583518846, 1.0562996997790166, 1.132702624997718, 1.1636130055810436, 1.274945462283407, 1.1505941832965554]
Training Loss (progress: 0.64), 0.2013687360893366, [0.00046792278804032105, 0.0021344515401008784, 0.0011002766606009664, 0.0024318325065435073, 0.010824586220198133, 0.007247898321646116], [0.9942275234909678, 1.0561604918860512, 1.1330598436292756, 1.1642481984437545, 1.2759884171262383, 1.151453954610017]
Training Loss (progress: 0.72), 0.21095264237007536, [0.0004843451183632343, 0.002053218456177452, 0.0010948782794536944, 0.0022969887476592746, 0.010754435656478054, 0.007190591503201866], [0.9938574276502018, 1.055837421821473, 1.1334482862187576, 1.163580422990424, 1.2764764623381983, 1.1519415185846067]
Training Loss (progress: 0.80), 0.18217864176376647, [0.00045950725375317066, 0.002126204522460194, 0.00106336377117482, 0.0022834866871489553, 0.010676160272375763, 0.007156936245590417], [0.9926834283829905, 1.0553166806223973, 1.133380719234614, 1.163620267778944, 1.277406221965827, 1.152584326926351]
Training Loss (progress: 0.88), 0.2039705321554061, [0.0005334946483711061, 0.0022469805101325564, 0.0010993130126129083, 0.0024572883377057837, 0.011254071070002744, 0.007055556035077839], [0.9917925244432373, 1.0551582856104824, 1.133288155897172, 1.164017553414663, 1.2787398594854147, 1.1531991937947934]
Training Loss (progress: 0.96), 0.20979994542661562, [0.0004584804263209319, 0.0021740666145330483, 0.0010814291546548576, 0.002393286297666244, 0.01108886961311657, 0.007120609988215063], [0.9917406324693292, 1.0551056831992367, 1.1333364118890485, 1.1640250426195475, 1.2797454974292644, 1.1538090308768587]
Evaluation on validation dataset:
Step 25, mean loss 0.08419602355863523
Step 50, mean loss 0.04726586953015163
Step 75, mean loss 0.10079145140486452
Step 100, mean loss 0.08844681292307213
Step 125, mean loss 0.10527297384864201
Step 150, mean loss 0.0949575007202746
Step 175, mean loss 0.150005181295238
Step 200, mean loss 0.23222410952191944
Step 225, mean loss 0.23742634906138013
Unrolled forward losses 3.4401530809852776
Unrolled forward base losses 2.565701273852575
Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00), 0.17095812660782142, [0.000494118861245001, 0.00216075095830575, 0.0010463104383272512, 0.0023868663099363497, 0.011133087697243277, 0.007244534173802767], [0.9909899480545804, 1.0551096075700324, 1.1333180433922667, 1.164444018178114, 1.2798845026615362, 1.1543058067512617]
Training Loss (progress: 0.08), 0.16251183669947533, [0.0004673525116566274, 0.002172525805148398, 0.0010539647020866227, 0.0023537676482306073, 0.011208535171349936, 0.0070816160447207364], [0.9913515099579668, 1.0552279645252163, 1.133632720954086, 1.1645004999109454, 1.2809642094520992, 1.154894272210374]
Training Loss (progress: 0.16), 0.16618573992748803, [0.0004640510499985807, 0.002181514176594409, 0.0010751410126581427, 0.0024097551338033703, 0.010901962402688077, 0.0070692153614314], [0.9914570109383231, 1.0551661294556818, 1.1335559121843926, 1.1649719097862148, 1.281224019385801, 1.1555000239300495]
Training Loss (progress: 0.24), 0.17292594260595173, [0.0004656578228144904, 0.0021954099341468924, 0.0010480394947630335, 0.0023893586050172423, 0.010949874020024338, 0.007067716081327259], [0.991149196398965, 1.0554789443166295, 1.133468804803614, 1.1652303788222986, 1.2820661135102127, 1.1560635946371551]
Training Loss (progress: 0.32), 0.17358706132039067, [0.00047966018303908007, 0.0022144793300098936, 0.0010309365989627998, 0.002373993766674715, 0.01090035955934257, 0.007043142721571333], [0.9907137343253237, 1.0555030506032475, 1.1332271449837514, 1.1653435602516353, 1.2826047333292396, 1.1565956699162496]
Training Loss (progress: 0.40), 0.16515024230872166, [0.0004610286898200141, 0.0022701847246587217, 0.001081565985554082, 0.0023634323952984775, 0.011060149537394317, 0.0070656759236615305], [0.9907846849105425, 1.0553258517800017, 1.1335048882752294, 1.1652935366549944, 1.2833948476181944, 1.1570275122710243]
Training Loss (progress: 0.48), 0.17988363175845506, [0.0004898412725915198, 0.002213159899706881, 0.00106909675011637, 0.002329258250077431, 0.01111283285081873, 0.00694398215881862], [0.9906267025232027, 1.0552834519496068, 1.133299329647453, 1.1656464820027002, 1.2842687259818426, 1.1575144392753773]
Training Loss (progress: 0.56), 0.16235094167389338, [0.0004654594862496868, 0.002184863058640548, 0.001077950059574574, 0.002402449524475215, 0.01095041382914072, 0.006992293894909354], [0.9903059069028783, 1.0552281824732428, 1.1334676264037917, 1.1659630122064035, 1.2846900721309997, 1.1579774221810288]
Training Loss (progress: 0.64), 0.186688071404942, [0.0004555613315652526, 0.0022112049043478445, 0.0010880306322908276, 0.00239696833266177, 0.010853292559613566, 0.006938574326569128], [0.9904162244631157, 1.0553096666228408, 1.1335553487337704, 1.1660021158230724, 1.2853916526734337, 1.158424485333559]
Training Loss (progress: 0.72), 0.15502408904830425, [0.0004508564244793393, 0.002289036816690816, 0.0010831405792258015, 0.0024052781297736535, 0.010778244256461246, 0.00689141950454164], [0.9905038394801899, 1.0553259187682018, 1.133442793213258, 1.1662460251860238, 1.2859834568354145, 1.1587585388684074]
Training Loss (progress: 0.80), 0.1726329457163202, [0.0004301341284112532, 0.0021934336014461228, 0.0011215961664183382, 0.0023597628241934965, 0.011055965819490652, 0.006932249271792229], [0.9904337844347897, 1.055367485102733, 1.1334433395077728, 1.1663523224478851, 1.2866416364479882, 1.1591761763328061]
Training Loss (progress: 0.88), 0.1797631068138517, [0.00045598025589074965, 0.002216757597361093, 0.001101479790695369, 0.0023752478651968994, 0.010829897325102906, 0.006916916532008247], [0.9902780891829924, 1.0553911983936195, 1.1333778435335164, 1.1667811040691596, 1.2872912468543378, 1.1597222592907857]
Training Loss (progress: 0.96), 0.16933872214131612, [0.0004928997796556175, 0.0022170304183176874, 0.0010923771643149044, 0.0022677820758347286, 0.010886937647429033, 0.006926015426149029], [0.98989458711551, 1.0552955656331637, 1.1333639721048787, 1.1668008632940798, 1.2878864404511277, 1.1600485937841112]
Evaluation on validation dataset:
Step 25, mean loss 0.06641711237971094
Step 50, mean loss 0.03371455331181267
Step 75, mean loss 0.08076055535610698
Step 100, mean loss 0.07622495427531714
Step 125, mean loss 0.08252241293160312
Step 150, mean loss 0.07468583652240249
Step 175, mean loss 0.11802937903229105
Step 200, mean loss 0.15936572244458752
Step 225, mean loss 0.21172980150220594
Unrolled forward losses 2.3636680162428645
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 2.44130e-35, 3.49715e-08, 7.12706e-16, 1.46918e-07, 4.33824e-02, 5.46804e-03
Node: 21 (pos: 0.212): 4.45468e-39, 5.44078e-09, 1.62425e-17, 2.64027e-08, 3.00762e-02, 3.06529e-03
Node: 22 (pos: 0.222): 5.22731e-43, 7.69432e-10, 3.04913e-19, 4.34504e-09, 2.04633e-02, 1.66810e-03
Node: 23 (pos: 0.232): 3.94464e-47, 9.89102e-11, 4.71499e-21, 6.54804e-10, 1.36637e-02, 8.81213e-04
Node: 24 (pos: 0.242): 1.91426e-51, 1.15577e-11, 6.00575e-23, 9.03655e-11, 8.95369e-03, 4.51908e-04
Node: 25 (pos: 0.253): 5.97397e-56, 1.22763e-12, 6.30138e-25, 1.14200e-11, 5.75809e-03, 2.24972e-04
-
Node: 26 (pos: 0.263): 1.19892e-60, 1.18528e-13, 5.44610e-27, 1.32161e-12, 3.63409e-03, 1.08722e-04
Node: 27 (pos: 0.273): 1.54734e-65, 1.04025e-14, 3.87719e-29, 1.40060e-13, 2.25089e-03, 5.10055e-05
Node: 28 (pos: 0.283): 1.28424e-70, 8.29879e-16, 2.27369e-31, 1.35924e-14, 1.36822e-03, 2.32287e-05
Node: 29 (pos: 0.293): 6.85445e-76, 6.01802e-17, 1.09831e-33, 1.20796e-15, 8.16202e-04, 1.02694e-05
Node: 30 (pos: 0.303): 2.35270e-81, 3.96692e-18, 4.37021e-36, 9.83064e-17, 4.77839e-04, 4.40728e-06
Node: 31 (pos: 0.313): 5.19309e-87, 2.37693e-19, 1.43239e-38, 7.32628e-18, 2.74541e-04, 1.83615e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 6.22362e-17, 3.32520e-04, 8.66710e-08, 6.86868e-04, 2.63346e-01, 9.44704e-02
Node: 08 (pos: 0.081): 1.55136e-14, 1.09597e-03, 9.78596e-07, 2.06401e-03, 3.33048e-01, 1.36906e-01
Node: 09 (pos: 0.091): 2.48684e-12, 3.28355e-03, 9.10154e-06, 5.67967e-03, 4.13361e-01, 1.92602e-01
Node: 10 (pos: 0.101): 2.56360e-10, 8.94229e-03, 6.97281e-05, 1.43122e-02, 5.03493e-01, 2.63033e-01
Node: 11 (pos: 0.111): 1.69949e-08, 2.21368e-02, 4.40030e-04, 3.30266e-02, 6.01864e-01, 3.48712e-01
Node: 12 (pos: 0.121): 7.24528e-07, 4.98131e-02, 2.28738e-03, 6.97901e-02, 7.06066e-01, 4.48781e-01
-
Node: 00 (pos: 0.000): 4.45468e-39, 5.44078e-09, 1.62425e-17, 2.64027e-08, 3.00762e-02, 3.06529e-03
Node: 01 (pos: 0.010): 2.44130e-35, 3.49715e-08, 7.12706e-16, 1.46918e-07, 4.33824e-02, 5.46804e-03
Node: 02 (pos: 0.020): 8.60383e-32, 2.04328e-07, 2.57603e-14, 7.48646e-07, 6.14108e-02, 9.46894e-03
Node: 03 (pos: 0.030): 1.94998e-28, 1.08518e-06, 7.66958e-13, 3.49341e-06, 8.53136e-02, 1.59177e-02
Node: 04 (pos: 0.040): 2.84206e-25, 5.23889e-06, 1.88094e-11, 1.49278e-05, 1.16314e-01, 2.59759e-02
Node: 05 (pos: 0.051): 2.66381e-22, 2.29899e-05, 3.79979e-10, 5.84138e-05, 1.55628e-01, 4.11499e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.056208042688735534
Step 50, mean loss 0.023718772558457394
Step 75, mean loss 0.039704050520801316
Step 100, mean loss 0.05891290941068092
Step 125, mean loss 0.06249113413389555
Step 150, mean loss 0.12878130361499984
Step 175, mean loss 0.10989142450769215
Step 200, mean loss 0.29278395021709713
Step 225, mean loss 0.15546447292847337
Unrolled forward losses 1.9268791959404665
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00), 0.17525296093179452, [0.000462326941927054, 0.0022005375422139376, 0.0010775644597364855, 0.0023330821206833997, 0.01083146356907128, 0.0069292550252459005], [0.9898454361659955, 1.0553063854710496, 1.1332880658115658, 1.1670243516744248, 1.287898639390243, 1.1602208076401361]
Training Loss (progress: 0.08), 0.17136196368693093, [0.0004898959037395805, 0.0022565926652097783, 0.001053277154536177, 0.002342441700975917, 0.010845460931489111, 0.0068766830647496325], [0.9894776660471782, 1.0552700260731935, 1.1332058175233495, 1.1670237029995365, 1.288856664157046, 1.160576521133684]
Training Loss (progress: 0.16), 0.16531109519166245, [0.0004990073209718787, 0.002221895610404875, 0.0010595429570386483, 0.0023842282011925222, 0.010913606278267313, 0.006887259570952626], [0.9893855472687593, 1.0551482500727725, 1.1333912958670456, 1.1672903126139718, 1.289314549940519, 1.1609609208170433]
Training Loss (progress: 0.24), 0.1553893771333398, [0.00047270676095165464, 0.0021658899106089717, 0.0010748475142511194, 0.0023317899610643257, 0.010861781465903548, 0.006896980497009971], [0.9889963654793001, 1.0552074838814929, 1.1334611488109154, 1.167436382215865, 1.289987599129204, 1.1612758698737429]
Training Loss (progress: 0.32), 0.17022365454736543, [0.00046946469316205787, 0.0021806720877430986, 0.0010789285149697292, 0.0023251677914964686, 0.010877421474634337, 0.00684966159345573], [0.9888093762952046, 1.0552158700962413, 1.133523122177874, 1.1673546898663285, 1.2906845870164463, 1.1616354807287195]
Training Loss (progress: 0.40), 0.16228958510635955, [0.00046178864186230563, 0.0022433244013678917, 0.0011082350098700093, 0.002360497099949362, 0.010777961793863382, 0.0068692326425788585], [0.9886863467827073, 1.0552329345573765, 1.1334616201332974, 1.1676007770693615, 1.2912284307299988, 1.1619476365368333]
Training Loss (progress: 0.48), 0.16126937860337553, [0.0004512193816377683, 0.002272469817776444, 0.0010862142112163685, 0.0023257040795461995, 0.010905528512307187, 0.006884049599988249], [0.988736621509763, 1.055117719792412, 1.1333523355648167, 1.167469257867746, 1.2916267624831146, 1.162424601773248]
Training Loss (progress: 0.56), 0.16254962089093603, [0.00048735648997124304, 0.0022765325837605203, 0.0011017218319457909, 0.002372182345208765, 0.010885195459171442, 0.0068870942880806], [0.9888580521163011, 1.0552058486555977, 1.1335143243345145, 1.1677475345906019, 1.2920504143533016, 1.1628281832575897]
Training Loss (progress: 0.64), 0.17149398303378183, [0.0004613915768157176, 0.002258824533273346, 0.001110161243394525, 0.0023795106216938, 0.010792462884569958, 0.006751015381336594], [0.9887083183687848, 1.0551285475677517, 1.1337553141647745, 1.1677696278414258, 1.292664647314257, 1.1631491780562473]
Training Loss (progress: 0.72), 0.15611695762415864, [0.0004423737208234415, 0.0022155519854472594, 0.0010820449784047256, 0.002369592233635399, 0.010760083374495329, 0.006804110690521639], [0.9884414302574827, 1.0552403692394006, 1.13347508449542, 1.1680027996136015, 1.2931197093990743, 1.163561082363673]
Training Loss (progress: 0.80), 0.17114162946025038, [0.00046296444788924957, 0.0021987411847395762, 0.001106181175731538, 0.0023228453442289956, 0.01092330629186486, 0.0068770161098104295], [0.9882872121788695, 1.0552012307179706, 1.1334374005630197, 1.168051146965916, 1.2936329381563758, 1.1638815452550657]
Training Loss (progress: 0.88), 0.15122258324911406, [0.0004822213906554954, 0.002178902870077706, 0.0010774875224910442, 0.0023325003560006156, 0.010856608911656521, 0.0067682346443054785], [0.9879508557520145, 1.0550950825537142, 1.1333800958928366, 1.1682869824776632, 1.294223099658921, 1.1642340312619288]
Training Loss (progress: 0.96), 0.16697270274213583, [0.0004750168752758925, 0.002257031873021875, 0.0010650375809895596, 0.002332618350004697, 0.010796176120939251, 0.006844585629569232], [0.9878088009676264, 1.0551019564012885, 1.1332201677462874, 1.1686186664139067, 1.2948398601260784, 1.1646399058138688]
Evaluation on validation dataset:
Step 25, mean loss 0.06432400901828661
Step 50, mean loss 0.029838158416626265
Step 75, mean loss 0.08444347847658397
Step 100, mean loss 0.06855876642565498
Step 125, mean loss 0.08312338193759092
Step 150, mean loss 0.06408743242623285
Step 175, mean loss 0.10426388421884332
Step 200, mean loss 0.15987327819228245
Step 225, mean loss 0.21600518564508267
Unrolled forward losses 2.359640172462854
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 1.04584e-35, 5.37335e-08, 1.33486e-15, 1.29138e-07, 4.13278e-02, 5.33902e-03
Node: 21 (pos: 0.212): 1.74177e-39, 8.75698e-09, 3.25552e-17, 2.28827e-08, 2.84856e-02, 2.98399e-03
Node: 22 (pos: 0.222): 1.85673e-43, 1.30034e-09, 6.56290e-19, 3.71039e-09, 1.92628e-02, 1.61874e-03
Node: 23 (pos: 0.232): 1.26689e-47, 1.75937e-10, 1.09361e-20, 5.50541e-10, 1.27799e-02, 8.52311e-04
Node: 24 (pos: 0.242): 5.53298e-52, 2.16896e-11, 1.50634e-22, 7.47515e-11, 8.31848e-03, 4.35575e-04
Node: 25 (pos: 0.253): 1.54672e-56, 2.43637e-12, 1.71503e-24, 9.28771e-12, 5.31220e-03, 2.16058e-04
-
Node: 26 (pos: 0.263): 2.76756e-61, 2.49361e-13, 1.61404e-26, 1.05598e-12, 3.32825e-03, 1.04021e-04
Node: 27 (pos: 0.273): 3.16968e-66, 2.32546e-14, 1.25559e-28, 1.09866e-13, 2.04583e-03, 4.86086e-05
Node: 28 (pos: 0.283): 2.32362e-71, 1.97599e-15, 8.07372e-31, 1.04600e-14, 1.23378e-03, 2.20470e-05
Node: 29 (pos: 0.293): 1.09030e-76, 1.52988e-16, 4.29131e-33, 9.11288e-16, 7.29986e-04, 9.70574e-06
Node: 30 (pos: 0.303): 3.27461e-82, 1.07925e-17, 1.88538e-35, 7.26508e-17, 4.23745e-04, 4.14716e-06
Node: 31 (pos: 0.313): 6.29514e-88, 6.93721e-19, 6.84696e-38, 5.30010e-18, 2.41327e-04, 1.71995e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 4.18011e-17, 4.06532e-04, 1.16265e-07, 6.47113e-04, 2.58165e-01, 9.36151e-02
Node: 08 (pos: 0.081): 1.10481e-14, 1.30063e-03, 1.25692e-06, 1.96219e-03, 3.27717e-01, 1.35928e-01
Node: 09 (pos: 0.091): 1.86904e-12, 3.79146e-03, 1.12319e-05, 5.44455e-03, 4.08142e-01, 1.91565e-01
Node: 10 (pos: 0.101): 2.02387e-10, 1.00706e-02, 8.29646e-05, 1.38242e-02, 4.98697e-01, 2.62039e-01
Node: 11 (pos: 0.111): 1.40274e-08, 2.43725e-02, 5.06551e-04, 3.21203e-02, 5.97824e-01, 3.47904e-01
Node: 12 (pos: 0.121): 6.22307e-07, 5.37452e-02, 2.55650e-03, 6.82932e-02, 7.03108e-01, 4.48327e-01
-
Node: 00 (pos: 0.000): 1.74177e-39, 8.75698e-09, 3.25552e-17, 2.28827e-08, 2.84856e-02, 2.98399e-03
Node: 01 (pos: 0.010): 1.04584e-35, 5.37335e-08, 1.33486e-15, 1.29138e-07, 4.13278e-02, 5.33902e-03
Node: 02 (pos: 0.020): 4.01951e-32, 3.00422e-07, 4.52420e-14, 6.66901e-07, 5.88264e-02, 9.27189e-03
Node: 03 (pos: 0.030): 9.88807e-29, 1.53043e-06, 1.26748e-12, 3.15157e-06, 8.21511e-02, 1.56285e-02
Node: 04 (pos: 0.040): 1.55698e-25, 7.10380e-06, 2.93515e-11, 1.36286e-05, 1.12555e-01, 2.55688e-02
Node: 05 (pos: 0.051): 1.56923e-22, 3.00444e-05, 5.61840e-10, 5.39306e-05, 1.51297e-01, 4.06019e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.05632805604496631
Step 50, mean loss 0.02516275124868844
Step 75, mean loss 0.04305430553207911
Step 100, mean loss 0.055885758053581286
Step 125, mean loss 0.05507203176912634
Step 150, mean loss 0.14047215188559242
Step 175, mean loss 0.11163612297397728
Step 200, mean loss 0.27378265724940765
Step 225, mean loss 0.12669272282046784
Unrolled forward losses 2.0195302647617295
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00), 0.16922488025331647, [0.0004642564815277803, 0.002190797813231682, 0.001094415828061547, 0.002315940665687963, 0.010828537932080615, 0.006814185716822558], [0.987561425495468, 1.0550645074143838, 1.133287832762205, 1.1686941677099358, 1.2950729841543278, 1.1647464382748978]
Training Loss (progress: 0.08), 0.15749573731149102, [0.00047998378836524116, 0.00222714298383939, 0.0011177386406349616, 0.002317892095077295, 0.010929259405315988, 0.006809855650038938], [0.9875910782916055, 1.0550514640092041, 1.133271514072795, 1.1687103518848314, 1.295662830624808, 1.165099996702216]
Training Loss (progress: 0.16), 0.16495879681165423, [0.00047458330870415137, 0.002170000710533603, 0.0010849345142158102, 0.0023242753242062775, 0.011009968705105901, 0.006913071874877665], [0.9871827465728913, 1.0548538575653779, 1.1331249995922164, 1.1688830554500607, 1.2964591290953196, 1.1654517813436862]
Training Loss (progress: 0.24), 0.1624804865802792, [0.00047881830476414594, 0.0021676256331649695, 0.0011135587790355335, 0.0023557741414370103, 0.01081137343531854, 0.006824237339979733], [0.9869382995489794, 1.0550047685090844, 1.1331917891021277, 1.1689479940720564, 1.2968020189998617, 1.165818517299522]
Training Loss (progress: 0.32), 0.14869681477670066, [0.0004571259718905335, 0.0022180365771534848, 0.001085649567416515, 0.0023304929106813376, 0.010974884106570635, 0.00676914465821087], [0.9866393288806595, 1.0548703567226208, 1.132982991397199, 1.16900959570381, 1.2974571183874766, 1.16592814309118]
Training Loss (progress: 0.40), 0.1614124021777152, [0.00046138195410055254, 0.0022797462007212166, 0.0010930321583132899, 0.0023570164341595134, 0.010900038652569497, 0.0067324092386733545], [0.9866649949598466, 1.054978836972435, 1.133151818182834, 1.1691128874544163, 1.2977746700260584, 1.1662470242489626]
Training Loss (progress: 0.48), 0.16177787088998696, [0.00048067119603456624, 0.002196922719584378, 0.0010660123094936031, 0.0023383035183548646, 0.010907294175603422, 0.006774365646125289], [0.9863958745392184, 1.0547884359386177, 1.1331655446900364, 1.1691931265135571, 1.2982870513809506, 1.1666250813980108]
Training Loss (progress: 0.56), 0.14885756992665422, [0.0004756686845202132, 0.00227591658495631, 0.0010596749046919225, 0.002323997655612152, 0.010930424795218226, 0.006812963642575098], [0.9860655683942486, 1.054893155997016, 1.133315041587548, 1.169317361590839, 1.2989074927871211, 1.166940640721092]
Training Loss (progress: 0.64), 0.17531083067178202, [0.00045543143901584875, 0.0022615608735112014, 0.0011451011696750472, 0.002308759577186452, 0.010744468852464367, 0.006777671642813445], [0.9860079491201237, 1.0550103644446938, 1.1329578541105987, 1.1696262563946518, 1.299204842724822, 1.1672025958286765]
Training Loss (progress: 0.72), 0.14958451445916324, [0.00045689038055530076, 0.0022658263811227582, 0.0010998483600804833, 0.002345207511275865, 0.011055236662708518, 0.006813439032501223], [0.98553990152008, 1.0548806430034052, 1.1330545869622468, 1.1695103196551633, 1.2998750508210972, 1.167627631270224]
Training Loss (progress: 0.80), 0.1443982305539492, [0.0004565695854304928, 0.0022954981056636866, 0.0010591431549260874, 0.002339853128109527, 0.011101208390200719, 0.006774649572065423], [0.9856571118838261, 1.0549378705588255, 1.1329726272418017, 1.169766436578253, 1.3004621406400476, 1.1678671474642683]
Training Loss (progress: 0.88), 0.15481323421633386, [0.00046327780129136687, 0.002298355959868008, 0.0010902586738437913, 0.002288531101096718, 0.011102493853753422, 0.0067665509704541455], [0.9854825030249469, 1.0547756138016176, 1.1330385380983907, 1.169855882715086, 1.30096879906157, 1.168087615033281]
Training Loss (progress: 0.96), 0.16344863469063342, [0.0004740397592126928, 0.0022772324673905188, 0.0010960471451851252, 0.002370010593092062, 0.010978688583053193, 0.006745159255867867], [0.9851010226518391, 1.0548428787798587, 1.1333122302233554, 1.1700195395850126, 1.3012562587315042, 1.1683610377692297]
Evaluation on validation dataset:
Step 25, mean loss 0.053432166467700985
Step 50, mean loss 0.03048894700824751
Step 75, mean loss 0.08162750021514573
Step 100, mean loss 0.066762209294109
Step 125, mean loss 0.07317689887646402
Step 150, mean loss 0.07136933350393068
Step 175, mean loss 0.10198785395591728
Step 200, mean loss 0.1551180739404076
Step 225, mean loss 0.20206921547843607
Unrolled forward losses 2.496472090539122
Unrolled forward base losses 2.565701273852575
Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00), 0.15513320999080976, [0.00044794096627751945, 0.0022029826270441133, 0.0011205761324545225, 0.0023916897449070165, 0.010938480705522054, 0.006822798044130347], [0.9850673908986894, 1.0546233703824262, 1.133214958471346, 1.1700967955838142, 1.3015753201680902, 1.1685366433651072]
Training Loss (progress: 0.08), 0.17311948270628083, [0.00045524858116271057, 0.002277969655235309, 0.001088391391342123, 0.002352983373840769, 0.011013564746703996, 0.006744770771789542], [0.9849448533310292, 1.0547904373562296, 1.1333426823583252, 1.1703137750577313, 1.3021120657323273, 1.1688676412253367]
Training Loss (progress: 0.16), 0.17462345274154153, [0.0004718208747685953, 0.002209013075021613, 0.0011121481740652593, 0.0023293002996040636, 0.011089354301574993, 0.006828967353725659], [0.9847740528314728, 1.054766721796066, 1.133405535514197, 1.1704287001642852, 1.302870445007451, 1.169245116427335]
Training Loss (progress: 0.24), 0.16729912705847336, [0.0004420781559463785, 0.002229782359208953, 0.0010668088216291595, 0.0023768148773528415, 0.010936002716877766, 0.006775601838592748], [0.9843862541261478, 1.0545640757121584, 1.1330633031436415, 1.1707221443908316, 1.3033353128025595, 1.1696079654544895]
Training Loss (progress: 0.32), 0.15972705523468927, [0.00048161708493790924, 0.002191931571873413, 0.0010816651943788725, 0.0023095184613557878, 0.011053606549099856, 0.006756405430706872], [0.9843391507767588, 1.0545941311046438, 1.1330612217626104, 1.1706336674981817, 1.3037520452925864, 1.169886126763088]
Training Loss (progress: 0.40), 0.13651593470748624, [0.00046373703476968635, 0.0022706673827621623, 0.0010376254659806106, 0.002308066015226946, 0.010974706161416544, 0.00673315495581049], [0.9840214438413482, 1.0546036763686109, 1.1330909127759226, 1.170630814855293, 1.3040942769043173, 1.170035613440992]
Training Loss (progress: 0.48), 0.1616904808488298, [0.00048165936600451793, 0.0022609635175748406, 0.0010997943044184903, 0.0022944210619266735, 0.01112667015346258, 0.006625063910918071], [0.9836771611362862, 1.0544942764610004, 1.1331034674099105, 1.1706803738206932, 1.3046633495733568, 1.1702398275423087]
Training Loss (progress: 0.56), 0.15282949474825064, [0.00045205743546534533, 0.0022858737911043646, 0.0010922724631618242, 0.0023280335696745946, 0.010992971450519435, 0.006739602193647741], [0.9834947468986729, 1.054517685934908, 1.1331264669054, 1.17074190038661, 1.3050917854293747, 1.1706355324951851]
Training Loss (progress: 0.64), 0.16356464316133118, [0.00047157189703688217, 0.0022375273008681047, 0.0010849358351446604, 0.002326165009686804, 0.011089636848557521, 0.0067669672107270595], [0.9836118641934317, 1.0544443955883438, 1.1330996325845772, 1.1708147784622291, 1.3055496700290166, 1.1708955319693621]
Training Loss (progress: 0.72), 0.15404856621126897, [0.00047046666514703354, 0.00226147938093601, 0.0010844825415665272, 0.0023128767674841953, 0.011005626553784599, 0.0067313877246182735], [0.9833424015592201, 1.0544508510850656, 1.1330978379584695, 1.1710611138910112, 1.3059971154937184, 1.1711598425863097]
Training Loss (progress: 0.80), 0.15120922147736252, [0.00045394501160230763, 0.002287120790393313, 0.0010962200013056576, 0.002392564178864502, 0.011100108183511464, 0.006762137301132012], [0.9830202979771132, 1.0543484234121387, 1.1328054629112454, 1.1711989893069565, 1.3062923634453918, 1.1714641965174633]
Training Loss (progress: 0.88), 0.157142442684181, [0.00046730718530478305, 0.0023343910095712296, 0.0010973787132816583, 0.0023541689261568546, 0.011064923102640322, 0.006675338091317011], [0.9827007966216879, 1.0541578404474647, 1.132918741562136, 1.1713037105761834, 1.3065986735995978, 1.171684597651964]
Training Loss (progress: 0.96), 0.16354882433668588, [0.00048358986637965387, 0.0022528573467923622, 0.0011009217662598192, 0.0023689905818043587, 0.011178536099812099, 0.006686753540645174], [0.9824988164469443, 1.0541396870096864, 1.1328594787012973, 1.1713194592041238, 1.3072919897632322, 1.1719819656553714]
Evaluation on validation dataset:
Step 25, mean loss 0.04825117780726462
Step 50, mean loss 0.030706877363736167
Step 75, mean loss 0.08138591669863904
Step 100, mean loss 0.06698372496084623
Step 125, mean loss 0.06960282250311259
Step 150, mean loss 0.06248745418372849
Step 175, mean loss 0.09776047155451159
Step 200, mean loss 0.15349606658985537
Step 225, mean loss 0.19512023508306556
Unrolled forward losses 2.1849117032631664
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 9.59346e-36, 1.02431e-07, 3.58118e-15, 2.17897e-07, 4.77841e-02, 5.12295e-03
Node: 21 (pos: 0.212): 1.58376e-39, 1.79000e-08, 9.71691e-17, 4.08454e-08, 3.34212e-02, 2.84852e-03
Node: 22 (pos: 0.222): 1.67278e-43, 2.86040e-09, 2.19128e-18, 7.02663e-09, 2.29508e-02, 1.53690e-03
Node: 23 (pos: 0.232): 1.13039e-47, 4.17974e-10, 4.10708e-20, 1.10934e-09, 1.54744e-02, 8.04641e-04
Node: 24 (pos: 0.242): 4.88709e-52, 5.58500e-11, 6.39787e-22, 1.60728e-10, 1.02439e-02, 4.08776e-04
Node: 25 (pos: 0.253): 1.35179e-56, 6.82411e-12, 8.28333e-24, 2.13714e-11, 6.65817e-03, 2.01511e-04
-
Node: 26 (pos: 0.263): 2.39226e-61, 7.62464e-13, 8.91335e-26, 2.60787e-12, 4.24896e-03, 9.63913e-05
Node: 27 (pos: 0.273): 2.70858e-66, 7.79009e-14, 7.97157e-28, 2.92046e-13, 2.66225e-03, 4.47410e-05
Node: 28 (pos: 0.283): 1.96206e-71, 7.27806e-15, 5.92534e-30, 3.00143e-14, 1.63777e-03, 2.01513e-05
Node: 29 (pos: 0.293): 9.09323e-77, 6.21782e-16, 3.66058e-32, 2.83086e-15, 9.89221e-04, 8.80697e-06
Node: 30 (pos: 0.303): 2.69626e-82, 4.85748e-17, 1.87955e-34, 2.45031e-16, 5.86642e-04, 3.73490e-06
Node: 31 (pos: 0.313): 5.11495e-88, 3.47004e-18, 8.02090e-37, 1.94642e-17, 3.41578e-04, 1.53695e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 4.00370e-17, 5.49587e-04, 1.84508e-07, 8.27683e-04, 2.77746e-01, 9.21333e-02
Node: 08 (pos: 0.081): 1.06415e-14, 1.68137e-03, 1.86286e-06, 2.42080e-03, 3.49280e-01, 1.34219e-01
Node: 09 (pos: 0.091): 1.80960e-12, 4.70369e-03, 1.56319e-05, 6.49781e-03, 4.31260e-01, 1.89732e-01
Node: 10 (pos: 0.101): 1.96878e-10, 1.20327e-02, 1.09021e-04, 1.60062e-02, 5.22807e-01, 2.60253e-01
Node: 11 (pos: 0.111): 1.37041e-08, 2.81475e-02, 6.31942e-04, 3.61843e-02, 6.22274e-01, 3.46400e-01
Node: 12 (pos: 0.121): 6.10294e-07, 6.02096e-02, 3.04445e-03, 7.50697e-02, 7.27209e-01, 4.47392e-01
-
Node: 00 (pos: 0.000): 1.58376e-39, 1.79000e-08, 9.71691e-17, 4.08454e-08, 3.34212e-02, 2.84852e-03
Node: 01 (pos: 0.010): 9.59346e-36, 1.02431e-07, 3.58118e-15, 2.17897e-07, 4.77841e-02, 5.12295e-03
Node: 02 (pos: 0.020): 3.71790e-32, 5.35989e-07, 1.09696e-13, 1.06678e-06, 6.70784e-02, 8.94024e-03
Node: 03 (pos: 0.030): 9.21844e-29, 2.56467e-06, 2.79269e-12, 4.79299e-06, 9.24528e-02, 1.51393e-02
Node: 04 (pos: 0.040): 1.46236e-25, 1.12217e-05, 5.90908e-11, 1.97630e-05, 1.25111e-01, 2.48766e-02
Node: 05 (pos: 0.051): 1.48418e-22, 4.48988e-05, 1.03917e-09, 7.47843e-05, 1.66230e-01, 3.96648e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.039558524625262916
Step 50, mean loss 0.02114080403582108
Step 75, mean loss 0.03659484339217345
Step 100, mean loss 0.05128438521733974
Step 125, mean loss 0.053451666251494
Step 150, mean loss 0.1072689936797441
Step 175, mean loss 0.09321457968233798
Step 200, mean loss 0.26070990944730893
Step 225, mean loss 0.13850242366331714
Unrolled forward losses 1.7292814117010489
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00), 0.15544328621882564, [0.0004881972253066834, 0.0023168646516409375, 0.0010725941848864543, 0.0023606715959818302, 0.011010770481160653, 0.006742640189443777], [0.982607028347534, 1.0541779573761225, 1.1328118460890766, 1.1713346079233997, 1.3073208437973838, 1.1721585261798062]
Training Loss (progress: 0.08), 0.16030891258454183, [0.00044604583333505635, 0.002303978365326933, 0.0010611791044204725, 0.0023392660097979116, 0.011209944484930179, 0.00674151912756707], [0.982225015039704, 1.0542479259897317, 1.13281258379562, 1.1714488921465274, 1.3079839171082888, 1.1725040487231846]
Training Loss (progress: 0.16), 0.15037363008380336, [0.00046937410460428053, 0.00222218633895237, 0.0011049580923309752, 0.002320369097579537, 0.011145884084980776, 0.006733430009769446], [0.9821015095013759, 1.0542804103584706, 1.132655029069645, 1.171536361164928, 1.3083719908993603, 1.1727662425219154]
Training Loss (progress: 0.24), 0.15554875087149073, [0.00047098721289495977, 0.002293294298495643, 0.0010774767626770172, 0.002419403129736768, 0.010988167875252124, 0.006734566726158356], [0.9818680085436562, 1.0543160035672228, 1.1328493995625681, 1.1717604060874058, 1.3088537214218892, 1.1729946663211595]
Training Loss (progress: 0.32), 0.1452429648592018, [0.0004859797860577253, 0.0022550701252455198, 0.0010919840542713778, 0.002326665300196859, 0.011120093439378776, 0.006637983632552614], [0.9815851404075482, 1.0541901669584424, 1.1327268620324824, 1.1717550723034762, 1.309537985650496, 1.1732580722819006]
Training Loss (progress: 0.40), 0.14588439583050536, [0.00046169773381425736, 0.0021786497883611515, 0.0010615456809875952, 0.00237598560201184, 0.011141357056241648, 0.006722366896645351], [0.9811925626666509, 1.0540003248112362, 1.1327683760202412, 1.1718506733451586, 1.3097389610443821, 1.1735548439211232]
Training Loss (progress: 0.48), 0.14605591027904335, [0.00046442348981233896, 0.002284659630371313, 0.0010854304859393455, 0.0023889357869369917, 0.011102856951056847, 0.0067250622769151], [0.9812376251269609, 1.0540946300224296, 1.1327290635404845, 1.171933724858909, 1.3103379487311697, 1.1738001436640677]
Training Loss (progress: 0.56), 0.14973088118255531, [0.0004609999474654851, 0.0022713586056216585, 0.001051108084419288, 0.002349073993042852, 0.011107857560026304, 0.006782878785610554], [0.9809014459131202, 1.054092512024668, 1.1325725300199354, 1.1719189175546758, 1.3107831224490076, 1.1740905083747555]
Training Loss (progress: 0.64), 0.15602013234669732, [0.00046259039091367147, 0.0022799085359096797, 0.0010954978582108478, 0.0023210969532291756, 0.01103759799750692, 0.00668022869206993], [0.9806587386785073, 1.0539815004457005, 1.1325460878006575, 1.1720809432804242, 1.3112594209896895, 1.1743688609190606]
Training Loss (progress: 0.72), 0.13685170442021946, [0.0004548194554809926, 0.002320935536253098, 0.0011250760481987725, 0.00234081974497537, 0.01125414296621546, 0.006633103345426491], [0.9804670086356764, 1.0537586038720648, 1.1327533158528207, 1.172177771397427, 1.3117792939112214, 1.174593692356119]
Training Loss (progress: 0.80), 0.14532262367111612, [0.0004670256297623334, 0.0022519295041590953, 0.0010944858131123772, 0.0022796338732505948, 0.01095972085611002, 0.00677683269099971], [0.9802489691251622, 1.0539517259456448, 1.1326155504297812, 1.1722764610083127, 1.3121838819821656, 1.1749290770871903]
Training Loss (progress: 0.88), 0.1565698435114326, [0.0004815403638950541, 0.002241540367326731, 0.001111204550714101, 0.0023481643657133466, 0.010965727672133217, 0.006717318313899566], [0.9801741122336308, 1.0537841622241848, 1.1325820080145979, 1.1724468117996591, 1.3124384569928018, 1.1751408898345923]
Training Loss (progress: 0.96), 0.15270534098101896, [0.0004649812268010554, 0.0022953739296568167, 0.0010998343802336521, 0.002300848702772803, 0.011469671055270598, 0.006666475388314467], [0.9799506658000952, 1.0539676499753905, 1.1326684032095662, 1.1725630845426587, 1.312975133782626, 1.17535736231387]
Evaluation on validation dataset:
Step 25, mean loss 0.050194396042898964
Step 50, mean loss 0.03174795735765227
Step 75, mean loss 0.08708081198487214
Step 100, mean loss 0.06398672603377446
Step 125, mean loss 0.06920196764991074
Step 150, mean loss 0.06864868234646555
Step 175, mean loss 0.10627622038379647
Step 200, mean loss 0.1592532727464639
Step 225, mean loss 0.2155495592209875
Unrolled forward losses 2.6647516432018374
Unrolled forward base losses 2.565701273852575
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00), 0.14315294567473796, [0.0004545455169709404, 0.002284360409203075, 0.0010889767133911304, 0.0023568987670553724, 0.011240020862803474, 0.006725984300189906], [0.9797487976142037, 1.0539094291100513, 1.132534155342887, 1.172585225737928, 1.313335876375678, 1.1755405919380713]
Training Loss (progress: 0.08), 0.14140564155603239, [0.000459161758840318, 0.0022466984163888744, 0.0010676286114416893, 0.0023290520295354894, 0.011136153092220285, 0.006708031716475016], [0.9797321603560631, 1.0538160800066094, 1.1325191808999653, 1.172725114371448, 1.3133859881046612, 1.1757320714903206]
Training Loss (progress: 0.16), 0.1411743974519434, [0.00045681877759047633, 0.0022752921891320173, 0.0010811754718486638, 0.0023328286695258825, 0.011144092680542105, 0.006713028188432824], [0.9796421068391525, 1.0538283591967814, 1.1325437417509059, 1.1727895107871025, 1.3136950424600176, 1.1759117134420294]
Training Loss (progress: 0.24), 0.14683593950949148, [0.0004534389466884895, 0.002265261858285705, 0.001084760350569261, 0.0023086590936609564, 0.011213001306324105, 0.006733845630431131], [0.9795659441338849, 1.0538263908128649, 1.1325672746516522, 1.1728829157962428, 1.3139915822670853, 1.1760918447715765]
Training Loss (progress: 0.32), 0.148181154384194, [0.0004675556595115687, 0.0022361221273856317, 0.001090941458491787, 0.0023342078641389024, 0.01117656328964011, 0.006730023540059258], [0.9795107753808492, 1.05377875502434, 1.1325057989155989, 1.1730582006128267, 1.3142800545401014, 1.1762659068285994]
Training Loss (progress: 0.40), 0.13227207638436236, [0.0004557700366568375, 0.0022692915133027424, 0.0010858130633859555, 0.0023252830500342606, 0.011194658781969366, 0.006684172593764047], [0.9794972289115411, 1.0538610135133257, 1.132549730104915, 1.173199074198086, 1.3144903046240444, 1.1764253862339193]
Training Loss (progress: 0.48), 0.13098083039575018, [0.0004569702427298047, 0.002293644801375621, 0.001091095174558914, 0.0023007140074251223, 0.011168580677124028, 0.006704811330733707], [0.9794765044446122, 1.053782668229967, 1.132465497160898, 1.1732327581920068, 1.3147827376119017, 1.1766107200894025]
Training Loss (progress: 0.56), 0.13660631029293144, [0.0004605023097310028, 0.002257763472081376, 0.0010914933920590961, 0.0022932469519050076, 0.0111121746045857, 0.006709159518989233], [0.979418401451908, 1.0538671542607274, 1.1323885928025905, 1.173369160279649, 1.3149906893414705, 1.1767739288963972]
Training Loss (progress: 0.64), 0.1368762680027063, [0.0004554036731786473, 0.0022538621676644085, 0.0010881065469394223, 0.002337851765811683, 0.011098312346267661, 0.006633782759605724], [0.9793657747898714, 1.0538140689329152, 1.1323775664010138, 1.173429639496912, 1.3151846863726795, 1.1769048890371443]
Training Loss (progress: 0.72), 0.14591237683490885, [0.00044218034185364557, 0.0022725247041390424, 0.0010781866598113265, 0.0023366635070399557, 0.011250528934688354, 0.006696908505674559], [0.9791839699029999, 1.0538191833758497, 1.1323773067416538, 1.173517553317221, 1.315553327246203, 1.1770696732141184]
Training Loss (progress: 0.80), 0.1359636402293087, [0.0004617767480137558, 0.0022382057935596256, 0.0010951199431043167, 0.0023455128729528967, 0.011165415418166775, 0.00669071620999107], [0.9791415849386376, 1.053824915957892, 1.1323518407699704, 1.1736203329805412, 1.3157549712406826, 1.1772575468709672]
Training Loss (progress: 0.88), 0.14554007553730614, [0.0004564406861257575, 0.002255429642720801, 0.0010995503581048185, 0.002346049465038738, 0.011261059804426843, 0.006715685818435455], [0.9790867559363008, 1.0538468562161656, 1.1323734716090934, 1.173717136746702, 1.3160388495152409, 1.1774472329151553]
Training Loss (progress: 0.96), 0.13475976376147863, [0.0004527483268279508, 0.0022673430594178107, 0.0011280703512709974, 0.002353816603971513, 0.0111854450928153, 0.006690722085777627], [0.9790916752702686, 1.0538370529931793, 1.1324495241464478, 1.1738228737087821, 1.3162636771258434, 1.177601698582044]
Evaluation on validation dataset:
Step 25, mean loss 0.04338870418558402
Step 50, mean loss 0.026270369724893654
Step 75, mean loss 0.07137830690868198
Step 100, mean loss 0.0563863717686299
Step 125, mean loss 0.058286001427600875
Step 150, mean loss 0.05372853753077686
Step 175, mean loss 0.085008087271714
Step 200, mean loss 0.13908391762762462
Step 225, mean loss 0.18016032184441022
Unrolled forward losses 1.9927459541232468
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 1.91184e-36, 9.91281e-08, 2.56092e-15, 1.23963e-07, 4.89304e-02, 4.58973e-03
Node: 21 (pos: 0.212): 2.65250e-40, 1.72621e-08, 6.70170e-17, 2.18587e-08, 3.42857e-02, 2.52063e-03
Node: 22 (pos: 0.222): 2.33359e-44, 2.74828e-09, 1.45491e-18, 3.52617e-09, 2.35898e-02, 1.34241e-03
Node: 23 (pos: 0.232): 1.30184e-48, 4.00037e-10, 2.62027e-20, 5.20396e-10, 1.59374e-02, 6.93286e-04
Node: 24 (pos: 0.242): 4.60528e-53, 5.32367e-11, 3.91488e-22, 7.02609e-11, 1.05727e-02, 3.47210e-04
Node: 25 (pos: 0.253): 1.03304e-57, 6.47730e-12, 4.85234e-24, 8.67850e-12, 6.88708e-03, 1.68626e-04
-
Node: 26 (pos: 0.263): 1.46941e-62, 7.20523e-13, 4.98937e-26, 9.80676e-13, 4.40517e-03, 7.94164e-05
Node: 27 (pos: 0.273): 1.32536e-67, 7.32780e-14, 4.25599e-28, 1.01381e-13, 2.76674e-03, 3.62700e-05
Node: 28 (pos: 0.283): 7.58037e-73, 6.81351e-15, 3.01173e-30, 9.58822e-15, 1.70629e-03, 1.60634e-05
Node: 29 (pos: 0.293): 2.74922e-78, 5.79215e-16, 1.76804e-32, 8.29602e-16, 1.03328e-03, 6.89890e-06
Node: 30 (pos: 0.303): 6.32257e-84, 4.50174e-17, 8.61055e-35, 6.56677e-17, 6.14410e-04, 2.87326e-06
Node: 31 (pos: 0.313): 9.22023e-90, 3.19884e-18, 3.47880e-37, 4.75536e-18, 3.58739e-04, 1.16044e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 1.87790e-17, 5.41157e-04, 1.57667e-07, 6.36313e-04, 2.81855e-01, 8.77308e-02
Node: 08 (pos: 0.081): 5.57981e-15, 1.65931e-03, 1.62922e-06, 1.93550e-03, 3.54033e-01, 1.28824e-01
Node: 09 (pos: 0.091): 1.05131e-12, 4.65163e-03, 1.39662e-05, 5.38598e-03, 4.36655e-01, 1.83440e-01
Node: 10 (pos: 0.101): 1.25604e-10, 1.19221e-02, 9.93204e-05, 1.37116e-02, 5.28826e-01, 2.53304e-01
Node: 11 (pos: 0.111): 9.51573e-09, 2.79366e-02, 5.85948e-04, 3.19345e-02, 6.28876e-01, 3.39191e-01
Node: 12 (pos: 0.121): 4.57136e-07, 5.98501e-02, 2.86775e-03, 6.80429e-02, 7.34339e-01, 4.40452e-01
-
Node: 00 (pos: 0.000): 2.65250e-40, 1.72621e-08, 6.70170e-17, 2.18587e-08, 3.42857e-02, 2.52063e-03
Node: 01 (pos: 0.010): 1.91184e-36, 9.91281e-08, 2.56092e-15, 1.23963e-07, 4.89304e-02, 4.58973e-03
Node: 02 (pos: 0.020): 8.73796e-33, 5.20442e-07, 8.11835e-14, 6.43153e-07, 6.85683e-02, 8.10432e-03
Node: 03 (pos: 0.030): 2.53241e-29, 2.49816e-06, 2.13501e-12, 3.05271e-06, 9.43510e-02, 1.38771e-02
Node: 04 (pos: 0.040): 4.65396e-26, 1.09632e-05, 4.65795e-11, 1.32558e-05, 1.27482e-01, 2.30427e-02
Node: 05 (pos: 0.051): 5.42344e-23, 4.39874e-05, 8.43043e-10, 5.26596e-05, 1.69133e-01, 3.71040e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03722558969177459
Step 50, mean loss 0.018236731521715215
Step 75, mean loss 0.032164215053669086
Step 100, mean loss 0.04368817558383993
Step 125, mean loss 0.046655740751556915
Step 150, mean loss 0.10465249928926257
Step 175, mean loss 0.08507812999659442
Step 200, mean loss 0.2436242470976575
Step 225, mean loss 0.11660094207470395
Unrolled forward losses 1.5406965226010376
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00), 0.13772182117061937, [0.00045755094484303306, 0.002284649679693238, 0.0010810027704305277, 0.002346555440557344, 0.011155791668258714, 0.0066478510798416044], [0.9790439792987623, 1.0538378041842384, 1.1324174192565999, 1.1737575803569673, 1.3163682918693362, 1.1776762948029487]
Training Loss (progress: 0.08), 0.1243687845449808, [0.0004704916614554856, 0.0022614663053684187, 0.0010811014742495323, 0.002318023344323271, 0.011171494587091966, 0.006691040764973244], [0.9788921825339473, 1.0537739840475655, 1.132301053765719, 1.1738353566451891, 1.3165797199051055, 1.177852358652413]
Training Loss (progress: 0.16), 0.13460943050703364, [0.000463399355829348, 0.002293579677874501, 0.0011006549759049055, 0.00227266329243482, 0.011157527292354398, 0.006686307181651926], [0.9788656295160907, 1.0538052178362525, 1.1323930151912995, 1.173817808074319, 1.3168197478255306, 1.1780636658589987]
Training Loss (progress: 0.24), 0.12882169313830685, [0.00045839848391757425, 0.002233277229007706, 0.0011041777935075826, 0.002306696815822777, 0.011232109686672867, 0.006618103369842947], [0.9787310868877734, 1.0537963371560766, 1.1322721766652148, 1.1739131620034473, 1.3171459137978463, 1.1781840639730656]
Training Loss (progress: 0.32), 0.13561392203091607, [0.000464445536760131, 0.0022670633225690216, 0.0010878974684239014, 0.0023100906131040726, 0.011159472241333995, 0.006643672771860255], [0.978779458020646, 1.0537899656820497, 1.1322864881033108, 1.173961801351485, 1.3174038807371382, 1.1782855912124777]
Training Loss (progress: 0.40), 0.13590167775150996, [0.0004562316757013771, 0.0022446080161030156, 0.0010994059754509211, 0.002325921896458916, 0.0111147808823627, 0.0066473267168478045], [0.9786998641296446, 1.0539048748887305, 1.1322749631755862, 1.1740193257734628, 1.3175470267714386, 1.1784636206602361]
Training Loss (progress: 0.48), 0.1309556427875811, [0.0004602237542650721, 0.0023031184527323415, 0.001073521867213073, 0.0023340803880328087, 0.011156979196323163, 0.006658005146510354], [0.9785732735361974, 1.0538220468334265, 1.1322628005515294, 1.1741576842108685, 1.3178118430943957, 1.1786104199273144]
Training Loss (progress: 0.56), 0.14354506949327858, [0.00047155409674506357, 0.002260080820641103, 0.0010832305443722902, 0.0023481270573644914, 0.01120991559573718, 0.006637419664821544], [0.9786112741586047, 1.0538964934748514, 1.1322957360126569, 1.1742540015671787, 1.3180575394927516, 1.1787603937492777]
Training Loss (progress: 0.64), 0.1344666368691097, [0.000453734599248408, 0.00226100964213289, 0.001095626244699267, 0.0023444704356588705, 0.011162536089321955, 0.006675422434117518], [0.9784793098316235, 1.053867825959384, 1.1322828364794042, 1.17429207700428, 1.3183555424819051, 1.1789286759668907]
Training Loss (progress: 0.72), 0.14266610572141236, [0.0004529155440689334, 0.002250238647393553, 0.001098667397177394, 0.002338578172813628, 0.011135606962789701, 0.006654735999143891], [0.9784121545005448, 1.053900788129507, 1.1323057151243325, 1.1743757760625295, 1.3186915523676994, 1.1790831265046284]
Training Loss (progress: 0.80), 0.1366644805597433, [0.0004588834355241582, 0.0022373605542300367, 0.0011141178447162594, 0.0023011397470761464, 0.01116146355079151, 0.006631588840101634], [0.9783747179003356, 1.0538474237501445, 1.1321803329823243, 1.1744044664402817, 1.3189342177972827, 1.179251226797372]
Training Loss (progress: 0.88), 0.13947561118654245, [0.00046679196167668597, 0.0022744425973357076, 0.0010845336950885028, 0.0023321861616054486, 0.011068644208060566, 0.006637748524023806], [0.9783847491090957, 1.053936348177674, 1.1321980931317985, 1.1744345994581786, 1.319172999277319, 1.1794231698455198]
Training Loss (progress: 0.96), 0.13008441195762152, [0.00046425208373123875, 0.002293977351658839, 0.0010990524462111016, 0.0023065381480581867, 0.011144115525126723, 0.006593053966838496], [0.9782561956094634, 1.0538691422513844, 1.132191717639337, 1.1744780467725018, 1.3194310332818422, 1.1795729937358892]
Evaluation on validation dataset:
Step 25, mean loss 0.045657491679294504
Step 50, mean loss 0.025430557988208948
Step 75, mean loss 0.07073125151487568
Step 100, mean loss 0.05484280080239538
Step 125, mean loss 0.057178227786042216
Step 150, mean loss 0.05358612646513936
Step 175, mean loss 0.08701587033764271
Step 200, mean loss 0.13444203718060177
Step 225, mean loss 0.1784919479780479
Unrolled forward losses 2.0041473471389875
Unrolled forward base losses 2.565701273852575
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00), 0.13647925486840268, [0.00045445003786199773, 0.002293589031377989, 0.0011030843431220692, 0.002332598821336704, 0.011157203246005113, 0.006622552224657748], [0.9781665436414924, 1.0539099947139143, 1.1322162960700557, 1.1745127510556554, 1.3195497799817983, 1.1796406772073302]
Training Loss (progress: 0.08), 0.12614216712291107, [0.00044134757513459314, 0.0022593037323364058, 0.0011122252313900028, 0.002309662578496947, 0.01114179782050804, 0.006625485739738389], [0.9780384904657231, 1.0539461335053504, 1.1321797052057674, 1.1745646928248539, 1.3197961198421246, 1.179803888891125]
Training Loss (progress: 0.16), 0.14237255070122587, [0.00046481966411844323, 0.0022849604699066873, 0.0010849707206112608, 0.0023127678314773864, 0.011237719378537214, 0.006658764208383043], [0.977966330456648, 1.0539412928096963, 1.1321465461646407, 1.1746328382145534, 1.3201581697785436, 1.1799192524928308]
Training Loss (progress: 0.24), 0.13537531090843744, [0.0004603082789095158, 0.002217902796925506, 0.0010875607308896726, 0.0023067324673438304, 0.011166120952898846, 0.006680500156130379], [0.9779208969004718, 1.0538642664950577, 1.1321716304162182, 1.174707491974406, 1.3203480923388373, 1.1801126759567873]
Training Loss (progress: 0.32), 0.11845203702784815, [0.0004575188347185603, 0.0022558013528784756, 0.001103659504853189, 0.002312711358464291, 0.011269010785826103, 0.0066540994565838785], [0.9778342698700606, 1.0538703708160637, 1.1322325485183211, 1.1748326586658584, 1.3205654528189643, 1.1802160935800086]
Training Loss (progress: 0.40), 0.14084718200314253, [0.00045394019834336404, 0.002303700898370349, 0.001091757191579843, 0.002307297160283739, 0.011218890390335478, 0.006632452691898613], [0.9777542531413023, 1.0539116132687962, 1.1320760376049719, 1.1748855906493687, 1.3207880756021653, 1.1803417517325598]
Training Loss (progress: 0.48), 0.13350437022116587, [0.0004476858969110522, 0.0022683956495241284, 0.001085168266392099, 0.002332426395216303, 0.011074760498623796, 0.006660356978612839], [0.977765057000614, 1.0539296891886096, 1.132205991633491, 1.1748567921305741, 1.320997408481643, 1.1805314452465554]
Training Loss (progress: 0.56), 0.13960884963938225, [0.00046272524032031604, 0.0022882692648447385, 0.001074345737238476, 0.0023293564637375764, 0.01121882905931734, 0.0066434303470087825], [0.9776357357100096, 1.0538865836991684, 1.1320841192060578, 1.174920046497048, 1.3213837629109926, 1.180674196347913]
Training Loss (progress: 0.64), 0.13130345872514335, [0.00045593962188394163, 0.0022859722417936973, 0.001072782432984677, 0.0023250041687439627, 0.011166140875588228, 0.006607307571243515], [0.9775310177599348, 1.0538876894820917, 1.1320827370564845, 1.1749266381744556, 1.3215224214606118, 1.1807564576557021]
Training Loss (progress: 0.72), 0.13529704820321012, [0.0004680004138524298, 0.002297457686543338, 0.0010939624245543905, 0.002338605923132925, 0.01110364614488373, 0.00658899386521384], [0.9773750051927695, 1.0539139488567637, 1.1321144183675749, 1.1749973171983348, 1.321698880430193, 1.1809629978968332]
Training Loss (progress: 0.80), 0.1344812582780957, [0.0004484757641098678, 0.0022932804001228736, 0.0011012354557898482, 0.0023277721538738578, 0.011190407570648007, 0.0066401354867462094], [0.9774351629439464, 1.0538951710264026, 1.1320945384613765, 1.175110495822308, 1.3219718683062516, 1.1811523424488901]
Training Loss (progress: 0.88), 0.14529295040463508, [0.00045467931381463323, 0.0022962850537233586, 0.0010806445545316074, 0.0023210931325386847, 0.011186779907945558, 0.006624357170610072], [0.9773957480196559, 1.053924765446647, 1.1320952120907153, 1.1750919174751395, 1.322154351936832, 1.1812217828865363]
Training Loss (progress: 0.96), 0.1327777083219025, [0.000471155059262408, 0.0023079145154681078, 0.0010855009948738416, 0.0023690143006631793, 0.011231581272005721, 0.006623057577399246], [0.9772850099592492, 1.053867816163373, 1.1320299139192058, 1.175242222357448, 1.3224850646539652, 1.1813576772758791]
Evaluation on validation dataset:
Step 25, mean loss 0.040768421152568235
Step 50, mean loss 0.02371121898998428
Step 75, mean loss 0.06992539503054203
Step 100, mean loss 0.05200663057727077
Step 125, mean loss 0.05935567403644601
Step 150, mean loss 0.05297639424795275
Step 175, mean loss 0.08221896700098345
Step 200, mean loss 0.13404921647680995
Step 225, mean loss 0.1790008495358
Unrolled forward losses 1.9559261579266252
Unrolled forward base losses 2.565701273852575
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 20 (pos: 0.202): 4.03326e-35, 8.61348e-08, 3.88381e-15, 1.28710e-07, 4.89924e-02, 4.57210e-03
Node: 21 (pos: 0.212): 7.78043e-39, 1.47734e-08, 1.06316e-16, 2.27848e-08, 3.43166e-02, 2.50904e-03
Node: 22 (pos: 0.222): 9.67957e-43, 2.31480e-09, 2.41993e-18, 3.69075e-09, 2.36021e-02, 1.33517e-03
Node: 23 (pos: 0.232): 7.76628e-47, 3.31346e-10, 4.58005e-20, 5.47044e-10, 1.59392e-02, 6.88967e-04
Node: 24 (pos: 0.242): 4.01861e-51, 4.33294e-11, 7.20778e-22, 7.41937e-11, 1.05695e-02, 3.44744e-04
Node: 25 (pos: 0.253): 1.34104e-55, 5.17627e-12, 9.43184e-24, 9.20768e-12, 6.88195e-03, 1.67275e-04
-
Node: 26 (pos: 0.263): 2.88612e-60, 5.64918e-13, 1.02625e-25, 1.04561e-12, 4.39987e-03, 7.87048e-05
Node: 27 (pos: 0.273): 4.00582e-65, 5.63231e-14, 9.28490e-28, 1.08650e-13, 2.76210e-03, 3.59092e-05
Node: 28 (pos: 0.283): 3.58570e-70, 5.13004e-15, 6.98495e-30, 1.03306e-14, 1.70258e-03, 1.58872e-05
Node: 29 (pos: 0.293): 2.06996e-75, 4.26863e-16, 4.36931e-32, 8.98787e-16, 1.03050e-03, 6.81590e-06
Node: 30 (pos: 0.303): 7.70643e-81, 3.24482e-17, 2.27261e-34, 7.15530e-17, 6.12433e-04, 2.83553e-06
Node: 31 (pos: 0.313): 1.85034e-86, 2.25333e-18, 9.82884e-37, 5.21238e-18, 3.57387e-04, 1.14388e-06
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 07 (pos: 0.071): 7.81948e-17, 5.06727e-04, 1.91580e-07, 6.48045e-04, 2.82720e-01, 8.77219e-02
Node: 08 (pos: 0.081): 1.88090e-14, 1.56894e-03, 1.92334e-06, 1.96624e-03, 3.55202e-01, 1.28874e-01
Node: 09 (pos: 0.091): 2.91782e-12, 4.43785e-03, 1.60555e-05, 5.45888e-03, 4.38192e-01, 1.83593e-01
Node: 10 (pos: 0.101): 2.91914e-10, 1.14676e-02, 1.11444e-04, 1.38679e-02, 5.30791e-01, 2.53620e-01
Node: 11 (pos: 0.111): 1.88347e-08, 2.70711e-02, 6.43209e-04, 3.22369e-02, 6.31326e-01, 3.39740e-01
Node: 12 (pos: 0.121): 7.83729e-07, 5.83810e-02, 3.08682e-03, 6.85699e-02, 7.37316e-01, 4.41311e-01
-
Node: 00 (pos: 0.000): 7.78043e-39, 1.47734e-08, 1.06316e-16, 2.27848e-08, 3.43166e-02, 2.50904e-03
Node: 01 (pos: 0.010): 4.03326e-35, 8.61348e-08, 3.88381e-15, 1.28710e-07, 4.89924e-02, 4.57210e-03
Node: 02 (pos: 0.020): 1.34839e-31, 4.58787e-07, 1.17973e-13, 6.65299e-07, 6.86790e-02, 8.07902e-03
Node: 03 (pos: 0.030): 2.90721e-28, 2.23243e-06, 2.97967e-12, 3.14673e-06, 9.45343e-02, 1.38432e-02
Node: 04 (pos: 0.040): 4.04245e-25, 9.92376e-06, 6.25775e-11, 1.36188e-05, 1.27769e-01, 2.30013e-02
Node: 05 (pos: 0.051): 3.62508e-22, 4.03003e-05, 1.09278e-09, 5.39333e-05, 1.69563e-01, 3.70597e-02
=========================================================================================================
Evaluation on test dataset:
Step 25, mean loss 0.03408278542954493
Step 50, mean loss 0.017323021478394353
Step 75, mean loss 0.030795724695067757
Step 100, mean loss 0.041032422947464134
Step 125, mean loss 0.04798114163423586
Step 150, mean loss 0.11177297661626198
Step 175, mean loss 0.08756441818328102
Step 200, mean loss 0.2588588188003747
Step 225, mean loss 0.11355807746351729
Unrolled forward losses 1.5050810406212607
Unrolled forward base losses 2.5892662621222584
Saved model at models/GNN_euclidean_CE_E1_xresolution100-200_lr0.0001_n50_s0.01_tw25_unrolling2_time5192325.tar

Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00), 0.1340030914112644, [0.00045252424982933066, 0.0022543647911840276, 0.0010926662046215861, 0.0023423402126361756, 0.011176859025625204, 0.00666301395399355], [0.9772231414753629, 1.0538793583547301, 1.132018455131452, 1.1752798886140425, 1.3225705829763836, 1.1814613083981687]
Training Loss (progress: 0.08), 0.13670219354056412, [0.00045279844242565234, 0.002285075363442306, 0.0011106287320067944, 0.0023292212352291493, 0.011234812053761697, 0.006613244402740401], [0.9770951978454496, 1.0539094503384332, 1.1320536751627042, 1.1753174214663564, 1.322893189776528, 1.1815970436874914]
Training Loss (progress: 0.16), 0.12878584142756946, [0.00046608140505497933, 0.0023096854830043844, 0.001097829700269793, 0.0023450363753953194, 0.011338472015320216, 0.006629966959085466], [0.9770448703706559, 1.053941037057331, 1.1320511142328273, 1.175401180858267, 1.3231409658893656, 1.1817227104830974]
Training Loss (progress: 0.24), 0.1381735522595832, [0.00044964747507880037, 0.0022634748220636616, 0.00110582556425292, 0.00232665431369445, 0.011173999928987014, 0.00662546615973679], [0.9770069688195671, 1.0538839698593647, 1.1320240445129148, 1.1753356952479876, 1.3233633283810575, 1.1818367243757044]
Training Loss (progress: 0.32), 0.12884357621995182, [0.00045885104646738476, 0.0022865518537604004, 0.001089864671991325, 0.0023340193186341743, 0.011267522530748243, 0.006641941506203612], [0.9769422925031676, 1.0538442167249666, 1.1320018102719, 1.1754316993684686, 1.323647747191881, 1.1819894827289399]
Training Loss (progress: 0.40), 0.14784309342200205, [0.0004679864272971057, 0.002290334969393789, 0.0010915089882943371, 0.002336200687332753, 0.011202725162333766, 0.0066195707903679655], [0.9768556316814, 1.0539441058563612, 1.1319836971379134, 1.175553926830263, 1.3238518926126013, 1.1821183770188324]
Training Loss (progress: 0.48), 0.1269853208546537, [0.0004450716351749837, 0.002313181030910699, 0.0010747492860795254, 0.0023680824275320745, 0.011198687212463513, 0.006686115971426708], [0.9768636829890257, 1.0539558635138493, 1.1320191042524068, 1.175619724054539, 1.3239634530458868, 1.182345617344701]
