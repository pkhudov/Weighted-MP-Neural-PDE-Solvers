Training on dataset data/CE_train_E1.h5
Specified device: cuda:0
Using NVIDIA A100 80GB PCIe
models/GNN_norm+kernel+net_CE_E1_xresolution100-200_lr0.0001_n6_s0.01_tw25_unrolling2_time53754.tar
Number of parameters: 1032635.0
Epoch 0
Starting epoch 0...
=========================================================================================================

Kernel Weights for neighbours of Node 6 (for each GNN layer):

Node: 00 (pos: 0.000): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
Node: 01 (pos: 0.010): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 02 (pos: 0.020): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 03 (pos: 0.030): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 04 (pos: 0.040): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 05 (pos: 0.051): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
-
Node: 07 (pos: 0.071): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 08 (pos: 0.081): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 09 (pos: 0.091): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 10 (pos: 0.101): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 11 (pos: 0.111): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 12 (pos: 0.121): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
---------------------------------------------------------------------------------------------------------

Kernel Weights for neighbours of Node 56 (for each GNN layer):

Node: 57 (pos: 0.576): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 58 (pos: 0.586): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 59 (pos: 0.596): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 60 (pos: 0.606): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 61 (pos: 0.616): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 50 (pos: 0.505): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
-
Node: 51 (pos: 0.515): 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01, 7.74858e-01
Node: 52 (pos: 0.525): 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01, 8.49380e-01
Node: 53 (pos: 0.535): 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01, 9.12263e-01
Node: 54 (pos: 0.545): 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01, 9.60009e-01
Node: 55 (pos: 0.556): 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01, 9.89849e-01
Node: 62 (pos: 0.626): 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01, 6.92595e-01
=========================================================================================================
Training Loss (progress: 0.00), 1.320725810491906, 7.301284541171064e-09, -8.200094669542327e-09, -3.6192563840141416e-08, 6.773208160908349e-08, -1.1549961628071075e-07, -6.545403856136172e-08, [0.005622306200630375, 0.016985760786472834, 0.01782763329410148, 0.01654161511994366, 0.01332645893770262, 0.017346876331740587]
Training Loss (progress: 0.08), 0.2534471637826637, -8.865296246471442e-08, 2.758550945778342e-08, -1.1810921417349592e-07, -3.479758288303104e-08, 4.4244665375368947e-07, 1.5313821452989342e-08, [0.0012276803183741054, 0.0009726787706366459, 0.0011756758784554396, 0.0008732617176760138, 0.0010375310647814148, 0.005482301978008231]
Training Loss (progress: 0.16), 0.200260981135408, 8.001080728287851e-08, -3.395440970237429e-08, -1.0837119096491588e-07, -2.1517554292818615e-06, -8.124826241471789e-07, -9.34662570384016e-08, [0.0012852769827435397, 0.0008758541371337116, 0.0012313262522589594, 0.0008745312775885945, 0.0010453725123553712, 0.005165047701881574]
Training Loss (progress: 0.24), 0.16927829976965467, 8.994573627758766e-08, 2.130232819197597e-09, -2.3290862362095373e-07, -1.565816119830266e-06, -1.668271666606332e-06, -2.1979635059741603e-07, [0.001227747246992028, 0.0007608419433347827, 0.0009783749168313554, 0.0007803363226266876, 0.0008472994436487022, 0.004697568995708163]
Training Loss (progress: 0.32), 0.15306209539324422, -5.5000487045061355e-08, 5.593567834628548e-09, -4.5808498248086707e-07, -2.6292686131748055e-06, 3.806157612077178e-08, 8.872588201213504e-08, [0.0022534699281120175, 0.0006894795415063488, 0.0008924397848404223, 0.001646690787546834, 0.0020132905518336864, 0.0038482173877051485]
Training Loss (progress: 0.40), 0.1414184629969342, -1.2970910763530174e-07, 7.303081288290981e-09, -5.589856086384635e-07, -3.051646432528097e-06, 6.970661228760886e-07, 3.4286837715762705e-07, [0.001847652010051785, 0.0007513978616681611, 0.001036003999629719, 0.0006203950046920979, 0.0018072754750783511, 0.0034426555073671095]
Training Loss (progress: 0.48), 0.13524746695822903, -5.043190465719464e-08, -4.714040223936616e-08, 1.5416394080999832e-08, 5.835997284324462e-07, 9.243735099408956e-07, -1.1328990128866715e-07, [0.0010580907784157193, 0.0009024159183986277, 0.0012072266768958662, 0.0009703724175863635, 0.001068723290918276, 0.0030784826359791574]
Training Loss (progress: 0.56), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.64), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.72), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.80), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.88), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Training Loss (progress: 0.96), nan, nan, nan, nan, nan, nan, nan, [nan, nan, nan, nan, nan, nan]
Evaluation on validation dataset:
